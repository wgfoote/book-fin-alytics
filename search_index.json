[
["index.html", "Financial Analytics Chapter 1 Preliminaries 1.1 The basic problem 1.2 Analytics 1.3 Why this book? 1.4 A useful way to start: private placement finance 1.5 Chapter Outline", " Financial Analytics Bill Foote 2019-12-23 Chapter 1 Preliminaries Science alone of all the subjects contains within itself the lesson of the danger of belief in the infallibility of the greatest teachers of the preceding generation. - Richard Feynman This book is designed to provide students, analysts, and practitioners (the collective “we” and “us”) with approaches to analyze various types of financial data sets, and to use analytical techniques to support financial decisions based on statistics obtained from the data. There are two decisions that financial managers make: investment and raising funds to make an investment. It turns out that the simple rule of invest in what creates more value for stakeholders is the fundamental and most useful rule for making financial decisions. There are many analytical techniques financial managers use to calculate value. Statistics and optimization rank high on the list of broadly available techniques. The book covers various areas in the financial industry, from analyzing credit data (credit card receivables), to studying global relations among macroeconomic events, to managing risk and return in multi-asset portfolios. The topics in the book employ a wide range of techniques including non-linear estimation, portfolio analytics, risk measurement, extreme value analysis, forecasting and predictive techniques, and financial modeling. 1.1 The basic problem The premises of this book include the following chain of reasoning: Financial decisions occur in the context of markets Markets determine value Value can change abruptly, or not, due to changes in information among market participants The interaction of market volatility with financial decisions changes market value Since all financial decisions occur in markets (both overt and latent), the market value of an asset (or liability) is its present value. However, market volatility depends on changes in forces (and information) that drive value. For most organizations revenue, cost, and assets drive value. Thus to the extent that information changes these drivers, so does it change value. How can we analyze changes in market value? The finance in financial analytics boils down to answering four questions about value and volatility: How do we measure the interactions of risk and return in multiple markets? Given the interactions of risk and return in assets and liabilities, what is the combination of assets and liabilities that return the highest value for the risk? Given this combination (a portfolio) what is amount of capital needed to support the portfolio? Given risk tolerances and thresholds for loss, how much cash should be held on the balance sheet? We will be taking great pains to define exactly what we mean by risk (think of standard deviation) and return (a rate of change of value). 1.2 Analytics By its very nature the science of data analytics is disruptive. That means, among many other things, that much attention should be paid to the scale and range of invalid, as yet not understood, outlying, and emerging trends. This is as true within the finance domain of knowledge as any other. Outliers are where value is gain and lost. Spikes in interest rates, commodity prices, information about the reputation of an organization, as well as the doldrums of a very slow news day, are all deviations from a supposed trend. Finance lives in the deviation from the trend. Analytics, especially statistically based techniques, is all about the analysis of deviations from current and projected beliefs, otherwise known as the trend. Throughout the book, we will learn the core of ideas of programming software development to implement financial analyses (functions, objects, data structures, flow control, input and output, debugging, logical design and abstraction) through writing code. We will learn how to set up stochastic simulations, manage data analyses, employ numerical optimization algorithms, diagnose their limitations, and work with and filter large data sets. Since code is also an important form of communication among analysts, we will learn how to comment and organize code, as well as document work product. This book supports the notion that coding as the best path to data analytics. Coding gives us the ability to tackle the most complex problems, because code is flexible, reusable, inspectable, and reproducible. Throughout this book the R programming language is ussed as the lingua franca of financial analytics. Yes, Python is widely used as well; so is Julia, and less so C++ and even less than that FORTRAN. All have their operational place according to the maxim use what works (and also makes us happy). The R programming language has a large following in the statistical and optimization user side of many domains, including finance. This makes it a natural enviornment to start financial analytics on the R programming platform. The RStudio integrated development environment (IDE) continues to develop resources for cross-language collaboration, mirroring the membership of most data analytics teams. 1.3 Why this book? There are many data analytics books available, but only a few devoted to the finance domain. Among these are Ruppert and Matteson (2015) and (???) supported by package Pfaff and McNeil (2016). The first uses a statistically topical approach and is invaluable as a reference with copious R examples. This book takes a different approach by starting with financial decision topics and then applying typically practiced and useful techniques from data analytics and optimization to analyze results. QRM also takes a topical finance approach while using a much more mathematically oriented framework than countenanced in this book. 1.4 A useful way to start: private placement finance The best way to learn any discipline is to build out a practical example. Here is such an example from the work of privately placing financings in a transnational environment. The financing uses multiple markets, with valuation and yields analysis, requiring portfolio management, hedging, and capital requirements. Perfect for our purposes. This example, and many others like it, can serve as the independent way to build an arsenal of techniques practically applied. Key questions that relate to this example are located at the end of each of the chapters. 1.4.1 The market for private placement The dual-currency private placement market arises for several reasons: Investors with excess cash crave higher returns Higher returns occur in higher risk countries than the investor’s domicile Borrowers are not large enough or diversified enough to access public capital markets Some borrowers require non-disclosure of financial activity Issuers (borrowers) will usually have a lower credit rating than investors (lenders) The private placement market had a banner year in 2017 with growth continuing into 2018 and beyond. Importantly there is an increased investor appetite for sub-investment grade paper with good credit stories and inherent guarantees. Since the bulk of investors are insurers, the National Association of Investment Commissioners (NAIC) requires capital charges assessed against risk weighted assets. Sub-investment grade assets might require more capital and thus more return for the insurer-investor. NAIC reports that U.S. insurers held more than $370 billion in privately placed securities with NAIC credit ratings of 1 or 2. The private placement market provides financing for projects that could not be financed on the public side of the capital markets. Decisions to lend are made on the basis of the initial credit quality of the issuer and negative covenants and other protections, such as parental guarantees and swap indemnification. Because information about the issuer, the issue and the financed projects are not publicly disseminated, private placements tend to be less liquid than publicly traded bonds. Insurers have the financial and legal sophistication to assess the issuer’s credit risk and the other risks presented by the issuers, the issue and the project. Insurers and their bankers will be able to negotiate protections for their loans. Insurers need long-term assets like bonds to match long-term claims liabilities. Thusthey can hold relatively illiquid assets like private placements to maturity, with higher coupons to compensate for illiquidity. Insurers can also monitor the performance of these assets, using covenants and other contractual protections to manage deteriorations in the credit quality of the issuer/borrower in a way which has, over time and collectively, resulted in much lower default rates for this asset class than for comparable publicly traded bonds. Banks play a non-conventional role in this market. Typically a lending officer asks “Does this borrower’s cash flow requirements match our bank’s financing criteria?” In the private placement market the bank as intermediary asks instead “How can this borrower’s financing requirements be structured to be acceptable to some investor?” The investor might, and often is, not a client of the bank. Since the search for returns carries lenders and borrowers across borders, currencies will often need to be swapped. Swaps will need to be indemnified with collateral. Issuer default will need to be insured with stand-by letters of credit and credit default swaps. Banks are in the regular business of providing these services. 1.4.2 A deal emerges from the mist There are three primary players in this deal: The borrower in this example is an energy services company that operates in Cameroon and is a wholly-owned subsidiary of a U.S. domiciled company. The subsidiary requires 10 year fixed-rate XAF debt to build facilities, expand technical resources, and participate in regional renewable energy initiatives. The U.S. parent is rated BBB+. The investor is a U.S. insurer rated A- that is looking for 10-year floating rate USD. Regulators require the insurer to invest in A- rated or better paper. The intermediary is the U.S. branch of a French bank. The French bank is exposed to XAF currency and has branches throughout the Central African region. The bank also clears funds for the XAF issuer Bank of the Central African States. It seeks 2- to 7-year EUR floating rate investments. The bank will lend to any secured credit priced in liquid markets. Can these three counterparties agree to a structure? Using the entries in this table to assess if borrower terms might be acceptable (“yes”) or not (“not”) to these counterparties. table &lt;- matrix(&quot;&quot;, nrow = 4, ncol = 2) colnames(table) &lt;- c(&quot;French Bank&quot;, &quot;U.S. Insurer&quot;) rownames(table) &lt;- c(&quot;maturity&quot;, &quot;credit&quot;, &quot;rate&quot;, &quot;currency&quot;) table[1,1] &lt;- &quot;no&quot; table[1,2] &lt;- &quot;yes&quot; table[2,1] &lt;- &quot;yes&quot; table[2,2] &lt;- &quot;no&quot; table[2:4,2] &lt;- &quot;no&quot; table[3,1] &lt;- &quot;no&quot; table[4,1] &lt;- &quot;yes&quot; table %&gt;% kable() %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;), full_width = F) French Bank U.S. Insurer maturity no yes credit yes no rate no no currency yes no A deal indeed might emerge. Analysts for each of the counterparties will trawl through historical data, interview one another in a due diligence process, gear up attorneys, accountants, tax professionals, energy experts, and in-country contacts. They will examine the feasibility of the many legs of the deal to offer favorable and competitive returns for the risk. At the least they will develop cash flows, net present values, and internal rates of return in their analysis. In the end they will end up asking the four questions of this book. There are USD, XAF, energy commodity, bonds, and swap markets interacting dynamically across risk and returns. Given these intertwined risks and returns, each private placement participant must allocate funds across multiple markets. Given these allocations, each private placement participant must account for the appropriate level of capital given return requirements and the range of risks in this arrangement. In the end each participant will need to adhere to their own risk tolerance and loss thresholds to determine the amount of cash needed in their balance sheets. An appendix contains some key financial and statistical concepts with measurements to aid our quest. 1.5 Chapter Outline Here is an outline of topics covered by chapter. 1. Preliminaries. This chapter covering some finance basics, the R programming language platform on RStudio, and a working example for use in developing a project throughout the rest of the book. 2. R Warm-Ups for Finance. R computations, data structures, financial, probability, and statistics calculations, visualization. Documentation with R Markdown. 3. More R Warm-Ups. Functions, loops, control bootstrapping, simulation, and more visualization. Of use if the definition of what is meant by a return and by value, along with an excursion into the shape, and thus the risk, of a financial series. 4. Stylized Facts of Financial Markets. Data from FRED, Yahoo, and other sources. Empirical characteristics of economic and financial time series. Boostrapping confidence intervals and a stab at managing a list of potential risk and return interactions we will, with much trepidation, call stylized facts. 5. Market Risk. Quantile (i.e., Value at Risk) and coherent (i.e., Expected Shortfall) risk measures and a provisional answer the the question about how much capital is needed to support a risky financial decision. 6. Portfolio Optimization. Combining risk management with portfolio allocations. Optimizing allocations. Simulating the efficient frontier and answers the question about what combination of assets (and liabilities) returns the highest value for the risk as well as how much cash to hold. 7. Aggregating Enterprise Risks. Enterprise risk management analytics and application. Workflow to build an online application. Introduction to Shiny and ShinyDashboard. Building a simple app using R Markdown, flexdashboards, and Shiny. References "],
["r-warm-ups-in-finance.html", "Chapter 2 R Warm-ups in Finance 2.1 Imagine This 2.2 Tickling the Ivories 2.3 Building Some Character 2.4 Arrays and You 2.5 More Array Work 2.6 Summary 2.7 Further Reading 2.8 Practice Set", " Chapter 2 R Warm-ups in Finance 2.1 Imagine This You work for the division president of an aerospace company that makes testing equipment for high-tech manufacturers. Orders arrive “lumpy” at best as some quarters are big producers, others stretch the company’s credit revolver. The president, call her Nancy, found a new way to make money: lease equipment. This would help finance operations and smooth cash flow. You had a follow-on idea: build a captive finance company to finance and insure the manufactured product for customers. “Nice ideas,” Nancy quips. “Show me how we can make money.” For starters we want to borrow low and sell (leases) high! How? We can define three salient factors: The “money factor” Sets the monthly payments Depends on the length and frequency of payment of the lease Also depends on the value of money at monthly forward borrowing rates Residual value of the equipment Uncertain based on competitors’ innovations, demand for manufacturers’ products, etc. Uncertain based on quality of equipment at end of lease (is there a secondary market?) Portfolio of leases By maturity By equipment class By customer segment This simple vignette from leasing introduces us to the complexities and challenges of financial analytics. We see that cash flows evolve over time and may depend on various uncertain factors. The time series of cash flows and their underlying factors in turn have distributions of potential outcomes and also may be related to one another. Groups of cashflows may also be related to one another structurally through portfolios. These are simply combinations of sets of cashflows and, of course, their underlying and uncertain factors. Sets of cash flows and their factors may be delineated according to customer segments, classification of equipment, and introducing a timeing element, by their maturity. 2.1.1 Modeling process Throughout financial analytics there is a modeling process we can deploy. Let’s begin the modeling process by identifying leasing cash flow components and financial considerations we might use to begin to build reasonable scenarios. In ourleasing scenario here is a concordance of cash flow elements with models that will be illustrated throughout this book: Lease payments Chapter 5: Term structure of interest rates Chapter 7: Credit risk of customers Chapter 4: Impact of economy on customers’ market value Residual cashflow Chapter 8: Operational risk Chapter 11: Aggregating risks Borrowing costs Chapter 5: Term structure of interest rates Chapter 7: Our own credit risk Collateral Chapter 10: Portfolio optimization Chapter 6: Market risk Regulatory issues Chapter 8: Operational risk Shareholder tolerance for risk Chapter 6: Market risk Chapter 9: Hedging Chapter 11: Capital requirements In this first Chapter we will review some aspects of R programming to whet our appetite for further work in finance and risk. The first steps here will prepare the way for us to tackle the many issues encountered with financial time series, portfolios of risk and return factors, market, credit, and operational risk measurement, the aggregation of risk and return, and the fundamental measurement of volatility. 2.1.2 Chapter overview In this first chapter we will Introduce R and calculations, arrays, text handling, graphics Review basic finance and statistics content Use introductory R calculations in financial and statistical examples Extend introductory calculations with further examples 2.1.3 What is R? R is software for interacting with data along a variety of user generated paths. With R you can create sophisticated (even interactive) graphs, you can carry out statistical and operational research analyses, and you can create and run simulations. R is also a programming language with an extensive set of built-in functions. With increasing experience, you can extend the language and write your own code to build your own financial analytical tools. Advanced users can even incorporate functions written in other languages, such as C, C++, and Fortran. The current version of R derives from the S language. S has been around for more than twenty years and has been with extrensive use in statistics and finance, first as S and then as the commercially available S-PLUS. R is an open source implementation of the S language that is now a viable alternative to S-PLUS. A core team of statisticians and many other contributors work to update and improve R and to make versions that run well under all of the most popular operating systems. Importantly, R is a free, high-quality statistical software that will be useful as you learn financial analytics even though it is also a first-rate tool for professional statisticians, operational researchers, and financial analysts and engineers.\\footnote(But see this post on a truly big data language APL: https://scottlocklin.wordpress.com/2013/07/28/ruins-of-forgotten-empires-apl-languages/) 2.1.4 R for analytics There are several reasons that make R an excellent choice of software for an analytics course. Some benefits of using R include: R is free and available online. R is open-source and runs on UNIX, Windows, and Macintosh operating systems. R has a well-documented, context-based, help system enhanced by a wide, and deep, ranging user community globally and across several disciplines. R has excellent native static graphing capabilities. Interactive dynamic graphics are evolving along with the ability to embed analytics into online applications. With R you can build dashboards and websites to communicate results dynamically with consumers of the analytics you generate. Practitioners can easily migrate to the commercially supported S-Plus program, if commercial software is required. S and S-Plus are the immediate ancestors of the R programming environment. Cloud computing is now available with large data implementations. R’s language has a powerful, easy-to-learn syntax with many built-in statistical and operational research functions. Just as important are the extensive web-scraping, text structuring, object class construction, and the extensible functional programming aspects of the language. A formal language definition is being developed. This will yield more standardization and better control of the language in future versions. R is a computer programming language. For programmers it will feel more familiar than for others, for example Excel users. R requires array thinking and object relationships that are not necessarily native, but indeed are possible, in an Excel spreadsheet environment. In many ways, the Excel style and R style of environments complement one another. Even though it is not necessarily the simplest software to use, the basics are easy enough to master, so that learning to use R need not interfere with learning the statistical, operational research, data, and domain-specific concepts encountered in an analytics-focused course. There is at least one drawback. The primary hurdle to using R is that most existing documentation and plethora of packages are written for an audience that is knowledgable about statistics and operational research and has experience with other statistical computing programs. In contrast, this course intends to make R accessible to you, especially those who are new to both statistical concepts and statistical computing. 2.1.5 Hot and cold running resources Much is available in books, e-books, and online for free. This is an extensive online commChaptery that links expert and novice modelers globally. The standard start-up is at CRAN http://cran.r-project.org/manuals.html. A script in the appendix can be dropped into a workspace and played with easily. Other resources include Julian Faraway’s https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf complete course on regression where you can imbibe deeply of the many ways to use R in statistics. Along econometrics lines is Grant Farnsworth’s https://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf. Winston Chang’s http://www.cookbook-r.com/ and Hadley Wickham’s example at http://ggplot2.org/ are terrific online graphics resources. 2.2 Tickling the Ivories Or if you paint and draw, the 2-minute pose will warm you up. In the RStudio console panel (in the SW pane of the IDE) play with these by typing these statements at the &gt; symbol: 1 + (1:5) ## [1] 2 3 4 5 6 This will produce a vector from 2 to 6. We can use alt- (hold alt and hyphen keys down simultaneously) to produce &lt;-, and assign data to an new object. This is a from R’s predecessor James Chamber’s S (ATT Bell Labs) that was ported from the single keystroke \\(\\leftarrow\\) in Ken Iverson’s APL (IBM), where it is reserved as a binary logical operator. We can now also use = to assign variables in R. But, also a holdover from APL, we will continue to use = only for assignments within functions. [Glad we got that over!] Now let’s try these expressions. x &lt;- 1+ (1:5) sum(x) ## [1] 20 prod(x) ## [1] 720 These actions assign the results of a calculation to a variable x and then sum and multiply the elements. x is stored in the active workspace. You can verify that by typing ls() in the console to list the objects in the workspace. Type in these statements as well. ls() ## [1] &quot;IRR&quot; &quot;table&quot; &quot;x&quot; length(x) ## [1] 5 x[1:length(x)] ## [1] 2 3 4 5 6 x[6:8] ## [1] NA NA NA x[6:8] &lt;- 7:9 x/0 ## [1] Inf Inf Inf Inf Inf Inf Inf Inf x has length of 5 and we use that to index all of the current elements of x. Trying to access elements 6 to 8 produces na because they do not exist yet. Appending 7 to 9 will fill the spaces. Dividing by 0 produces inf. (x1 &lt;- x-2) ## [1] 0 1 2 3 4 5 6 7 x1 ## [1] 0 1 2 3 4 5 6 7 x/x1 ## [1] Inf 3.00 2.00 1.67 1.50 1.40 1.33 1.29 Putting parentheses around an expression is the same as printing out the result of the expression. Element-wise division (multiplication, addition, subtraction) produces inf as the first element. 2.2.1 Try this exercise Suppose we have a gargleblaster machine that produces free cash flows of $10 million each year for 8 years. The machine will be scrapped and currently you believe you can get $5 million at the end of year 8 as salvage value. The forward curve of interest rates for the next 1 to 8 years is 0.06, 0.07, 0.05, 0.09, 0.09, 0.08, 0.08, 0.08. What is the value of $1 received at the end of each of the next 8 years? Use this script to begin the modeling process. Describe each calculation. rates &lt;- c(0.06, 0.07, 0.05, 0.09, 0.09, 0.08, 0.08, 0.08) t &lt;- seq(1, 8) (pv.1 &lt;- sum(1/(1+rates)^t)) What is the present value of salvage? Salvage would be at element 8 of an 8-element cash flow vector, and thus would use the eighth forward rate, rate[8], and t would be 8 as well. Eliminate the sum in the above script. Make a variable called salvage and assign salvage value to this variable. Use this variable in place of the 1 in the above script for pv.1. Call the new present value pv.salvage. What is the present value of the gargleblaster machine? Type in these statements. The rep function makes an 8 element cash flow vector. We change the value of the 8th element of the cash flow vector to include salvage. Now use the pv.1 statement above and substitute cashflow for 1. You will have your result. cashflow &lt;- rep(10, 8) cashflow[8] &lt;- cashflow[8] + salvage Some results follow. The present value of $1 is The present value of a $1 is this mathemetical formula. \\[ PV = \\sum_{t=1}^{8}\\frac{1}{(1+r)^t} \\] This mathematical expression can be translated into R this way rates &lt;- c(0.06, 0.07, 0.05, 0.09, 0.09, 0.08, 0.08, 0.08) t &lt;- seq(1, 8) (1/(1+rates)^t) ## [1] 0.943 0.873 0.864 0.708 0.650 0.630 0.583 0.540 (pv.1 &lt;- sum(1/(1+rates)^t)) ## [1] 5.79 We define rates as a vector using the c() concatenation function. We then define a sequence of 8 time indices t starting with 1. The present value of a $1 is sum of the vector element-by-element calculation of the date by date discounts \\(1/(1+r)^t\\). The present value of salvage is the discounted salvage that is expected to occur at, and in this illustration only at, year 8. \\[ PV_{salvage} = \\frac{salvage}{(1+r)^8} \\] Translated into R we have salvage &lt;- 5 (pv.salvage &lt;- salvage/(1 + rates[8])^8) ## [1] 2.7 The present value of the gargleblaster machine is the present value of cashflows from operations from year 1 to year 8 plus the present value of salvage received in year 8. Salvage by definition is realized at the of the life of the operational cashflows upon disposition of the asset, here at year 8. \\[ PV_{total} = \\sum_{t=1}^{8}\\frac{cashflow_t}{(1+r)^t} + \\frac{salvage}{(1+r)^8} \\] This expression translates into R this way: cashflow &lt;- rep(10, 8) cashflow[8] &lt;- cashflow[8] + salvage (pv.machine &lt;- sum(cashflow/(1+rates)^t)) ## [1] 60.6 The rep or “repeat” function creates cash flows of $10 for each of 8 years. We adjust the year 8 cash flow to reflect salvage so that \\(cashflow_8 = 10 + salvage\\). The [8] indexes the eighth element of the cashflow vector. 2.3 Building Some Character Let’s type these expressions into the console at the &gt; prompt: x[length(x)+1] &lt;- &quot;end&quot; x[length(x)+1] &lt;- &quot;end&quot; x.char &lt;- x[-length(x)] x &lt;- as.numeric(x.char[-length(x.char)]) str(x) ## num [1:8] 2 3 4 5 6 7 8 9 We have appended the string “end” to the end of x, twice. We use the - negative operator to eliminate it. By inserting a string of characters into a numeric vector we have forced R to transform all numerical values to characters. To keep things straight we called the character version x.char. In the end we convert x.char back to numbers that we check with the str(ucture) function. We will use this procedure to build data tables (we will call these “data frames”) when comparing distributions of variables such as stock returns. Here’s a useful set of statements for coding and classifying variables. Type these statements into the console. set.seed(1016) n.sim &lt;- 10 x &lt;- rnorm(n.sim) y &lt;- x/(rchisq(x^2, df = 3))^0.5 We did a lot of R here. First, we set a random seed to reproduce the same results every time we run this simulaton. Then, we store the number of simulations in n.sim and produced two new variables with normal and a weirder looking distribution (almost a Student’s t distribution). Invoking help will display help with distributions in the SE pane of the RStudio IDE. z &lt;- c(x,y) indicator &lt;- rep(c(&quot;normal&quot;,&quot;abnormal&quot;), each=length(x)) xy.df &lt;- data.frame(Variates = z, Distributions = indicator) Next we concatenate the two variables into a new variable z. We built into the variable indicator the classifier to indicate which is x and which is y. But let’s visualize what we want. (Paint in words here.) We want a column the first n.sim elements of which are x and the second are y. We then want a column the first n.sim elements of which are indicated by the character string “normal”, and the second n.sim elements by “abnormal”. The rep function replicates the concatenation of “normal” and “abnormal” 10 times (the length(x)). The each feature concatenates 10 replications of “normal” to 10 replications of “abnormal”. We concatenate the variates into xy with the c() function. We can see the first 5 components of the data frame components using the $ subsetting notation as below. str(xy.df) ## &#39;data.frame&#39;: 20 obs. of 2 variables: ## $ Variates : num 0.777 1.373 1.303 0.148 -1.825 ... ## $ Distributions: Factor w/ 2 levels &quot;abnormal&quot;,&quot;normal&quot;: 2 2 2 2 2 2 2 2 2 2 ... head(xy.df$Variates, n = 5) ## [1] 0.777 1.373 1.303 0.148 -1.825 head(xy.df$Distributions, n = 5) ## [1] normal normal normal normal normal ## Levels: abnormal normal The str call returns the two vectors inside of xy. One is numeric and the other is a “factor” with two levels. R and many of the routines in R will interpret these as zeros and ones in developing indicator and dummy variables for regressions and filtering. 2.3.1 The plot thickens We will want to see our handiwork, so load the ggplot2 library using install.packages(\"ggplot2\"). \\footnote(Visit Hadley Wickham’s examples at http://ggplot2.org/.) This plotting package requires data frames. A “data frame” simply put is a list of vectors and arrays with names. An example of a data frame in Excel is just the worksheet. There are columns with names in the first row, followed by several rows of data in each column. Here we have defined a data frame xy.df. All of the x and y variates are put into one part of the frame, and the distribution indicator into another. For all of this to work in a plot the two arrays must be of the same length. Thus we use the common n.sim and length(x) to insure this when we computed the series. We always examine the data, here using the head and tail functions. Type help(ggplot) into the console for details. The ggplot2 graphics package embodies Hadley Wickham’s “grammar of graphics” we can review at http://ggplot2.org. Hadley Wickham has a very useful presentation with numerous examples at http://ggplot2.org/resources/2007-past-present-future.pdf. As mentioned above, the package uses data frames to process graphics. A lot of packages other than ggplot2, including the base stats package, require data frames. We load the library first. The next statement sets up the blank but all too ready canvas (it will be empty!) on which a density plot can be rendered. library(ggplot2) ggplot(xy.df, aes(x = Variates, fill = Distributions)) The data frame name xy.df is first followed by the aesthetics mapping of data. The next statement inserts a geometrical element, here a density curve, which has a transparency parameter aesthetic alpha. 2.3.2 Try this example Zoom in with xlim and lower x-axis and upper x-axis limits using the following statement: ggplot(xy.df, aes(x = Variates, fill = Distributions)) + geom_density(alpha = .3) + xlim(-1,6) Now we are getting to extreme finance by visualizing the tail of this distribution. 2.4 Arrays and You Arrays have rows and columns and are akin to tables. All of Excel’s worksheets are organized into cells that are tables with columns and rows. Data frames are more akin to tables in data bases. Here are some simple matrix arrays and functions. We start by making a mistake: (A.error &lt;- matrix(1:11, ncol=4)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 1 The matrix() function takes as input here the sequence of numbers from 1 to 11. It then tries to put these 11 elements into a 4 column array with 3 rows. It is missing a number as the error points out. To make a 4 column array out of 11 numbers it needs a twelth number to complete the third row. We then type in these statements (A.row &lt;- matrix(1:12, ncol=4)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 (A.col &lt;- matrix(1:12, ncol=4, byrow=FALSE)) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 In A we take 12 integers in a row and specify they be organized into 4 columns, and in R this is by row. In the next statement we see that A.col and column binding cbind() are equivalent. (R &lt;- rbind(1:4, 5:8, 9:12)) # Concatenate rows ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 (C &lt;- cbind(1:3, 4:6, 7:9, 10:12)) # concatenate columns ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 A.col == C ## [,1] [,2] [,3] [,4] ## [1,] TRUE TRUE TRUE TRUE ## [2,] TRUE TRUE TRUE TRUE ## [3,] TRUE TRUE TRUE TRUE Using the outer product allows us to operate on matrix elements, first picking the minimum, then the maximum of each row. The pmin and pmax compare rows element by element. If you used min and max you would get the minimum and maximum of the whole matrix. (A.min &lt;- outer(3:6/4, 3:6/4, FUN=pmin)) # ## [,1] [,2] [,3] [,4] ## [1,] 0.75 0.75 0.75 0.75 ## [2,] 0.75 1.00 1.00 1.00 ## [3,] 0.75 1.00 1.25 1.25 ## [4,] 0.75 1.00 1.25 1.50 (A.max &lt;- outer(3:6/4, 3:6/4, FUN=pmax)) # ## [,1] [,2] [,3] [,4] ## [1,] 0.75 1.00 1.25 1.5 ## [2,] 1.00 1.00 1.25 1.5 ## [3,] 1.25 1.25 1.25 1.5 ## [4,] 1.50 1.50 1.50 1.5 We build a symmetrical matrix and replace the diagonal with 1. A.sym looks like a correlation matrix. Here all we were doing is playing with shaping data. (A.sym &lt;- A.max - A.min - 0.5) ## [,1] [,2] [,3] [,4] ## [1,] -0.50 -0.25 0.00 0.25 ## [2,] -0.25 -0.50 -0.25 0.00 ## [3,] 0.00 -0.25 -0.50 -0.25 ## [4,] 0.25 0.00 -0.25 -0.50 diag(A.sym) &lt;- 1 A.sym ## [,1] [,2] [,3] [,4] ## [1,] 1.00 -0.25 0.00 0.25 ## [2,] -0.25 1.00 -0.25 0.00 ## [3,] 0.00 -0.25 1.00 -0.25 ## [4,] 0.25 0.00 -0.25 1.00 2.4.1 Try this exercise The inner product %*% cross-multiplies successive elements of a row with the successive elements of a column. If there are two rows with 5 columns, there must be a matrix at least with 1 column that has 5 rows in it. Let’s run these statements. n.sim &lt;- 100 x.1 &lt;- rgamma(n.sim, 0.5, 0.2) x.2 &lt;- rlnorm(n.sim, 0.15, 0.25) hist(x.1) ; hist(x.2) X &lt;- cbind(x.1, x.2) rgamma allows us to generate n.sim versions of the gamma distribution with scale parameter 0.5 and shape parameter 0.2. rlnorm is a popular financial return distribution with mean 0.15 and standard deviation 0.25. We can call up ??distributions to get detailed information. Let’s plot the histograms of each simulated random variate using hist(). The cbind function binds into matrix columns the row arrays x.1 and x.2. These might be simulations of operational and financial losses. The X matrix could look like the “design” matrix for a regression. Let’s simulate a response vector, say equity, and call it y and look at its histogram. y &lt;- 1.5*x.1 + 0.8 * x.2 + rnorm(n.sim, 4.2, 5.03) Now we have a model for \\(y\\): \\[ y = X \\beta + \\varepsilon \\] where \\(y\\) is a 100 \\(\\times\\) 1 (rows \\(\\times\\) columns) vector, \\(X\\) is a 100 \\(\\times\\) 2 matrix, \\(\\beta\\) is a 2 \\(\\times\\) 1 vector, and \\(\\epsilon\\) is a 100 \\(\\times\\) 1 vector of disturbances (a.k.a., “errors”). Multiplying out the matrix term \\(X \\beta\\) we have \\[ y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon \\] where \\(y\\), \\(x_1\\), \\(x_2\\), and \\(\\varepsilon\\) are all vectors with 100 rows for simulated observations. If we look for \\(\\beta\\) to minimize the sum of squared \\(\\varepsilon\\) we would find that the solution is \\[ \\hat{\\beta} = (X^T X)^{-1} X^{T} y. \\] Where \\(\\hat{\\beta}\\) is read as “beta hat”. The result \\(y\\) with its hist() is hist(y) The rubber meets the road here as we compute \\(\\hat{\\beta}\\). X &lt;- cbind(x.1, x.2) XTX.inverse &lt;- solve(t(X) %*% X) (beta.hat &lt;- XTX.inverse %*% t(X) %*% y ) ## [,1] ## x.1 1.61 ## x.2 4.04 The beta.hat coefficients are much different than our model for y. Why? Because of the innovation, error, disturbance term rnorm(n.sim, 1, 2) we added to the 1.5*x.1 + 0.8 * x.2 terms. Now for the estimated \\(\\varepsilon\\) where we use the matrix inner product %*%. We need to be sure to pre-multiply beta.hat with X! e &lt;- y - X %*% beta.hat hist(e) We see that the “residuals” are almost centered at 0. 2.4.2 More about residuals For no charge at all let’s calculate the sum of squared errors in matrix talk, along with the number of obervations n and degrees of freedom n - k, all to get the standard error of the regression e.se. Mathematically we are computing \\[ \\sigma_{\\varepsilon} = \\sqrt{\\sum_{i=1}^N \\frac{\\varepsilon_i^2}{n-k}} \\] (e.sse &lt;- t(e) %*% e) ## [,1] ## [1,] 3021 (n &lt;- dim(X)[1]) ## [1] 100 (k &lt;- nrow(beta.hat)) ## [1] 2 (e.se &lt;- (e.sse / (n - k))^0.5) ## [,1] ## [1,] 5.55 The statement dim(X)[1] returns the first of two dimensions of the matrix X. Finally, again for no charge at all, lets load library psych (use install.packages(\"psych\") as needed). We will use pairs.panels() for a pretty picture of our work in this try out. First column bind cbind() the y, X, and e arrays to create a data frame for pairs.panel(). library(psych) all &lt;- cbind(y, X, e) We then invoke the pairs.panels() function using the all array we just created. The result is a scatterplot matrix with histograms of each variate down the diagonal. The lower triangle of the matrix is populated with scatterplots. The upper triangle of the matrix has correlations depicted with increasing font sizes for higher correlations. pairs.panels(all) We will use this tool again and again to explore the multivariate relationships among our data. 2.5 More Array Work We show off some more array operations in the following statements. nrow(A.min) ## [1] 4 ncol(A.min) ## [1] 4 dim(A.min) ## [1] 4 4 We calculate the number of rows and columns first. We then see that these exactly correspond to the two element vector produced by dim. Next we enter these statements into the console. rowSums(A.min) ## [1] 3.00 3.75 4.25 4.50 colSums(A.min) ## [1] 3.00 3.75 4.25 4.50 apply(A.min,1,sum) ## [1] 3.00 3.75 4.25 4.50 apply(A.min,2,sum) ## [1] 3.00 3.75 4.25 4.50 We also calculate the sums of each row and each column. Alternatively we can use the apply function on the first dimension (rows) and then on the second dimension (columns) of the matrix. Some matrix multiplications follow below. (A.inner &lt;- A.sym %*% t(A.min[,1:dim(A.min)[2]])) ## [,1] [,2] [,3] [,4] ## [1,] 0.750 0.750 0.812 0.875 ## [2,] 0.375 0.562 0.500 0.500 ## [3,] 0.375 0.500 0.688 0.625 ## [4,] 0.750 0.938 1.125 1.375 Starting from the inner circle of embedded parentheses we pull every row (the [,col] piece) for columns from the first to the second dimension of the dim() of A.min. We then transpose (row for column) the elements of A.min and cross left multiply in an inner product this transposed matrix with A.sym. We have already deployed very useful matrix operation, the inverse. The R function solve() provides the answer to the question: what two matrices, when multiplied by one another, produces the identity matrix? The identity matrix is a matrix of all ones down the diagonal and zeros elsewhere. (A.inner.invert &lt;- solve(A.inner)) ## [,1] [,2] [,3] [,4] ## [1,] 4.952380952380952550 -3.05 -1.14 -1.52 ## [2,] -2.285714285714285587 6.86 -2.29 0.00 ## [3,] 0.000000000000000127 -2.29 6.86 -2.29 ## [4,] -1.142857142857143016 -1.14 -3.43 3.43 Now we use our inverse with the original matrix we inverted. (A.result &lt;- A.inner %*% A.inner.invert) ## [,1] [,2] [,3] ## [1,] 0.999999999999999778 0.000000000000000000 0.000000000000000000 ## [2,] 0.000000000000000222 1.000000000000000222 0.000000000000000000 ## [3,] 0.000000000000000111 0.000000000000000111 1.000000000000000000 ## [4,] 0.000000000000000222 -0.000000000000000444 -0.000000000000000888 ## [,4] ## [1,] 0.000000000000000000 ## [2,] 0.000000000000000000 ## [3,] 0.000000000000000444 ## [4,] 1.000000000000000000 When we cross multiply A.inner with its inverse, we should, and do, get the identity matrix that is a matrix of ones in the diagonal and zeros in the off-diagonal elements. 2.6 Summary We covered very general data manipulation in R including arithmetical operations, vectors and matrices, their formation and operations, and data frames. We used data frames as inputs to plotting functions. We also built a matrix-based linear regression model and a present value calculator. 2.7 Further Reading This introductory chapter covers material from Teetor, chapters 1, 2, 5, 6. Present value, salvage, and other valuation topics can be found in Brealey et al. under present value in the index of any of several editions. 2.8 Practice Set 2.8.1 Purpose, Process, Product These practice sets will repeat various R features in this chapter. Specifically we will practice defining vectors, matrices (arrays), and data frames and their use in present value, growth, future value calculations, We will build on this basic practice with the computation of ordinary lease squares coefficients and plots using ggplot2. We will summarize our findings in debrief documented with an R markdown file and output. 2.8.2 R Markdown set up Open a new R Markdown pdf document file and save it with file name MYName-FIN654-PS01 to your working directory. The Rmd file extension will automatically be appended to the file name. Create a new folder called data in this working directory and deposit the .csv file for practice set #2 to this directory. Modify the YAML header in the Rmd file to reflect the name of this practice set, your name, and date. Replace the R Markdown example in the new file with the following script. ## Practice set 1: present value (INSERT results here) ## Practice set 2: regression (Insert results here) Click knit in the Rstudio command bar to produce the pdf document. 2.8.3 Exercise 1: a financial planning problem 2.8.3.1 Problem We work for a mutual fund that is legally required to fair value the stock of unlisted companies it owns. Your fund is about to purchase shares of InUrCorner, a U.S. based company, that provides internet-of-things legal services. We sampled several companies with business plans similar to InUrCorner and find that the average weighted average cost of capital is 18%. InUrCorner sales is $80 million and projected to growth at 50% per year for the next 3 years and 15% per year thereafter. Cost of services provided as a percent of sales is currently 75% and projected to be flat for the foreseeable future. Depreciation is also constant at 5% of net fixed assets (gross fixed asset minus accumulated depreciation), as are taxes (all-in) at 35% of taxable profits. Discussions with InUrCorner management indicate that the company will need an increase in working capital at the rate of 15% each year and an increase in fixed assets at the rate of 10% of sales each year. Currently working capital is $10, net fixed assets is $90, and accumulated depreciation is $15. 2.8.3.2 Questions Let’s project sales, cost, increments to net fixed assets NFA, increments to working capital WC, depreciation, tax, and free cash flow FCF for the next 4 years. We will use a table to report the projection. Let’s use this code to build and display a table. # Form table of results table.names &lt;- c(&quot;Sales&quot;, &quot;Cost&quot;, &quot;Working Capital (incr.)&quot;, &quot;Net Fixed Assets (incr.)&quot;, &quot;Free Cash Flow&quot;) # Assign projection labels table.year &lt;- year # Assign projection years table.data &lt;- rbind(sales, cost, WC.incr, NFA.incr, FCF) # Layer projections rownames(table.data) &lt;- table.names # Replace rows with projection labels colnames(table.data) &lt;- table.year # Replace columns with projection years knitr::kable(table.data) # Display a readable table Modify the assumptions by +/- 10% and report the results. 2.8.4 Exercise 2: risk driver model using regression analysis 2.8.4.1 Problem We work for a healthcare insurer and our management is interested in understanding the relationship between input admission and outpatient rates as drivers of expenses, payroll, and employment. We gathered a sample of 200 hospitals in a test market in this data set. x.data &lt;- read.csv(&quot;data/hospitals.csv&quot;) 2.8.4.2 Questions Build a table that explores this data set variable by variable and relationships among variables. Investigate the influence of admission and outpatient rates on expenses and payroll. First, form these arrays. Next, compute the regression coefficients. Finally, compute the regression statistics. Use this code to investigate further the relationship among predicted expenses and the drivers, admissions and outpatients. require(reshape2) require(ggplot2) actual &lt;- y predicted &lt;- X%*%beta.hat residual &lt;- actual - predicted results &lt;- data.frame(actual = actual, predicted = predicted, residual = residual) # Insert comment here min_xy &lt;- min(min(results$actual), min(results$predicted)) max_xy &lt;- max(max(results$actual), max(results$predicted)) # Insert comment here plot.melt &lt;- melt(results, id.vars = &quot;predicted&quot;) # Insert comment here plot.data &lt;- rbind(plot.melt, data.frame(predicted = c(min_xy, max_xy), variable = c(&quot;actual&quot;, &quot;actual&quot;), value = c(max_xy, min_xy))) # Insert comment here p &lt;- ggplot(plot, aes(x = predicted, y = value)) + geom_point(size = 2.5) + theme_bw() p &lt;- p + facet_wrap(~variable, scales = &quot;free&quot;) p 2.8.5 Exercise 3: commercial loan projection using regression 2.8.5.1 Purpose and problem This project will allow us to practice various R features using live data to support a decision regarding the provision of captive financing to customers at the beginning of this chapter. We will focus on translating regression statistics into R, plotting results, and interpreting ordinary least squares regression outcomes. As we researched how to provide captive financing and insurance for our customers we found that we needed to understand the relationships among lending rates and various terms and conditions of typical equipment financing contracts. We will focus on one question: What is the influence of terms and conditions on the lending rate of fully committed commercial loans with maturities greater than one year? The data set commloan.csv contains data from the St. Louis Federal Reserve Bank’s FRED website we will use to get some high level insights. The quarterly data extends from the first quarter of 2003 to the second quarter of 2016 and aggregates a survey administered by the St. Louis Fed. There are several time series included. Each is by the time that pricing terms Were set and by commitment, with maturities more than 365 Days from a survey of all commercial banks. Here are the definitions. rate: weighted-average effective loan rate in per cent per annum prepay: per cent of value of loans subject to prepayment penalty per annum maturity: weighted-average maturity/repricing interval in days size: average loan size in thousands USD volume: total value of loans in millions USD 2.8.6 Work Flow Prepare the data. Visit the FRED website. Include any information on the site to enhance the interpretation of results. Use read.csv to read the data into R. Be sure to set the project’s working directory where the data directory resides. Use na.omit() to clean the data. # setwd(&quot;C:/Users/Bill Foote/bookdown/bookdown-demo-master&quot;) the project directory x.data &lt;- read.csv(&quot;data/commloans.csv&quot;) x.data &lt;- na.omit(x.data) Assign the data to a variable called x.data. Examine the first and last 5 entries. Run a summary of the data set. What anomalies appear based on these procedures? Explore the data. Let’s plot the time series data using this code: require(ggplot2) require(reshape2) # Use melt() from reshape2 to build data frame with data as id and values of variables x.melted &lt;- melt(x.data[, c(1:4)], id = &quot;date&quot;) ggplot(data = x.melted, aes(x = date, y = value)) + geom_point() + facet_wrap(~variable, scales = &quot;free_x&quot;) Describe the data frame that melt() produces. Let’s load the psych library and produce a scatterplot matrix. Interpret this exploration. Analyze the data. Let’s regress rate on the rest of the variables in x.data. To do this we form a matrix of independent variables (predictor or explanatory variables) in the matrix X and a separate vector y for the dependent (response) variable rate. We recall that the 1 vector will produce a constant intercept in the regression model. y &lt;- as.vector(x.data[,&quot;rate&quot;]) X &lt;- as.matrix(cbind(1, x.data[,c(&quot;prepaypenalty&quot;, &quot;maturity&quot;, &quot;size&quot;, &quot;volume&quot;)])) head(y) head(X) Explain the code used to form y and X. Calculate the \\(\\hat{\\beta}\\) coefficients and interpret their meaning. Calculate actual and predicted rates and plot using this code. # Insert comment here require(reshape2) require(ggplot2) actual &lt;- y predicted &lt;- X%*%beta.hat residual &lt;- actual - predicted results &lt;- data.frame(actual = actual, predicted = predicted, residual = residual) # Insert comment here min_xy &lt;- min(min(results$actual), min(results$predicted)) max_xy &lt;- max(max(results$actual), max(results$predicted)) # Insert comment here plot.melt &lt;- melt(results, id.vars = &quot;predicted&quot;) # Insert comment here plot.data &lt;- rbind(plot.melt, data.frame(predicted = c(min_xy, max_xy), variable = c(&quot;actual&quot;, &quot;actual&quot;), value = c(max_xy, min_xy))) # Insert comment here p &lt;- ggplot(plot, aes(x = predicted, y = value)) + geom_point(size = 2.5) + theme_bw() p &lt;- p + facet_wrap(~variable, scales = &quot;free&quot;) p Insert explanatory comments into the code chunk to document the work flow for this plot. Interpret the graphs of actual and residual versus predicted values of rate. Calculate the standard error of the residuals, Interpret its meaning. Interpret and present results. We will produce an R Markdown document with code chunks to document and interpret our results. The format will introduce the problem to be analyzed, with sections that discuss the data to be used, and which follow the work flow we have defined. "],
["r-data-modeling.html", "Chapter 3 R Data Modeling 3.1 Imagine This 3.2 Pivot tables and Vertical Lookups 3.3 Why Functions? 3.4 Making distributions 3.5 Optimization 3.6 Estimate until morale improves… 3.7 Summary 3.8 Further Reading 3.9 Practice Sets 3.10 Project 3.11 References", " Chapter 3 R Data Modeling 3.1 Imagine This Your project team is assigned to work with the accounts receivable team. Specifically, accounts receivable is about to review a portfolio of customers from a potential acquisition. Managers would like to query data provided through due diligence during the acquisition process. Some of the questions include: What is the income risk across applicant pools? Are there differences in applicant income? Does age matter? Is there a pattern of family dependents across applicant pools? How much income per dependent? The analytics team will process the data, review its quality, and help accounts receivable answer these questions. In this chapter we will build approaches to manage such queries, including pivot tables, lookups, and the creation of new metrics from existing data. We will expand this assortment of skills into the writing of functions, such as net present value and internal rate of return, more plotting of data, working with time series data, and fitting data to probability distributions. 3.2 Pivot tables and Vertical Lookups These are, mythically at least, two of the most-used Excel features. Pivot tables are the slice and dice machine we use to partition data sets. Lookups allow us to relate one data table to another. We will explore these tools in R, here made easier and less apt to crash on large data sets. We start with some definitions. 3.2.1 Some definitions The pivot table is a data summarization tool that can automatically sort, count, total, or give the average of the data stored in one table or spreadsheet, displaying the results in a second table showing the summarized data. This tool transforms a flat table of fields with rows of data into a table with grouped row values and column header values. The specification of grouped row values and column headers can rotate the flat table’s data rows into the intersection of the row and column labels. “V” or “vertical” stands for the looking up of a value in a column. This feature allows the analyst to find approximate and exact matches between the look up value and a table value in a vertical column assigned to the look up value. A HLOOKUP function does the same lookup but for a specified row instead of a column. 3.2.2 Pivot and Parry Let’s return to the Credit Card Applicant business questions: What is the income risk across applicant pools? Are there differences in applicant income? Does age matter? Is there a pattern of dependents across applicant pools? How much income per dependent? The first step in building an analysis of the data relative to these questions is to understand the required dimensions of the data that apply to the questions. Here we would scan the table column names in the data base and look for Card status Ownership Employment CreditCard &lt;- read.csv(&quot;data/CreditCard.csv&quot;) str(CreditCard) ## &#39;data.frame&#39;: 1319 obs. of 13 variables: ## $ card : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ reports : int 0 0 0 0 0 0 0 0 0 0 ... ## $ age : num 37.7 33.2 33.7 30.5 32.2 ... ## $ income : num 4.52 2.42 4.5 2.54 9.79 ... ## $ share : num 0.03327 0.00522 0.00416 0.06521 0.06705 ... ## $ expenditure: num 124.98 9.85 15 137.87 546.5 ... ## $ owner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 1 2 1 2 1 1 2 2 1 ... ## $ selfemp : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ dependents : int 3 3 4 0 2 0 2 0 0 0 ... ## $ months : int 54 34 58 25 64 54 7 77 97 65 ... ## $ majorcards : int 1 1 1 1 1 1 1 1 1 1 ... ## $ active : int 12 13 5 7 5 1 5 3 6 18 ... ## $ state : Factor w/ 3 levels &quot;CT&quot;,&quot;NJ&quot;,&quot;NY&quot;: 3 3 3 3 3 3 3 3 3 3 ... The str() function allows us to see all of the objects in CreditCard. Next lext look at the data itself inside this object using head (for the beginning of the data). head(CreditCard, 3) ## card reports age income share expenditure owner selfemp dependents ## 1 yes 0 37.7 4.52 0.03327 124.98 yes no 3 ## 2 yes 0 33.2 2.42 0.00522 9.85 no no 3 ## 3 yes 0 33.7 4.50 0.00416 15.00 yes no 4 ## months majorcards active state ## 1 54 1 12 NY ## 2 34 1 13 NY ## 3 58 1 5 NY Knowing the structure and a sample of the data, we can build a summary of the data and review the minimum, maximum, and quartiles in each of CreditCard’s columns of data. summary(CreditCard) ## card reports age income share ## no : 296 Min. : 0.00 Min. : 0.2 Min. : 0.21 Min. :0.000 ## yes:1023 1st Qu.: 0.00 1st Qu.:25.4 1st Qu.: 2.24 1st Qu.:0.002 ## Median : 0.00 Median :31.2 Median : 2.90 Median :0.039 ## Mean : 0.46 Mean :33.2 Mean : 3.37 Mean :0.069 ## 3rd Qu.: 0.00 3rd Qu.:39.4 3rd Qu.: 4.00 3rd Qu.:0.094 ## Max. :14.00 Max. :83.5 Max. :13.50 Max. :0.906 ## expenditure owner selfemp dependents months ## Min. : 0 no :738 no :1228 Min. :0.00 Min. : 0 ## 1st Qu.: 5 yes:581 yes: 91 1st Qu.:0.00 1st Qu.: 12 ## Median : 101 Median :1.00 Median : 30 ## Mean : 185 Mean :0.99 Mean : 55 ## 3rd Qu.: 249 3rd Qu.:2.00 3rd Qu.: 72 ## Max. :3100 Max. :6.00 Max. :540 ## majorcards active state ## Min. :0.000 Min. : 0 CT:442 ## 1st Qu.:1.000 1st Qu.: 2 NJ:472 ## Median :1.000 Median : 6 NY:405 ## Mean :0.817 Mean : 7 ## 3rd Qu.:1.000 3rd Qu.:11 ## Max. :1.000 Max. :46 We immediately see an age minimum of 0.2. Either this is an anomaly, or outright error, or there is an application not quite a year old!. Let’s filter the data for ages greater than 18 to be safe. ccard &lt;- CreditCard[CreditCard$age &gt;= 18, ] In the filter, the comma means keep data on applicants only at or in excess of 18 years of age. When we leave the column empty, it means apply this filter across all columns. We next review the distribution of ages of applicants to be sure our filter does the job properly. The function hist() builds a simple frequency histogram to visualize this data. hist(ccard$age) 3.2.3 Try this exercise What is the basic design of this inquiry? Business questions? Dimensions? Taxonomy and metrics? To answer 1 and 2 we have business questions along the lines of indicator variables: Card issued (card) Own or rent (owner) Self-employed or not (selfemp) For 3 our basic taxonomy is: For each card issued…in New York …and for each owner… …who is employed… What are the range of income, average dependents, age, and income per dependent? Here is the basic 3 step pivot table design. We should check if we have installed the dplyr package into the R environment. # install.packages(&quot;dplyr&quot;) if not already library(dplyr) # set decimal places and turn off scientific notation options(digits = 4, scipen = 99999999) ## 1: filter to keep three states. pivot_table &lt;- filter(ccard, state %in% &quot;NY&quot;) ## 2: set up data frame for by-group processing. pivot_table &lt;- group_by(pivot_table, card, owner, selfemp) ## 3: calculate the three summary metrics options(dplyr.width = Inf) ## to display all columns pivot_table &lt;- summarise(pivot_table, income.cv = sd(income)/mean(income), age.avg = mean(age), income.per.dependent = sum(income)/sum(dependents)) We then visualize results in a table. Here we use knitr, which is a package that powers rmarkdown. The function kable() is short for “knitr table.” knitr::kable(pivot_table) card owner selfemp income.cv age.avg income.per.dependent no no no 0.4942 31.92 3.646 no no yes 0.5653 26.39 2.852 no yes no 0.3756 36.02 2.158 no yes yes NA 53.33 Inf yes no no 0.3299 28.09 5.314 yes no yes 0.4368 37.45 7.062 yes yes no 0.5520 36.80 3.155 yes yes yes 0.5032 41.92 3.195 3.2.4 Now to VLOOKUP Let’s start with a different data set. We load this IBRD (World Bank) data that has The variable life.expectancy is the average life expectancy for each country from 2009 through 2014. The variable sanitation is the percentage of population with direct access to sanitation facilities. le &lt;- read.csv(&quot;data/life_expectancy.csv&quot;, header = TRUE, stringsAsFactors = FALSE) sa &lt;- read.csv(&quot;data/sanitation_.csv&quot;, header = TRUE, stringsAsFactors = FALSE) Always we look at the first few records. head(le) ## country years.life.expectancy.avg ## 1 Afghanistan 46.62 ## 2 Albania 71.12 ## 3 Algeria 61.82 ## 4 Angola 41.66 ## 5 Antigua and Barbuda 69.81 ## 6 Arab World 60.93 head(sa) ## country sanitation.avg ## 1 Afghanistan 25.40 ## 2 Albania 85.36 ## 3 Algeria 84.22 ## 4 American Samoa 61.73 ## 5 Andorra 100.00 ## 6 Angola 36.01 The job here is to join sanitation data with life expectancy data, by country. In Excel we would typically use a VLOOKUP(country, sanitation, 2, FALSE) statement. In this statement country is the value to be looked up, for example, “Australia”. The variable sanitation is the range of the sanitation lookup table of two columns of country and sanitation data, for example, B2:C104 in Excel. The 2 is the second column of the sanitation lookup table, for example column C. FALSE means don’t find an exact match. In R we can use the merge() function. life_sanitation &lt;- merge(le[, c(&quot;country&quot;, &quot;years.life.expectancy.avg&quot;)], sa[, c(&quot;country&quot;, &quot;sanitation.avg&quot;)]) The whole range of countries is populated by the lookup. head(life_sanitation, 3) ## country years.life.expectancy.avg sanitation.avg ## 1 Afghanistan 46.62 25.40 ## 2 Albania 71.12 85.36 ## 3 Algeria 61.82 84.22 3.2.5 Try this exercise We will load yet another data set on house prices. Suppose we work for a housing developer like Toll Brothers (NYSE: TOL) and want to allocate resources to marketing and financing the building of luxury homes in major US metropolitan areas. We have data for one test market. hprice &lt;- read.csv(&quot;data/hprice.csv&quot;) Let’s look at the available data: summary(hprice) ## ID Price SqFt Bedrooms ## Min. : 1.0 Min. : 69100 Min. :1450 Min. :2.00 ## 1st Qu.: 32.8 1st Qu.:111325 1st Qu.:1880 1st Qu.:3.00 ## Median : 64.5 Median :125950 Median :2000 Median :3.00 ## Mean : 64.5 Mean :130427 Mean :2001 Mean :3.02 ## 3rd Qu.: 96.2 3rd Qu.:148250 3rd Qu.:2140 3rd Qu.:3.00 ## Max. :128.0 Max. :211200 Max. :2590 Max. :5.00 ## Bathrooms Offers Brick Neighborhood ## Min. :2.00 Min. :1.00 No :86 East :45 ## 1st Qu.:2.00 1st Qu.:2.00 Yes:42 North:44 ## Median :2.00 Median :3.00 West :39 ## Mean :2.44 Mean :2.58 ## 3rd Qu.:3.00 3rd Qu.:3.00 ## Max. :4.00 Max. :6.00 Our business questions include: What are the most valuable (higher price) neighborhoods? What housing characteristics maintain the most housing value? First, where and what are the most valuable houses? One way to answer this is to build a pivot table. Next we pivot the data and build metrics into the query. We will use the mean() and standard deviation sd() functions to help answer our questions. #library(dplyr) ## 1: filter to those houses with fairly high prices pivot_table &lt;- filter(hprice, Price &gt; 99999) ## 2: set up data frame for by-group processing pivot_table &lt;- group_by(pivot_table, Brick, Neighborhood) ## 3: calculate the summary metrics options(dplyr.width = Inf) ## to display all columns pivot_table &lt;- summarise(pivot_table, Price_avg = mean(Price), Price_cv = sd(Price)/mean(Price), SqFt_avg = mean(SqFt), Price.per_SqFt = mean(Price)/mean(SqFt)) Then we visualize in a table. knitr::kable(pivot_table) Brick Neighborhood Price_avg Price_cv SqFt_avg Price.per_SqFt No East 121096 0.1252 2020 59.96 No North 115307 0.0940 1958 58.88 No West 148230 0.0912 2073 71.49 Yes East 135468 0.0978 2031 66.70 Yes North 118457 0.1308 1857 63.78 Yes West 175200 0.0930 2091 83.78 Based on this data set from one metropolitan area, the most valuable properties (fetching the highest average price and price per square foot) are made of brick in the West neighborhood. Brick or not, the West neighborhood also seems have the lowest relative variation in price. Now for something different: functions. 3.3 Why Functions? We will encapsulate several operations into a reusable storage device called a function. The usual suspects and candidates for the use of functions are: Data structures rack together related values into one object. Functions group related commands into one object. In both cases the logic and coding is easier to understand, easier to work with, easier to build into larger things, and less prone to breaches of plain-old stubby finger breaches of operational safety and security. For example, here is an Excel look-alike NPV function. We enter this into a code-chunk in an R markdown file or directly into the console to store the function into the current R environment. Once that is done, we now have a new function we can use like any other function. ## Net Present Value function ## Inputs: vector of rates (rates) with 0 as the first rate for time 0, vector of cash flows (cashflows) ## Outputs: scalar net present value NPV_1 &lt;- function(rates, cashflows) { NPV &lt;- sum(cashflows/(1 + rates)^(seq_along(cashflows)-1)) return(NPV) } The structure of a function includes: A header describes the function along with inputs and outputs. Here we use comment characters # to describe and document the function. A definition names the function and identify the interface of inputs and outputs to the programming environment. The name is like a variable and is assigned to function(), where inputs are defined. Code statements take the inputs from the definition and program the tasks, logic, and decisions in the function’s work flow into output. An output statement releases the function’s results for use in other code statements outside of the function’s “mini-verse.” We use the formal return() function to identify the output that the function will produce. If we did not use return(), then R will us the last assigned variable as the output of the function. In this example We generate data internal to the function: We use seq_along to generate time index of cashflows. We must subtract 1 from this sequence as starting cashflow is time 0. We generate a net present value directly in one line of code. Our functions get used just like the built-in ones, for example, mean(). Let’s define rates and cashflows as vector inputs to the NPV.1() function and run this code. The interpretation of rates here is the per period rate a cash flow can grow to in so many periods. A rate of 0.04 assigned to the third period means that a unit of currency today can grow into $(1 + 0.04)(1 + 0.04)(1 + 0.04)=(1 + 0.04)^3=1.1249 units of currency in three periods. With this interpretation we calculate the net present value of cash flows next. rates &lt;- c(0.00, 0.08, 0.06, 0.04) ## first rate is always 0.00 cashflows &lt;- c(-100, 200, 300, 10) NPV_1(rates, cashflows) ## [1] 361.1 We go back to the declaration and look at the parts: ## Net Present Value function ## Inputs: vector of rates (rates) with 0 as the first rate for time 0, vector of cash flows (cashflows) ## Outputs: scalar net present value NPV_1 &lt;- function(rates, cashflows) { NPV &lt;- sum(cashflows/(1 + rates)^(seq_along(cashflows)-1)) return(NPV) } Interfaces refer to these components: inputs or arguments outputs or return value Calls other functions sum, seq_along(), operators /, +, ^ and - . We can also call other functions we’ve written. We use return() to explicitly say what the output is. This is simply good documentation. Alternately, a function will return the last evaluation. Comments, that is, lines that begin with #, are not required by R, but are always a good and welcome idea that provide a terse description of purpose and direction. Initial comments should also include a listing of inputs, also called “arguments,” and outputs. 3.3.1 What should be a function? Functions should be written for code we are going to re-run, especially if it will be re-run with changes in inputs. They can also be code chunks we keep highlighting and hitting return on. We often write functions for code chunkswhich are small parts of bigger analyses. In the next redition of irr.1 we improve the code with named and default arguments. ## Internal Rate of Return (IRR) function ## Inputs: vector of cash flows (cashflows), scalar interations (maxiter) ## Outputs: scalar net present value IRR_1 &lt;- function(cashflows, maxiter=1000) { t &lt;- seq_along(cashflows)-1 ## rate will eventually converge to IRR f &lt;- function(rate)(sum(cashflows/(1+rate)^t)) ## use uniroot function to solve for root (IRR = rate) of f = 0 ## c(-1,1) bounds solution for only positive or negative rates ## select the root estimate return(uniroot(f, c(-1,1), maxiter = maxiter)$root) } Here the default argument is maxiter which controls the number of iterations. At our peril we can eliminate this argument if we want. This illustrates yet another need for functions: we can put error and exception logic to handle somtimes fatal issues our calculations might present. Thw uniroot() function also bounds the roots between -1 and 1 using the concatenate function c(-1,1). Here are the cashflows for a 3% coupon bond bought at a hefty premium. cashflows &lt;- c(-150, 3, 3, 3, 3, 3, 3, 3, 103) IRR_1(cashflows) ## [1] -0.02554 IRR_1(cashflows, maxiter = 100) ## [1] -0.02554 We get a negative IRR or yield to maturity on this net present value = 0 calculation. A not so trivial interpretation of negative yields, and even negative prices is the existence of over-supply of a good or service. 3.3.2 Shooting trouble Problem: We see “odd”\" behavior when arguments aren’t as we expect. NPV_1(c(0.10, 0.05), c(-10, 5,6,100)) ## [1] 86.1 We do get a result, but… What does it mean? What rates correspond with what cashflows? Here the function calculates a net present value. But the analyst entered two rates for four cash flows. Solution: We put sanity checks into the code. Let’s use the stopifnot(some logical statment) is TRUE. ## Net Present Value function ## Inputs: vector of rates (rates) with 0 as the first rate for time 0, vector of cash flows (cashflows), length of rates must equal length of cashflows ## Outputs: scalar net present value NPV_2 &lt;- function(rates, cashflows) { stopifnot(length(rates) == length(cashflows)) NPV &lt;- sum(cashflows/(1 + rates)^(seq_along(cashflows)-1)) return(NPV) } Here are some thoughts about stopifnot TRUE error handling Arguments to stopifnot() are a series of logical expressions which should all be TRUE. Execution halts, with error message, at first FALSE. NPV_2(c(0.10, 0.05), c(-10, 5, 6,100)) Hit (not too hard!) the Escape key on your keyboard, This will take you out of Browse[1]&gt; mode and back to the console prompt &gt;. 3.3.3 What the function can see and do Each function has its own environment. Names here will override names in the global environment. The function’s internal environment starts with the named arguments. Assignments inside the function only change the internal environment. If a name is not defined in the function, the function will look for this name in the environment the function gets called from. 3.3.4 Try this … Your company is running a 100 million pound sterling project in the EU. You must post 25% collateral in a Landesbank using only high-quality government securities. You find a high-quality gilt fund that will pay 1.5% (coupon rate) annually for three years. Some questions for analysis How much would you pay for this collateral if the rate curve (yield to maturity of cash flows) is (from next year on…) rates &lt;- c(-0.001, 0.002, 0.01) Suppose a bond dealer asks for 130% of notional collateral value for this bond. What is the yield on this transaction (IRR)? Would you buy it? What is the return on this collateral if you terminate the project in one year and liquidate the collateral (i.e., sell it for cash) if the yield shifts down by 0.005? This is a “parallel” shift, which is finance for: “take each rate and deduct 0.005.” To get at these requirements we will build rates and cash flows across the 3-year time frame, remembering our previous work. (rates &lt;- c(0, rates)) ## [1] 0.000 -0.001 0.002 0.010 collateral_periods &lt;- 3 collateral_rate &lt;- 0.25 collateral_notional &lt;- collateral_rate * 100 coupon_rate &lt;- 0.015 cashflows &lt;- rep(coupon_rate * collateral_notional, collateral_periods) cashflows[collateral_periods] &lt;- collateral_notional + cashflows[collateral_periods] (cashflows &lt;- c(0, cashflows)) ## [1] 0.000 0.375 0.375 25.375 What just happened? We appended a 0 to the rate schedule so we can use the NPV.2 function. We then parameterized the term sheet (terms of the collateral transaction), We used rep() to form coupon cash flows. Then we added notional value repayment to the last cash flow. Now we can calculate the present value of the bond using NPV_2. (Value_0 &lt;- NPV_2(rates, cashflows)) ## [1] 25.38 The answer is 25.378 million pounds sterling or Value_0 / collateral_notional times the notional value. The yield to maturity averages the forward rates across the bond cash flows. A “forward rate” is the rate per period we would expect to earn each period for a specified number of periods forward. This is one interpretation of the Internal Rate of Return (“IRR”). cashflows_IRR &lt;- cashflows collateral_ask &lt;- 130 cashflows_IRR[1] &lt;- -(collateral_ask/100) * collateral_notional ## mind the negative sign! (collateral_IRR_1 &lt;- IRR_1(cashflows_IRR)) ## [1] -0.07112 You end up paying over 7% per annum for the privilege of owning this bond! You call up the European Central Bank, report this overly hefty haircut on your project. You send out a request for proposal to other bond dealers. They come back with an average asking price of 109 (109% of notional). cashflows_IRR &lt;- cashflows collateral_ask &lt;- 109 cashflows_IRR[1] &lt;- -(collateral_ask/100) * collateral_notional (collateral_IRR_1 &lt;- IRR_1(cashflows_IRR)) ## [1] -0.01416 That’s more like it: about 140 basis points (1.41% x 100 basis points per percentage) cost (negative sign). Let’s unwind the project, and the collateral transaction, in 1 year. Let’s suppose the yield curve in 1 year has parallel shifted down by 0.005. rate_shift &lt;- -0.005 rates_1 &lt;- c(0, rates[-2]) + rate_shift cashflows_1 &lt;- c(0, cashflows[-2]) (Value_1 &lt;- NPV_2(rates_1, cashflows_1)) ## [1] 25.38 (collateral_return_1 &lt;- Value_1 / (-cashflows_IRR[1]) - 1) ## [1] -0.06879 This results ooks much more than a break-even return on the collateral transaction: (collateral_gainloss &lt;- collateral_notional * collateral_return_1) * 1000000 ## [1] -1719807 ## adjust for millions of euros That’s probably some executive’s salary (with stock grants!)…in pounds sterling. 3.3.5 Mind the Interface! Interfaces mark out a controlled inner environment for our code; They allow us to interact with the rest of the system only at the interface. Arguments explicitly give the function all the information the function needs to operate and reduces the risk of confusion and error. There are exceptions such as true universals like \\(\\pi\\). Likewise, output should only be through the return value. Let’s build (parametric) distributions next. 3.4 Making distributions As always, let’s load some data, this time from the Bureau of Labor Statistics (BLS) and load the export-import price index whose description and graph are at http://data.bls.gov/timeseries/EIUIR?output_view=pct_1mth. We look up the symbols “EIUIR” and “EIUIR100” and download a text file that we then convert to a comma separated variable or csv file in Excel. We deposit the csv file in a directory and read it into a variable called EIUIR. require(xts) require(zoo) EIUIR &lt;- read.csv(&quot;data/EIUIR.csv&quot;) head(EIUIR) ## Date Value ## 1 2006-01-01 113.7 ## 2 2006-02-01 112.8 ## 3 2006-03-01 112.7 ## 4 2006-04-01 115.1 ## 5 2006-05-01 117.2 ## 6 2006-06-01 117.3 xmprice &lt;- na.omit(EIUIR) ## to clean up any missing data str(xmprice) ## &#39;data.frame&#39;: 131 obs. of 2 variables: ## $ Date : Factor w/ 131 levels &quot;2006-01-01&quot;,&quot;2006-02-01&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Value: num 114 113 113 115 117 ... We might have to have installed separately the xts and zoo packages that handle time series data explicitly. The str() function indicates that the Value column in the data frame contains the export price series. We then compute the natural logarithm of prices and calculate the differences to get rates of growth from month to month. A simple plot reveals aspects of the data series to explore. xmprice_r &lt;- as.zoo(na.omit((diff(log(xmprice$Value))))) ## compute rates head(xmprice_r) ## 1 2 3 4 5 6 ## -0.0079471 -0.0008869 0.0210719 0.0180806 0.0008529 0.0076433 plot(xmprice_r, type = &quot;l&quot;, col = &quot;blue&quot;, xlab = &quot;Date&quot;, main = &quot;Monthly 2/2000-9/2016&quot;) We further transform the data exploring the absolute value of price rates. This is a first stab at understanding the clustering of volatility in financial-economic time series, a topic to which we will return. xmprice_r_df &lt;- data.frame(xmprice_r, Date = index(xmprice_r), Rate = xmprice_r[, 1], Rate_abs = abs(xmprice_r[,1])) head(xmprice_r_df) ## xmprice_r Date Rate Rate_abs ## 1 -0.0079471 1 -0.0079471 0.0079471 ## 2 -0.0008869 2 -0.0008869 0.0008869 ## 3 0.0210719 3 0.0210719 0.0210719 ## 4 0.0180806 4 0.0180806 0.0180806 ## 5 0.0008529 5 0.0008529 0.0008529 ## 6 0.0076433 6 0.0076433 0.0076433 str(xmprice_r_df) ## &#39;data.frame&#39;: 130 obs. of 4 variables: ## $ xmprice_r: num -0.007947 -0.000887 0.021072 0.018081 0.000853 ... ## $ Date : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Rate : num -0.007947 -0.000887 0.021072 0.018081 0.000853 ... ## $ Rate_abs : num 0.007947 0.000887 0.021072 0.018081 0.000853 ... We can achieve a “prettier” plot with the ggplot2 package. In the ggplot statements we use aes, “aesthetics”, to pick x (horizontal) and y (vertical) axes. The added (+) geom_line is the geometrical method that builds the line plot. library(ggplot2) ggplot(xmprice_r_df, aes(x = Date, y = Rate)) + geom_line(colour = &quot;blue&quot;) Let’s try a bar graph of the absolute value of price rates. We use geom_bar to build this picture. library(ggplot2) ggplot(xmprice_r_df, aes(x = Date, y = Rate_abs)) + geom_bar(stat = &quot;identity&quot;, colour = &quot;green&quot;) 3.4.1 Try this exercise Let’s overlay returns (geom_line) and their absolute value geom_bar. ggplot declares the canvas using the price data frame. aes establishes the data series to be used to generate pictures. geom_bar builds a bar chart. geom_line overlays the bar chart with a line chart. By examining this chart, what business questions about your Univeral Export-Import Ltd supply chain could this help answer? Why is this helpful? library(ggplot2) ggplot(xmprice_r_df, aes(Date, Rate_abs)) + geom_bar(stat = &quot;identity&quot;, colour = &quot;darkorange&quot;) + geom_line(data = xmprice_r_df, aes(Date, Rate), colour = &quot;blue&quot;) The plot goes a long way to answering the question: When supply and demand tightens, does price volatility cluster? Here is an interpretation based on the data we see in the plot. If we are selling, we would experience strong swings in demand and thus in revenue at the customer fulfillment end of the chain. If we are buying, we would experience strong swings in cost and input product utilization at the procurement end of the chain. For the financial implications: we would have a tough time making the earnings we forecast to the market. 3.4.2 Picture this We import goods as input to our manufacturing process. We might want to know the odds that a very high export-import rate might occur. We can plot a cumulative distribution function (cdf or CDF) call. we can build this plot using the stat_ecdf() function in ggplot2. #library(ggplot2) ggplot(xmprice_r_df, aes(Rate)) + stat_ecdf(colour = &quot;blue&quot;) 3.4.3 Try another exercise Suppose the procurement team’s delegation of authority remit states: “Procurement may approve input invoices when there is only a 5% chance that prices will rise any higher than the price rate associated with that tolerance. If input prices do rise higher than the tolerable rate, you must get divisional approval.” Plot a vertical line to indicate the maximum tolerable rate for procurement using the BLS EIUR data from 2000 to the present. Use r_tol &lt;- quantile(xmprice_r_df$Rate, 0.95) to find the tolerable rate. Use + geom_vline(xintercept = r_tol) in the CDF plot. We can implement these requirements with the following code. #library(ggplot2) r_tol_pct &lt;- 0.95 r_tol &lt;- quantile(xmprice_r_df$Rate, r_tol_pct) r_tol_label &lt;- paste(&quot;Tolerable Rate = &quot;, round(r_tol, 4)) ggplot(xmprice_r_df, aes(Rate)) + stat_ecdf(colour = &quot;blue&quot;, size = 1.5) + geom_vline(xintercept = r_tol, colour = &quot;red&quot;, size = 1.5) + geom_hline(yintercept = 0.95, color = &quot;red&quot;, size = 1.5, linetype=&quot;dotted&quot;) + annotate(&quot;text&quot;, x = r_tol-.05 , y = 0.75, label = r_tol_label, colour = &quot;darkred&quot;) + xlab(&quot;price inflation&quot;) + ylab(&quot;cumulative probability&quot;) + ggtitle(&quot;Price inflation risk tolerance&quot;) This may be a little more than we bargained for originally. We used the paste and round (to two, 2, decimal places) functions to make a label. We made much thicker lines (size = 1.5). At price inflation of 2.6348% we drew a line with the vertical line geom_vline() annotated the line with text, and intersected the cumulative probability plot (stat_ecdf()) with a horizontal dotteed line geom_hline(), labeled the axes, and inserted a title. How might we interpret this plot? The y-axis is the cumulative probability that a rate of price inflation on the x-axis occurs. The risk management policy states that any inflation above an occurrence rate of 95% must be managed. This is the red horizontal dotted line. Recall that this tolerance is for goods and services purchased. The 95th quantile of the price inflation series is 2.6348% Now that we have made some distributions out of live data, let’s estimate the parameters of specific distributions that might be fit to that data. 3.5 Optimization The optimization we will conduct here helps us to find the distribution that best fits the data. We will use results from optimization to simulate that data to help us make decisions prospectively. There are many distributions in R: ?distributions will tell you all about them. If name is the name of a distribution (e.g., norm for “normal”), then dname = the probability density (if continuous) or probability mass function of name (pdf or pmf), think “histogram” pname = the cumulative probability function (CDF), think “s-curve” qname = the quantile function (inverse to CDF), “think tolerance line” rname = draw random numbers from name (first argument always the number of draws), think whatever you want…it’s kind of random And ways to write your own (like the pareto distribution we use in finance) 3.5.1 Try this exercise Suppose the EIUR price series is the benchmark in several import contracts you write as the procurement officer of your organization. Your concern is with volatility. Thus you think that you need to simulate the size of the price rates, whatever direction they go in. Draw the histogram of the absolute value of price rates. #library(ggplot2) r_tol &lt;- quantile(xmprice_r_df$Rate, 0.95) r_tol_label &lt;- paste(&quot;Tolerable Rate = &quot;, round(r_tol, 4)) ggplot(xmprice_r_df, aes(Rate_abs)) + geom_histogram(fill = &quot;cornsilk&quot;, colour = &quot;grey60&quot;) + geom_density() + geom_vline(xintercept = r_tol, colour = &quot;red&quot;, size = 1.5) + annotate(&quot;text&quot;, x = .055 , y = 30, label = r_tol_label, colour = &quot;darkred&quot;) + xlab(&quot;price inflation volatility&quot;) + ylab(&quot;frequency&quot;) + ggtitle(&quot;Price inflation risk tolerance&quot;) This series is right-skewed and thickly-tailed. We will use this function to pull several of the statistics calculations together. ## r_moments function ## INPUTS: r vector ## OUTPUTS: list of scalars (mean, sd, median, skewness, kurtosis) data_moments &lt;- function(data){ require(moments) mean_r &lt;- mean(data) sd_r &lt;- sd(data) median_r &lt;- median(data) skewness_r &lt;- skewness(data) kurtosis_r &lt;- kurtosis(data) result &lt;- data.frame(mean = mean_r, std_dev = sd_r, median = median_r, skewness = skewness_r, kurtosis = kurtosis_r) return(result) } We will need to install.packages(\"moments\") to make this function operate. We then run these sentences. ans &lt;- data_moments(xmprice_r_df$Rate_abs) ans &lt;- round(ans, 4) knitr::kable(ans) mean std_dev median skewness kurtosis 0.0109 0.0117 0.0074 2.523 12.15 As we visually surmised, the series is right-skewed and very thickly tailed. One interrpretation of teh thick tails is that price inflation volatility will beget further volatility. This is what is known as a stylized fact of many markets. For the manager it means that when volatility of purchased goods prices is low, the manager may expect that low volatility will persist until a major and substantive bit of news occurs. When the innovative news occurs then volatility will be pumped up and persist until the news cycle runs its course. This stylized fact may indicate that the gamma and pareto functions may help us to describe these series and prepare us for simulations, estimation, and inference. Next we will make liberal use of the fitdistr function from MASS and come back to this moments function. 3.6 Estimate until morale improves… Given data about an important series like export-import price changes, we might want to know how probable changes will occur in the future. We can estimate the parameters of anticipated movements using various probability distributions such as the gamma and pareto distributions. Of particular interest to managers is the simulation of the tail of the series, where the kurtotic thickness presides. Managers might choose to manage volatility in the tails using insurance, contracting, hedging, or even elimination of the problem altogether by, for example, moving facilities to other regions. We will try one method that works often enough in practice to estimate distributional parameters, the Method of Moments (“MM” or, more affectionately, “MOM”). This technique estimates the distribution parameters such that the moments of the data match the moments of the distribution. Other methods include: fitdistr: Let the opaque box do the job for you; look at the package MASS which uses the “maximum likelihood” approach in the fitdistr estimating function (like lm for regression). fitdistrplus: For the more adventurous analyst, this package contains several methods, including MM, to get the job done. Suppose we believe that absolute price rates somehow follow a gamma distribution. You can look up this distribution easily enough in Wikipedia’s good article on the subject. Behind managerial scenes, we can model potential losses with gamma severity function that allows for skewness and thick tails. We can specify the gamma distribution with shape, \\(\\alpha\\), and scale, \\(\\beta\\), parameters. We will find in operational loss analysis that this distribution is especially useful for time-sensitive losses. It turns out We can specify the shape and scale parameters using the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\) of the random severities, \\(X\\). The scale parameter is \\[ \\beta = \\sigma^2 / \\mu, \\] and shape parameter, \\[ \\alpha = \\mu^2 / \\sigma^2. \\] The distribution itself is defined as \\[ f(x; alpha, \\beta) = \\frac{\\beta^{\\alpha}x^{\\alpha-1}e^{-x\\beta}}{\\Gamma(\\alpha)}, \\] where, to have a complete statement, \\[ \\Gamma(x) = \\int_{0}^{\\infty} x^{t-1} e^{-x} dx. \\] Let’s finally implement our method into R. First, we will load a cost sample and calculate moments and gamma parameters: cost &lt;- read.csv(&quot;data/cost.csv&quot;) cost &lt;- abs(cost$x) cost_moments &lt;- data_moments(cost) cost_mean &lt;- cost_moments$mean cost_sd &lt;- cost_moments$std_dev (cost_shape &lt;- cost_mean^2/cost_sd^2) ## [1] 19.07 (cost_scale &lt;- cost_sd^2/cost_mean) ## [1] 0.5576 gamma_start &lt;- c(cost_shape, cost_scale) When performing these calculations, be sure to load the function data_moments() and moments library into the workspace. Second, we can use fitdistr from the Mass package to estimate the gamma parameters alpha and beta. library(MASS) fit_gamma_cost &lt;- fitdistr(cost, &quot;gamma&quot;) fit_gamma_cost ## shape rate ## 20.300 1.910 ## ( 2.373) ( 0.226) Here we get a shape and rate parameter estimation with the standard deviations of these estimates in the parentheses below the estimates. Third, we construct the ratio of estimates to the standard error of estimates. This computes the number of standard deviations away from zero the estimates are. If they are “far” enough away from zero, we have reason to reject the null hypothesis that the estimates are no different from zero. (cost_t &lt;- fit_gamma_cost$estimate / fit_gamma_cost$sd) ## shape rate ## 8.555 8.450 knitr::kable(cost_t) x shape 8.555 rate 8.450 Nice…but we also note that the scale parameter is fit_gamma_cost$estimate[2] / gamma_start[2] = 3.4247 times the moment estimates above. Suppose we believe that the method of moments works better for management decisions about cost. Also we believe that in the next quarter average and standard deviation of costs will follow history. Our risk management policy states that any cost volatility above the 95 quantile must be managed by the executive vice president of manufacturing, well above our pay grade. Using the gamma function how can we simulate this threshold? We start by loading up the paramters from the absolute value of cost. cost_moments &lt;- data_moments(cost) cost_mean &lt;- cost_moments$mean cost_sd &lt;- cost_moments$std_dev (cost_shape &lt;- cost_mean^2/cost_sd^2) ## [1] 19.07 (cost_scale &lt;- cost_sd^2/cost_mean) ## [1] 0.5576 gamma_start &lt;- c(cost_shape, cost_scale) Next we simply calculate the 95th quantile and locate it on a box plot of absolute values of costs. cost_95 &lt;- qgamma(0.95, cost_shape, cost_scale) # for comparison to the gamma indicated quantile cost_95_hist &lt;- quantile(cost, 0.95) cost_95_label &lt;- paste0(&quot;gamma anticipated 95th quantile = &quot;, round(cost_95, 4)) cost_95_hist_label &lt;- paste0(&quot;gamma anticipated 95th quantile = &quot;, round(cost_95_hist, 4)) cost_abs_df &lt;- data.frame(abs_cost = cost, name = &quot;cost volatility&quot;) ggplot(cost_abs_df, aes(x = name, y = abs_cost)) + geom_boxplot(outlier.colour=&quot;black&quot;, outlier.shape=16, outlier.size=2, notch=TRUE) + geom_jitter(shape=16, position=position_jitter(0.1)) + geom_hline(yintercept = cost_95, linetype = &quot;dotted&quot;, size = 1.25, color = &quot;red&quot;) + geom_hline(yintercept = cost_95_hist, linetype = &quot;dotted&quot;, size = 1.25, color = &quot;red&quot;) + annotate(&quot;text&quot;, x = 0.75, y = cost_95 - 2, label = cost_95_label) + annotate(&quot;text&quot;, x = 0.75, y = cost_95_hist + 2, label = cost_95_hist_label) The boxplot uses a notch to indicate the confidence interval for the median. The jitter allows us to see the original data on the same plot. But the gamma function exercise seems way off! The maximum historical cost volatility is only 20.5. But that is the point. If management believes that costs are so distributed, then a large cost variance might be anticipated. 3.6.1 Try this exercise Let’s use the export-input price series rates and the t distribution instead of the gamma. First, we calculate the moments (mean, etc.). rate &lt;- xmprice_r_df$Rate rate_moments &lt;- data_moments(rate) (rate_mean &lt;- rate_moments$mean) ## [1] 0.0004596 (rate_sd &lt;- rate_moments$std_dev) ## [1] 0.01602 Second, we use fitdistr from the Mass package to estimate the parameters of the t distribution. fit_t_rate &lt;- fitdistr(rate, &quot;t&quot;, hessian = TRUE) fit_t_rate ## m s df ## 0.001792 0.009833 2.888001 ## (0.001059) (0.001132) (0.843729) Third, we infer if we did a good job or not. The null hypothesis is that the parameters are no different than zero (\\(H_0\\)). We calculate t statistics to approximate the mapping of parameter estimates to a dimensionless scale that will compute the number of standard deviations from the null hypothesis that the parameters are just zero and of no further use. The hessian = TRUE parameter allows us to view the hessian matrix, the diagonals of which contain the variances of the estimated parameters. (rate_tstat &lt;- fit_t_rate$estimate / fit_t_rate$sd) ## m s df ## 1.692 8.689 3.423 Nice…but that location parameter is a bit low relative to the moment estimate. What else can we do? Simulate the estimated results and see if, at least, skewness and kurtosis line up with the moments. 3.7 Summary We used our newly found ability to write functions and built insightful pictures of distributions. We also ran nonlinear (gamma and t-distributions are indeed very nonlinear) regressions using a package and the method of moments. All of this we did to answer critical business questions. More specifically we waded into: Excel look alike processes: Pivot tables and VLOOKUP Excel look alike functions Graphics to get insights into distributions Estimating parameters of distribution Goodness of fit 3.8 Further Reading Teetor’s various chapters have much to guide us in the writing of functions and the building of expressions. Present value and internal rate of return can be found in Brealey et al. (???). Use of ggplot2 (Wickham et al. 2019) in this chapter relies heavily on Chang (2013). 3.9 Practice Sets These practice sets reference materials developed in this chapter. We will explore new problems and data with models, R packages, tables, and plots worked out already in the chapter. 3.9.1 Set A In this set we will build a data set using filters and if and diff statements. We will then answer some questions using plots and a pivot table report. We will then review a function to house our approach in case we would like to run the same analysis on other data sets. 3.9.1.1 Problem Supply chain managers at our company continue to note we have a significant exposure to heating oil prices (Heating Oil No. 2, or HO2), specifically New York Harbor. The exposure hits the variable cost of producing several products. When HO2 is volatile, so is earnings. Our company has missed earnings forecasts for five straight quarters. To get a handle on Brent we download this data set and review some basic aspects of the prices. # Read in data HO2 &lt;- read.csv(&quot;data/nyhh02.csv&quot;, header = T, stringsAsFactors = F) # stringsAsFactors sets dates as character type head(HO2) HO2 &lt;- na.omit(HO2) ## to clean up any missing data str(HO2) # review the structure of the data so far 3.9.1.2 Questions What is the nature of HO2 returns? We want to reflect the ups and downs of price movements, something of prime interest to management. First, we calculate percentage changes as log returns. Our interest is in the ups and downs. To look at that we use if and else statements to define a new column called direction. We will build a data frame to house this analysis. # Construct expanded data frame return &lt;- as.numeric(diff(log(HO2$DHOILNYH))) * 100 size &lt;- as.numeric(abs(return)) # size is indicator of volatility direction &lt;- ifelse(return &gt; 0, &quot;up&quot;, ifelse(return &lt; 0, &quot;down&quot;, &quot;same&quot;)) # another indicator of volatility date &lt;- as.Date(HO2$DATE[-1], &quot;%m/%d/%Y&quot;) # length of DATE is length of return +1: omit 1st observation price &lt;- as.numeric(HO2$DHOILNYH[-1]) # length of DHOILNYH is length of return +1: omit first observation HO2.df &lt;- na.omit(data.frame(date = date, price = price, return = return, size = size, direction = direction)) # clean up data frame by omitting NAs str(HO2.df) We can plot with the ggplot2 package. In the ggplot statements we use aes, “aesthetics”, to pick x (horizontal) and y (vertical) axes. Use group =1 to ensure that all data is plotted. The added (+) geom_line is the geometrical method that builds the line plot. Let’s try a bar graph of the absolute value of price rates. We use geom_bar to build this picture. Now let’s build an overlay of return on size. Let’s dig deeper and compute mean, standard deviation, etc. Load the data_moments() function. Run the function using the HO2.df$return subset and write a knitr::kable() report. Let’s pivot size and return on direction. What is the average and range of returns by direction? How often might we view positive or negative movements in HO2? 3.9.2 Set B We will use the data from the previous set to investigate the distribution of returns we generated. This will entail fitting the data to some parametric distributions as well as plotting and building supporting data frames. 3.9.2.1 Problem We want to further characterize the distribution of up and down movements visually. Also we would like to repeat the analysis periodically for inclusion in management reports. 3.9.2.2 Questions How can we show the differences in the shape of ups and downs in HO2, especially given our tolerance for risk? Let’s use the HO2.df data frame with ggplot2 and the cumulative relative frequency function stat_ecdf. How can we regularly, and reliably, analyze HO2 price movements? For this requirement, let’s write a function similar to data_moments. Let’s test HO2_movement(). Morale: more work today (build the function) means less work tomorrow (write yet another report). Suppose we wanted to simulate future movements in HO2 returns. What distribution might we use to run those scenarios? Here, let’s use the MASS package’s fitdistr() function to find the optimal fit of the HO2 data to a parametric distribution. 3.9.3 Practice Set Debrief List the R skills needed to complete these practice sets. What are the packages used to compute and graph results. Explain each of them. How well did the results begin to answer the business questions posed at the beginning of each practice set? 3.10 Project 3.10.1 Background Your company uses natural gas as a major input to recycle otherwise non-recoverable waste. The only thing that prevents the operation from being a little better than break-even is volatile natural gas prices. In its annual review, management requires information and analysis of recycling operations with a view to making decisions about outsourcing contracts. These contracts typically have three year tenors. Since management will be making do or outsource decisions over a three year forward span, analysts will build models that characterize historical movements in natural gas prices, the volatility of those prices, and probability distributions to simulate future natural gas scenarios. 3.10.2 Data In a preliminary analysis, you gather data from FRED on daily natural gas prices. You will use this data to characterize historical natural gas price movements and construct provisional probability distributions for eventual generation of forward scenarios. 3.10.3 Workflow Data collection. Collect, clean, and review data definitions, and data transformations of price into returns. Use tables and graphs to report results. Analysis. Group prices into up, same (no movement), and down movements using percent change in daily price as the criterion. Build a table of summary statistics that pivots the data and computes metrics. Graph the cumulative probability of an up, same, and down group of historical returns. Estimate Student’s t distribution parameters for up, same, and down movements in natural gas returns. Observations and Recommendations. Summarize the data, its characteristics, and applicability to attend to the problem being solved for management. Discuss key take-aways from analytical results that are relevant to the decisions that managers will make. Produce an R Markdown document with code chunks to document and interpret results. The format will introduce the problem to be analyzed, with sections that discuss the data used, and which follow the work flow. 3.11 References Brealey Chang Teetor knitr xts zoo BLS References "],
["macrofinancial-data-analysis.html", "Chapter 4 Macrofinancial Data Analysis 4.1 Imagine This 4.2 Building the Stylized Facts 4.3 Getting Caught in the Cross-Current 4.4 Time is on our Side 4.5 Give it the Boot 4.6 Summary 4.7 Further Reading", " Chapter 4 Macrofinancial Data Analysis 4.1 Imagine This Your US-based company just landed a contract worth more than 20 percent of your company’s current revenue in Spain. Now that everyone has recovered from this coup, your management wants you to Retrieve and begin to analyze data about the Spanish economy Compare and contrast Spanish stock market and government-issued debt value versus the United States and several other countries Begin to generate economic scenarios based on political events that may, or may not, happen in Spain Up to this point we had reviewed several ways to manipulate data in R. We then reviewed some basic finance and statistics concepts in R. We also got some idea of the financial analytics workflow. What decision(s) are we making? What are the key business questions we need to support this decision? What data do we need? What tools do we need to analyze the data? How do we communicate answers to inform the decision? 4.1.1 Working an example Let’s use this sequence to motivate our work in this here. We identify a decision at work (e.g., investment in a new machine, financing a building, acquisition of customers, hiring talent, locating manufacturing). For this decision we will list three business questions we need to inform the decision we chose. Now we consider data we might need to answer one of those questions and choose from this set: Macroeconomic data: GDP, inflation, wages, population Financial data: stock market prices, bond prices, exchange rates, commodity prices Here is the example using the scenario that started us off. Our decision is supply a new market segment Product: voltage devices with supporting software Geography: Spain Customers: major buyers at Iberdrola, Repsol, and Endesa We pose three business questions: How would the performance of these companies affect our ability to serve our customers and produce? How would the value of their products affect the value of our business with these companies? We are a US functional currency firm (see FAS 52), so how would we manage the repatriation of accounts receivable from Spain? Some data and analysis to inform the decision could include Customer stock prices: volatility and correlation Oil prices: volatility USD/EUR exchange rates: volatility All together: correlations among these indicators Volatility and correlation relate to the variability and co-variability of the values of these three companies. Each can have an up and a downside. Are there any systematically occurring patterns we need to be aware of as we design and implement our business plan? 4.1.2 How we will proceed This chapter will develop stylized facts of the market. These continue to be learned the hard Way: financial data is not independent, it possesses volatile volatility, and has extremes. Financial stock, bond, commodity…you name it…have highly interdependent relationships. Volatility is rarely constant and often has a structure (mean reversion) and is dependent on the past. Past shocks persist and may or may not dampen (rock in a pool). Extreme events are likely to happen with other extreme events. Negative returns are more likely than positive returns (left skew). One market’s volatility can spill over into another markets’s volatility further exacerbating persistent high aand low volatility. Many of these stylized facts are common to several areas of economic life and even more so to financial assets (???). Some of the mechanisms for persistent volatility include trading strategies (e.g., momentum and level trading). Others involve the digestion of information by traders who hedge versus those who speculate. Whatever the reasons, managers who depend on prices of purchased goods and services as well as receipts from customers, will see volatility in their earnings and cash flow. 4.2 Building the Stylized Facts Examples from the 70s, 80s, and 90s have multiple intersecting global events influencing decision makers. We will load some computational help and some data from Brent, format dates, and create a time series object (package zoo' will be needed by packagesfBasicsandevir`): library(fBasics) library(evir) library(qrmdata) library(zoo) data(OIL_Brent) str(OIL_Brent) ## An &#39;xts&#39; object on 1987-05-20/2015-12-28 containing: ## Data: num [1:7258, 1] 18.6 18.4 18.6 18.6 18.6 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;OIL_Brent&quot; ## Indexed by objects of class: [Date] TZ: UTC ## xts Attributes: ## NULL We will compute rates of change for Brent oil prices next. Brent.price &lt;- as.zoo(OIL_Brent) str(Brent.price) ## &#39;zoo&#39; series from 1987-05-20 to 2015-12-28 ## Data: num [1:7258, 1] 18.6 18.4 18.6 18.6 18.6 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : NULL ## ..$ : chr &quot;OIL_Brent&quot; ## Index: Date[1:7258], format: &quot;1987-05-20&quot; &quot;1987-05-21&quot; &quot;1987-05-22&quot; &quot;1987-05-25&quot; &quot;1987-05-26&quot; ... Brent.return &lt;- diff(log(Brent.price))[-1] * 100 colnames(Brent.return) &lt;- &quot;Brent.return&quot; head(Brent.return, n = 5) ## Brent.return ## 1987-05-22 0.5405 ## 1987-05-25 0.2692 ## 1987-05-26 0.1612 ## 1987-05-27 -0.1612 ## 1987-05-28 0.0000 tail(Brent.return, n = 5) ## Brent.return ## 2015-12-21 -3.9395 ## 2015-12-22 -0.2266 ## 2015-12-23 1.4919 ## 2015-12-24 3.9178 ## 2015-12-28 -0.3769 Let’s look at this data with box plots and autocorrelation functions. Box plots will show minimum to maximum with the mean in the middle of the box. Autocorrelation plots will reveal how persistent the returns are over time. We run these statements. boxplot(as.vector(Brent.return), title = FALSE, main = &quot;Brent Daily % Change&quot;, col = &quot;blue&quot;, cex = 0.5, pch = 19) skewness(Brent.return) kurtosis(Brent.return) This time series plot shows lots of return clustering and spikes, especially negative ones. Performing some “eyeball econometrics” these clusters seem to occur around - The oil embargo of the ’70s - The height of the new interest rate regime of Paul Volcker at the Fed - “Black Monday” stock market crash in 1987 - Gulf I - Barings and other derivatives business collapses in the ’90s Let’s look at the likelihood of positive versus negative returns. We might want to review skewness and kurtosis definitions and ranges to help us. Now to look at persistence: acf(coredata(Brent.return), main = &quot;Brent Daily Autocorrelogram&quot;, lag.max = 20, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) pacf(coredata(Brent.return), main = &quot;Brent Daily Partial Autocorrelogram&quot;, lag.max = 20, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) Confidence intervals are the red dashed lines. ACF at lag 6 means the correlation of current Brent returns with returns 6 trading days ago, including any correlations from trading day 1 through 6. PACF is simpler: it is the raw correlation between day 0 and day 6. ACF starts at lag 0 (today); PACF starts at lag 1 (yesterday). How many trading days in a typical week or in a month? Comment on the spikes (blue lines that grow over or under the red dashed lines). How thick is that tail? Here is a first look: boxplot(as.vector(Brent.return), title = FALSE, main = &quot;Brent Daily Returns&quot;, col = &quot;blue&quot;, cex = 0.5, pch = 10) … with some basic stats to back up the eyeball econometrics in the box plot: skewness(Brent.return) ## [1] -0.621 ## attr(,&quot;method&quot;) ## [1] &quot;moment&quot; kurtosis(Brent.return) ## [1] 14.62 ## attr(,&quot;method&quot;) ## [1] &quot;excess&quot; A negative skew means there are more observations less than the median than greater. This high a kurtosis means a pretty heavy tail, especially in negative returns. That means they have happened more often than positive returns. A preponderance of negative returns frequently happening spells trouble for anyone owning these assets. 4.2.1 Implications We should recommend that management budget for the body of the distribution from the mean and out to positive levels. At the same time management should build a comprehensive playbook for the strong possibility that bad tail events frequently happen and might happen again (and why shouldn’t they?). Now for something really interesting acf(coredata(Brent.return), main = &quot;Brent Autocorrelogram&quot;, lag.max = 20, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) pacf(coredata(Brent.return), main = &quot;Brent Partial Autocorrelogram&quot;, lag.max = 20, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) On average there are 5 days in the trading week and 20 in the trading month. Some further thoughts: There seems to be positive weekly and negative monthly cycles. On a weekly basis negative rates (5 trading days ago) are followed by negative rates (today) and vice-versa with positive rates. On a monthly basis negative rates (20 days ago) are followed by positive rates (today). There is memory in the markets: positive correlation at least weekly up to a month ago reinforces the strong and frequently occurring negative rates (negative skew and leptokurtotic, a.k.a. heavy tails). Run the PACF for 60 days to see a 40-day negative correlation as well. 4.2.2 Now for somthing really interesting…again Let’s look just at the size of the Brent returns. The absolute value of the returns (think of oil and countries entering and leaving the EU!) can signal contagion, herd mentality, and simply very large margin calls (and the collateral to back it all up!). Let’s run this code: Brent.return.abs &lt;- abs(Brent.return) ## Trading position size matters Brent.return.tail &lt;- tail(Brent.return.abs[order(Brent.return.abs)], 100)[1] ## Take just the first of the 100 observations and pick the first index &lt;- which(Brent.return.abs &gt; Brent.return.tail, arr.ind = TRUE) ## Build an index of those sizes that exceed the heavy tail threshold Brent.return.abs.tail &lt;- timeSeries(rep(0, length(Brent.return)), charvec = time(Brent.return)) ## just a lot of zeros we will fill up next Brent.return.abs.tail[index, 1] &lt;- Brent.return.abs[index] ## A Phew! is in order What did we do? Let’s run some plots next. plot(Brent.return.abs, xlab = &quot;&quot;, main = &quot;Brent Daily Return Sizes&quot;, col = &quot;blue&quot;) We see lots of return volatility – just in the pure size along. These are correlated with financial innovations from the ’80s and ’90s, as well as Gulf 1, Gulf 2, Great Recession, and its 9/11 antecedents. acf(coredata(Brent.return.abs), main = &quot;Brent Autocorrelogram&quot;, lag.max = 60, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) pacf(coredata(Brent.return.abs), main = &quot;Brent Partial Autocorrelogram&quot;, lag.max = 60, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) There is Volatility Clustering galore. Strong persistent lags of absolute movements in returns evidenced by the ACF plot. There is evidence of dampening with after shocks past trading 10 days 10 ago. Monthly volatility affects today’s performance. Some of this volatility arises from the way Brent is traded. It is lifted through well-heads in the North Sea. It then is scheduled for loading onto ships and loads are then bid, along with routes to destination. It takes about five days to load crude and another five to unload. At each partial loading and unloading, the crude is re-priced. Then there is the voyage lag itself, where paper claims to wet crude create further pricing, and volatility. Next we explore the relationships among financial variables. 4.3 Getting Caught in the Cross-Current Now our job is to ask the really important questions around connectivity. Suppose we are banking our investments in certain sectors of an economy, with its GDP, financial capability, employment, exports and imports, and so on. How will we decide to contract for goods and services, segment vendors, segment customers, based on these interactions? How do we construct out portfolio of business opportunities? How do we identify insurgent and relational risks and build a playbook to manage these? How will changes in one sector’s factors (say, finance, political will) affect factors in another? We will now stretch our univariate analysis a bit and look at cross-correlations to help us get the ground truth around these relationships, and begin to answer some of these business questions in a more specific context. Let’s load the zoo and qrmdata libraries first and look at the EuroStoxx50 data set. Here we can imagine we are rebuilding our brand and footprint in the European Union and United Kingdom. Our customers might be the companies based in these countries as our target market. The data: 4 stock exchange indices across Europe (and the United Kingdom) This will allow us to profile the forward capabilities of these companies across their economies. Again we will look at returns data using the diff(log(data))[-1] formula. library(zoo) library(qrmdata) library(xts) data(&quot;EuStockMarkets&quot;) EuStockMarkets.price &lt;- as.zoo(EuStockMarkets) EuStockMarkets.return &lt;- diff(log(EuStockMarkets.price))[-1] * 100 We then plot price levels and returns. plot(EuStockMarkets.price, xlab = &quot; &quot;, main = &quot; &quot;) plot(EuStockMarkets.return, xlab = &quot; &quot;, main = &quot; &quot;) We see much the same thing as Brent oil with volatility clustering and heavily weighted tails. Let’s then look at cross-correlations among one pair of these indices to see how they are related across time (lags) for returns and the absolute value of returns. THe function ccf will aid us tremendously. ccf(EuStockMarkets.return[, 1], EuStockMarkets.return[, 2], main = &quot;Returns DAX vs. CAC&quot;, lag.max = 20, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) ccf(abs(EuStockMarkets.return[, 1]), abs(EuStockMarkets.return[, 2]), main = &quot;Absolute Returns DAX vs. CAC&quot;, lag.max = 20, ylab = &quot;&quot;, xlab = &quot;&quot;, col = &quot;blue&quot;, ci.col = &quot;red&quot;) We see some small raw correlations across time with raw returns. More revealing, we see volatility of correlation clustering using return sizes. We can conduct one more experiment: a rolling correlation using this function: corr.rolling &lt;- function(x) { dim &lt;- ncol(x) corr.r &lt;- cor(x)[lower.tri(diag(dim), diag = FALSE)] return(corr.r) } We then embed our rolling correlation function, corr.rolling, into the function rollapply (look this one up using ??rollapply at the console). The question we need to answer is: What is the history of correlations, and from the history, the pattern of correlations in the UK and EU stock markets? If there is a “history” with a “pattern,” then we have to manage the risk that conducting business in one country will definitely affect business in another. The implication is that bad things will be followed by more bad things more often than good things. The implication compounds a similar implication across markets. corr.returns &lt;- rollapply(EuStockMarkets.return, width = 250, corr.rolling, align = &quot;right&quot;, by.column = FALSE) colnames(corr.returns) &lt;- c(&quot;DAX &amp; CAC&quot;, &quot;DAX &amp; SMI&quot;, &quot;DAX &amp; FTSE&quot;, &quot;CAC &amp; SMI&quot;, &quot;CAC &amp; FTSE&quot;, &quot;SMI &amp; FTSE&quot; ) plot(corr.returns, xlab = &quot;&quot;, main = &quot;&quot;) Again we observe the volatility clustering from bunching up of the the absolute sizes of returns. Economic performance is certainly subject here to the same dynamics we saw for a single financial variable such as Brent. Let’s redo some of the work we just did using another set of techniques. This time we are using the “Fisher” transformation. Look up Fisher in Wikipedia and in your reference texts. How can the Fisher Transformation possibly help us answer our business questions? For three Spanish companies, Iberdrola, Endesa, and Repsol, replicate the Brent and EU stock market experiments above with absolute sizes and tails. Here we already have “series” covered. First, the Fisher transformation is a smoothing routine that helps us tabilize the volitility of a variate. It does this by pulling some of the shockiness (i.e., outliers and aberrant noise) out of the original time series. In a phrase, it helps us see the forest (or the wood) for the trees. We now replicate the Brent and EU stock exchange experiments. We again load some packages and get some data using quantmod’s getSymbols off the Madrid stock exchange to match our initial working example of Iberian companies on account. Then compute returns and merge into a master file. library(xts) library(qrmdata) library(quantreg) library(quantmod) library(matrixStats) tickers &lt;- c(&quot;ELE.MC&quot;, &quot;IBE.MC&quot;, &quot;REP.MC&quot;) getSymbols(tickers) ## [1] &quot;ELE.MC&quot; &quot;IBE.MC&quot; &quot;REP.MC&quot; REP.r &lt;- diff(log(REP.MC[, 4]))[-1] IBE.r &lt;- diff(log(IBE.MC[, 4]))[-1] ELE.r &lt;- diff(log(ELE.MC[, 4]))[-1] ALL.r &lt;- merge(REP = REP.r, IBE = IBE.r, ELE = ELE.r, all = FALSE) Next we plot the returns and their absolute values, acf and pacf, all like we did in Brent. Again we see these univariate stylized facts: The persistence of returns The importance of return size Clustering of volatility plot(ALL.r) par(mfrow = c(2,1)) acf(ALL.r) par(mfrow = c(2,1)) acf(abs(ALL.r)) par(mfrow = c(2,1)) pacf(ALL.r) par(mfrow = c(2,1)) pacf(abs(ALL.r)) Let’s examine the correlation structure of markets where we can observe The relationship between correlation and volatility How quantile regression gets us to an understanding of high stress (high and low quantile) episodes R.corr &lt;- apply.monthly(ALL.r, FUN = cor) R.vols &lt;- apply.monthly(ALL.r, FUN = colSds) ## from MatrixStats head(R.corr, 3) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## 2007-01-31 1 0.3614 -0.27541 0.3614 1 0.10414 -0.27541 0.10414 1 ## 2007-02-28 1 0.5662 -0.09856 0.5662 1 0.10760 -0.09856 0.10760 1 ## 2007-03-30 1 0.4501 -0.08875 0.4501 1 0.08538 -0.08875 0.08538 1 head(R.vols, 3) ## REP.MC.Close IBE.MC.Close ELE.MC.Close ## 2007-01-31 0.009788 0.007893 0.009777 ## 2007-02-28 0.009181 0.014572 0.007675 ## 2007-03-30 0.015317 0.012720 0.010919 R.corr.1 &lt;- matrix(R.corr[1,], nrow = 3, ncol = 3, byrow = FALSE) rownames(R.corr.1) &lt;- tickers colnames(R.corr.1) &lt;- tickers head(R.corr.1) ## ELE.MC IBE.MC REP.MC ## ELE.MC 1.0000 0.3614 -0.2754 ## IBE.MC 0.3614 1.0000 0.1041 ## REP.MC -0.2754 0.1041 1.0000 R.corr &lt;- R.corr[, c(2, 3, 6)] colnames(R.corr) &lt;- c(&quot;ELE.IBE&quot;, &quot;ELE.REP&quot;, &quot;IBE.REP&quot;) colnames(R.vols) &lt;- c(&quot;ELE.vols&quot;, &quot;IBE.vols&quot;, &quot;REP.vols&quot;) head(R.corr, 3) ## ELE.IBE ELE.REP IBE.REP ## 2007-01-31 0.3614 -0.27541 0.10414 ## 2007-02-28 0.5662 -0.09856 0.10760 ## 2007-03-30 0.4501 -0.08875 0.08538 head(R.vols, 3) ## ELE.vols IBE.vols REP.vols ## 2007-01-31 0.009788 0.007893 0.009777 ## 2007-02-28 0.009181 0.014572 0.007675 ## 2007-03-30 0.015317 0.012720 0.010919 R.corr.vols &lt;- merge(R.corr, R.vols) plot.zoo(merge(R.corr.vols)) ELE.vols &lt;- as.numeric(R.corr.vols[,&quot;ELE.vols&quot;]) IBE.vols &lt;- as.numeric(R.vols[,&quot;IBE.vols&quot;]) REP.vols &lt;- as.numeric(R.vols[,&quot;REP.vols&quot;]) length(ELE.vols) ## [1] 156 fisher &lt;- function(r) {0.5 * log((1 + r)/(1 - r))} rho.fisher &lt;- matrix(fisher(as.numeric(R.corr.vols[,1:3])), nrow = length(ELE.vols), ncol = 3, byrow= FALSE) 4.3.1 On to quantiles Here is the quantile regression part of the story. Quantile regression finds the average relationship between dependent and independent variables just like ordinary least squares with one exception. Instead of centering the regression on the arithmetic mean of the dependent variable, quantile regression centers the regression on a specified quantile of the dependent variable. So instead of using the arithemetic average of the rolling correlations, we now use the 10th quantile, or the median, which is the 50th quantile as our reference. This makes great intuitive sense since we have already established that the series we deal with here are thick tailed, skewed, and certainly not normally distributed. This technique will allow us to examine the multivariate stylized facts of financial markets, especially the spillover from one market’s volatility into anothers through the mechanism of correlation of returns. Here is how we use the quantreg package. We set taus as the quantiles of interest. We run the quantile regression using the quantreg package and a call to the rq function. We can overlay the quantile regression results onto the standard linear model regression. We can sensitize our analysis with the range of upper and lower bounds on the parameter estimates of the relationship between correlation and volatility. This sensitivity analysis is really a quantile-based confidence interval. taus &lt;- seq(.05,.95,.05) fit.rq.ELE.IBE &lt;- rq(rho.fisher[,1] ~ ELE.vols, tau = taus) fit.lm.ELE.IBE &lt;- lm(rho.fisher[,1] ~ ELE.vols) plot(summary(fit.rq.ELE.IBE), parm = &quot;ELE.vols&quot;) Here we build the estimations and plot the upper and lower bounds. taus1 &lt;- c(.05, .95) ## fit the confidence interval (CI) plot(ELE.vols,rho.fisher[, 1], xlab=&quot;ELE.vol&quot;, ylab=&quot;ELE.IBE&quot;) abline(fit.lm.ELE.IBE, col = &quot;red&quot;) for (i in 1:length(taus1)){ ## these lines will be the CI abline(rq(rho.fisher[,1] ~ ELE.vols, tau = taus1[i]), col = &quot;blue&quot;) } grid() Quantile regression helps us to see the upper and lower bounds. Relationships between high-stress periods and correlation are abundant. These markets simply reflect normal buying behaviors across many types of exchanges: buying food at Safeway or Whole Foods, buying collateral to insure a project, selling off illiquid assets. 4.4 Time is on our Side Off to another important variable, the level and growth rate of Gross National Product. Let’s start with some US Gross National Product (GNP) data from the St. Louis Fed’s open data website (“FRED”). We build a URL using the paste() function. name &lt;- &quot;GNP&quot; URL &lt;- paste(&quot;http://research.stlouisfed.org/fred2/series/&quot;, name, &quot;/&quot;, &quot;downloaddata/&quot;, name, &quot;.csv&quot;, sep = &quot;&quot;) download &lt;- read.csv(URL) Look at the data: hist(download[,2]) summary(download[, 2]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 244 730 3616 6197 10632 21852 We create a raw time series object using the ts function where rownames are dates, select some data, and calculate growth rates. This will allow us and plotting functions to use the dates to index the data. Again we make use of the `diff(log(data)) GNP &lt;- ts(download[1:85, 2], start = c(1995, 1), freq = 4) GNP.rate &lt;- 100 * diff(log(GNP)) # In percentage terms str(GNP) ## Time-Series [1:85] from 1995 to 2016: 244 247 251 261 267 ... head(GNP) ## [1] 244.1 247.1 250.7 261.0 267.1 274.0 head(GNP.rate) ## [1] 1.189 1.468 4.013 2.330 2.555 2.392 Let’s plot the GNP level and rate and comment on the patterns. plot(GNP, type = &quot;l&quot;, main = &quot;US GNP Level&quot;) plot(GNP.rate, type = &quot;h&quot;, main = &quot;GNP quarterly growth rates&quot;) abline(h = 0, col = &quot;darkgray&quot;) We see a phenomenon called “nonstationarity.” The probability distribution (think hist()) would seem to change over time (many versions of a hist()). This means that the standard deviation and mean change as well (and higher moments such as skewness and kurtosis). There is trend in the level and simply dampened sinusoidal activity in the rate. In a nutshell we observe several distributions mixed together in this series. 4.4.1 Forecasting GNP As always let’s look at ACF and PACF: par(mfrow = c(2,1)) ##stacked up and down acf(GNP.rate) acf(abs(GNP.rate)) par(mfrow = c(1,1)) ##default setting What do we think is going on? There are several significant autocorrelations within the last 4 quarters. Partial autocorrelation also indicates some possible relationship 8 quarters back. Let’s use R’s time series estimation and prediction tool arima. In this world we think there is a regression that looks like this: \\[ x_t = a_0 + a_1 x_{t-1} ... a_p x_{t-p} + b_1 \\epsilon_{t-1} + ... + b_q \\epsilon_{t-q} \\] where \\(x_t\\) is a first, \\(d = 1\\), differenced level of a variable, here GNP. There are \\(p\\) lags of the rate itself and \\(q\\) lags of residuals. We officially call this an Autoregressive Integrated Moving Average process of order \\((p,d,q)\\), or ARIMA(p,d,q) for short. Estimation is quick and easy. fit.rate &lt;- arima(GNP.rate, order = c(2, 1, 1)) The order is 2 lags of rates, 1 further difference (already differenced once when we calculated diff(log(GNP))), and 1 lag of residuals. Let’s diagnose the results with tsdiag(). What are the results? fit.rate ## ## Call: ## arima(x = GNP.rate, order = c(2, 1, 1)) ## ## Coefficients: ## ar1 ar2 ma1 ## -0.790 0.005 0.568 ## s.e. 0.421 0.203 0.405 ## ## sigma^2 estimated as 1.65: log likelihood = -138.6, aic = 285.3 Let’s take out the moving average term and compare: fit.rate.2 &lt;- arima(GNP.rate, order = c(2,0,0)) fit.rate.2 ## ## Call: ## arima(x = GNP.rate, order = c(2, 0, 0)) ## ## Coefficients: ## ar1 ar2 intercept ## 0.473 0.071 1.589 ## s.e. 0.109 0.109 0.279 ## ## sigma^2 estimated as 1.41: log likelihood = -133.7, aic = 275.5 We examine the residuals next. The qqnorm function plots actual quantiles against theoretical quantile values from the normal distribution. A line through the scatterplot will reveal deviations of actual quantiles from the normal ones. Those deviations are the key to understanding tail behavior, and thus the potential influence of outliers, on our understanding of the data. qqnorm(GNP.resid); qqline(GNP.resid) Some ways to interpret the qq-chart include The diagonal line is the normal distribution quantile line. Deviations of actual quantiles from the normal quantile line mean nonnormal. Especially deviations at either (or both) end of the line spell thick tails and lots more “shape” than the normal distribution allows. 4.4.2 Residuals again How can we begin to diagnose the GNP residuals? Let’s use the ACF and the moments package to calculate skewness and kurtosis. We find that the series is very thick tailed and serially correlated as evidenced by the usual statistical suspects. But no volatility clustering. acf(GNP.resid) Now let’s look at the absolute values of growth (i.e., GNP growth sizes). This will help us understand the time series aspects of the volatility of the GNP residuals. acf(abs(GNP.resid)) …and compute tail statistics. library(moments) skewness(GNP.resid) ## [1] 0.4041 ## attr(,&quot;method&quot;) ## [1] &quot;moment&quot; kurtosis(GNP.resid) ## [1] 0.3095 ## attr(,&quot;method&quot;) ## [1] &quot;excess&quot; The residuals are positively skewed and not so thick tailed, as the normal distribution has by definition a kurtosis equal to 3.00. By the by: Where’s the forecast? (GNP.pred &lt;- predict(fit.rate, n.ahead = 8)) ## $pred ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2016 2.681 3.041 2.754 ## 2017 2.982 2.800 2.945 2.830 ## 2018 2.922 ## ## $se ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2016 1.284 1.627 2.041 ## 2017 2.293 2.586 2.801 3.035 ## 2018 3.227 Now for something really interesting, yet another rendering of the notorious Efficient Markets Hypothesis. 4.5 Give it the Boot Our goal is to infer the significance of a statistical relationship among variates. However, we do not have access to, or a “formula” does not exist, that allows us to compute the sample standard deviation of the mean estimator. The context is just how dependent is today’s stock return on yesterday’s? We want to use the distribution of real-world returns data, without needing assumptions about normality. The null hypothesis \\(H_0\\) is lack of dependence (i.e., an efficient market). The alternative hypothesis \\(H_1\\) is that today’s returns depend on past returns, on average. Our strategy is to change the data repeatedly, and re-estimate a relationship. The data is sampled using the replicate function, and the sample ACF is computed. This gives us the distribution of the coefficient of the ACF under the null hypotheses, \\(H0\\): independence, while using the empirical distribution of the returns data. Let’s use the Repsol returns and pull the 1st autocorrelation from the sample with this simple code, acf(REP.r, 1) There is not much for us to see, barely a blip, but there is a correlation over the 95% line. Let’s further test this idea. We obtain 2500 draws from the distribution of the first autocorrelation using the replicate function. We operate under the null hypothesis of independence, assuming rational markets (i.e, rational markets is a “maintained hypothesis”). set.seed(1016) acf.coeff.sim &lt;- replicate(2500, acf(sample(REP.r, size = 2500, replace = FALSE), lag = 1, plot=FALSE)$acf[2]) summary(acf.coeff.sim) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.05383 0.00741 0.02098 0.02129 0.03551 0.10260 Here is a plot of the distribution of the sample means of the one lag correlation between successive returns. hist(acf.coeff.sim, probability = TRUE, breaks = &quot;FD&quot;, xlim = c(.04, .05), col = &quot;steelblue&quot;, border = &quot;white&quot;) 4.5.1 Exercises 4.5.1.1 Exercise 1: leaning against the wind Download the stock prices of Tesla (NASDAQ: TSLA) and the Ford (NYSE: F). Calculate series of log price ratios for continuously compounded returns. What are the univariate and multivariate stylized facts of these markets? If we were to put together a portfolio of these two assets, how would the stylized facts impact our allocations? 4.5.1.2 Exercise 2: inferring patterns Investigate tolerances of \\(5\\%\\) and \\(1\\%\\) from both ends of the distribution of the 1-lag acf coefficient using these statements. That was a mouthful! When we think of inference, we first identify a parameter of interest, and its estimator. That parameter is the coefficient of correlation between the current return and its 1-period lag. We estimate this parameter using the history of returns. If the parameter is significantly, and probably, not equal to zero, then we would have reason to believe there is “pattern” in the “history.” ## At 95% tolerance level quantile(acf.coeff.sim, probs=c(.025,.975)) ## 2.5% 97.5% ## -0.02026 0.06252 ## At 99% tolerance level quantile(acf.coeff.sim, probs=c(.005,.995)) ## 0.5% 99.5% ## -0.03300 0.07503 ## And the (t.sim &lt;- mean(acf.coeff.sim)/sd(acf.coeff.sim)) ## [1] 1.012 (1-pt(t.sim, df = 2)) ## [1] 0.2089 Here are some highly preliminary and provisional answers to ponder. Quantile values are very narrow… How narrow (feeling like rejecting the null hypothesis)? The t-stat is huge, but… …no buts!, the probability that we would be wrong to reject the null hypothesis is very small. Plot the simulated density and lower and upper quantiles, along with the estimate of the lag-1 coefficient: plot(density(acf.coeff.sim), col=&quot;blue&quot;) abline(v=0) abline(v=quantile(acf.coeff.sim, probs=c(.025,.975)), lwd=2, col=&quot;red&quot;) abline(v=acf(REP.r, 1, plot=FALSE)$acf[2], lty=2, lwd=4, col=&quot;orange&quot;) Can we reject the null hypothesis that the coefficient = 0? Is the market “efficient”? Here are some provisional answers: Reject the null hypothesis since there is a less than 0.02% chance that the coefficient is zero. Read [Fama(2013, p. 365-367)]https://www.nobelprize.org/nobel_prizes/economic-sciences/laureates/2013/fama-lecture.pdf for a diagnosis. If the model is correct (ACF lag-1) then the previous day’s return can predict today’s return according to our analysis. Thus the market would seem to be inefficient. This means we might be able to create a profitable trading strategy that makes use of the little bit of correlation we found to be significant (net of the costs of trading). 4.6 Summary We explored time series data using ACF, PACF, and CCF. We showed how to pull data from Yahoo! and FRED. We characterized several stylized facts of financial returns and inferred behavior using a rolling correlation regression on volatility. We then supplemented the ordinary least squares regression confidence intervals using the entire distribution of the data with quantile regression. We also built Using bootstrapping techniques we simulated coefficient inference to check the efficient markets hypothesis. This, along with the quantile regression technique, allows us to examine risk tolerance from an inference point of view. 4.7 Further Reading In this chapter we touch on the voluminous topic of time series analysis as discussed in Ruppert and Matteson (2015) : chapters 12, 13, 14, and 15 explore the basics, as in this chapter, as well as far more advanced topics such as GARCH and cointegration. We will explore GARCH in a later chapter as well. McNeil, Frey, and Embrechts (2015) . in their chapter 1 surveys the perspective of risk, all of whichhelps to yield the so-called stylized facts of financial data in chapter 5 and a more formal treatment of time series topics in chapter 4. It is always useful to read the user manuals and vignettes associated with the packages used. Here we can dip into Zeileis, Grothendieck, and Ryan (2019), Hofert and Hornik (2016), Koenker (2019), Pfaff and McNeil (2018), Komsta and Novomestky (2015), and Wuertz, Setz, and Chalabi (2017) for more insights and examples. References "],
["market-risk.html", "Chapter 5 Market Risk 5.1 Imagine This 5.2 What is Market Risk? 5.3 History Speaks 5.4 Now to the Matter at Hand 5.5 Carl Friedrich Gauss, I Presume… 5.6 Back to the Future 5.7 Try this example 5.8 Let’s Go to Extremes 5.9 All Together Now 5.10 Summary 5.11 Further Reading 5.12 Practice Laboratory 5.13 Project 5.14 References", " Chapter 5 Market Risk 5.1 Imagine This Suppose a division in our company buys electricity to make steel. We know of two very volatile factors in this process: The price of steel at the revenue end, and the other is The price of electricity at the cost end. To model the joint volatility of the power-steel spread We can use electricity and steel prices straight from the commodity markets. We can also use stock market indexes or company stock prices from electricity producers and transmitters and from a steel products company to proxy for commodities. Using company proxies gives us a broader view of these commodities than just the traded pure play in them. In this chapter we will Measure risks using historical and parametric approaches Interpret results relative to business decisions Visualize market risk 5.2 What is Market Risk? Market risk for financial markets is the impact of unanticipated price changes on the value of an organization’s position in instruments, commodities, and other contracts. In commodity markets there is sometimes considered a more physical type of market risk called volumetric risk that relates to the delivery of the commodity to a buyer. This risk might be triggered by a complex contract such as a CDO or a spark spread tolling agreement in power and energy markets. Here we will assume that volumetric changes are physical in the sense that a electricity system operator governs a physical process such as idling an electric generation plant. A “position” is the physical holding of an asset or commodity, such as a share of Apple stock, or an ounce of gold. A long position means that the market participant possesses a positive amount of the asset or commodity. A short position means that the market participant does not possess an asset or commodity. The “position” is considered exogenous to the price stochastic process. This implies that changes in position do not affect the liquidity of the market relative to that position. 5.2.1 Try this exercise Suppose you are in charge of the project to manage the contracting for electricity and steel at your speciality steel company. Your company and industry have traditionally used tolling agreements to manage the steel-power spread, both from a power input and a steel output point of view. Look up a tolling agreement and summarize its main components. What are the input and output decisions in this kind of agreement. Here are some results In the electric power to steel tolling agreement a steel buyer supplies power to a steel plant and receives from the plant supplier an amount of steel based on an assumed power-to-steel transformation rate at an agreed cost. Prices of power and steel Transformation rate Agree cost Decisions include Buying an amount of power (MWh) Selling an amount of steel (tons) Scheduling plant operations Decisions will thus depend on the prices of electricity and steel, the customer and vendor segments served, the technology that determines the steel (tons) / Power (MWh) transformation rate, start-up, idle, and shut-down timing and costs and overall plant production costs. 5.3 History Speaks To get the basic idea of risk measures across we develop the value at risk and expected shortfall metrics from the historical simulated distributions of risk factors. Given these risk factors we combine them into a portfolio and calculate their losses. Finally with the loss distribution in hand we can compute the risk measures. We will use a purely, non-parametric historical simulation approach in this section. This means that we will not need to compute means, standard deviations, correlations, or other statistical estimators, also known as parameters. First we need to get some data. We will use throughout these computations several libraries: mvtnorm builds multivariate normal (Gaussian) simulations and QRM estimates Student-t and generalized pareto distribution (GPD) simulation. We will hold off on these parametric approaches till later and start with historical simulation. The psych library helps us to explore the interactions among data through scatter plots and histograms. The ggplot2 library allows us to build complex vizualizations that will aid the generation of further insights. We read in the csv file from the working directory. This file contains dates and several risk factors. In this setup we will use RWE stock prices will stand in for electricity price risk factor input and THYSSEN stock prices for the steel price risk factor. #Download the data data.all &lt;- read.csv(&quot;data/eurostock.csv&quot;,stringsAsFactors = FALSE) ## This will convert string dates to date objects below str(data.all) ## Check the structure and look for dates ## &#39;data.frame&#39;: 6147 obs. of 24 variables: ## $ X : chr &quot;1973-01-01&quot; &quot;1973-01-02&quot; &quot;1973-01-03&quot; &quot;1973-01-04&quot; ... ## $ ALLIANZ.HLDG. : num 156 156 161 162 164 ... ## $ COMMERZBANK : num 147 147 149 152 152 ... ## $ DRESDNER.BANK : num 18.4 18.4 18.8 18.9 18.9 ... ## $ BMW : num 104 109 110 111 109 ... ## $ SCHERING : num 36.9 37.4 37.8 37.9 37.4 ... ## $ BASF : num 15 15.4 15.6 15.8 15.8 ... ## $ BAYER : num 12.2 11.9 12.1 12.7 12.7 ... ## $ BAYERISCHE.VBK.: num 23.5 22.9 23.4 23.7 23.9 ... ## $ BAYER.HYPBK. : num 23.4 23.2 23.3 23.5 23.4 ... ## $ DEGUSSA : num 203 207 208 210 214 ... ## $ DEUTSCHE.BANK : num 22.3 22.5 22.9 23 23.3 ... ## $ CONTINENTAL : num 8.54 8.83 8.78 8.83 8.73 8.82 8.74 8.73 8.74 8.74 ... ## $ VOLKSWAGEN : num 134 140 145 144 140 ... ## $ DAIMLER.BENZ : num 17 17.6 17.8 17.8 17.7 ... ## $ HOECHST : num 13.8 13.8 14.2 14.3 14.2 ... ## $ SIEMENS : num 20.8 21.1 21.3 21.4 21.5 ... ## $ KARSTADT : num 360 360 362 369 368 ... ## $ LINDE : num 136 137 140 142 144 ... ## $ THYSSEN : num 67.5 68.4 67.5 71.6 71.2 ... ## $ MANNESMANN : num 85 86.5 87.8 88.7 88.6 ... ## $ MAN : num 118 119 125 125 127 ... ## $ RWE : num 11.7 11.9 12 11.9 12 ... ## $ INDEX : num 536 545 552 556 557 ... The next thing we must do is transform the data set into a time series object. The way we do that is to make the dates into row names so that dates are the index for the two risk factors. Making dates an index allows us to easily filter the data. str(row.names &lt;- data.all$X) ## We find that the first field X contains dates ## chr [1:6147] &quot;1973-01-01&quot; &quot;1973-01-02&quot; &quot;1973-01-03&quot; &quot;1973-01-04&quot; ... date &lt;- as.Date(row.names) ## convert string dates to date objects str(date) ##Always look at structure to be sure ## Date[1:6147], format: &quot;1973-01-01&quot; &quot;1973-01-02&quot; &quot;1973-01-03&quot; &quot;1973-01-04&quot; &quot;1973-01-05&quot; ... rownames(data.all) &lt;- date head(data.all) ## X ALLIANZ.HLDG. COMMERZBANK DRESDNER.BANK BMW ## 1973-01-01 1973-01-01 155.5 147.4 18.40 104.0 ## 1973-01-02 1973-01-02 155.5 147.4 18.40 109.0 ## 1973-01-03 1973-01-03 160.6 149.1 18.80 109.8 ## 1973-01-04 1973-01-04 162.3 152.1 18.91 110.8 ## 1973-01-05 1973-01-05 164.3 152.1 18.89 109.4 ## 1973-01-08 1973-01-08 164.3 152.2 18.99 109.0 ## SCHERING BASF BAYER BAYERISCHE.VBK. BAYER.HYPBK. DEGUSSA ## 1973-01-01 36.88 14.96 12.24 23.47 23.40 203.5 ## 1973-01-02 37.44 15.43 11.95 22.92 23.22 206.8 ## 1973-01-03 37.79 15.61 12.10 23.45 23.34 208.2 ## 1973-01-04 37.86 15.85 12.71 23.66 23.49 210.1 ## 1973-01-05 37.44 15.75 12.74 23.87 23.40 214.3 ## 1973-01-08 37.79 15.80 12.74 24.07 23.46 216.7 ## DEUTSCHE.BANK CONTINENTAL VOLKSWAGEN DAIMLER.BENZ HOECHST ## 1973-01-01 22.29 8.54 134.1 16.97 13.77 ## 1973-01-02 22.50 8.83 140.0 17.59 13.77 ## 1973-01-03 22.86 8.78 144.5 17.79 14.22 ## 1973-01-04 23.04 8.83 144.0 17.81 14.32 ## 1973-01-05 23.29 8.73 139.9 17.73 14.23 ## 1973-01-08 23.18 8.82 143.8 17.70 14.19 ## SIEMENS KARSTADT LINDE THYSSEN MANNESMANN MAN RWE INDEX ## 1973-01-01 20.76 359.6 135.9 67.47 84.97 117.9 11.68 536.4 ## 1973-01-02 21.06 360.0 136.9 68.41 86.51 118.8 11.87 545.4 ## 1973-01-03 21.29 362.0 139.6 67.47 87.75 125.0 12.03 552.5 ## 1973-01-04 21.44 369.3 142.2 71.62 88.71 125.0 11.95 556.1 ## 1973-01-05 21.48 368.5 143.7 71.24 88.63 127.3 12.03 557.4 ## 1973-01-08 21.48 366.9 143.8 70.77 89.01 125.3 11.91 555.5 tail(data.all) ##And always look at data ## X ALLIANZ.HLDG. COMMERZBANK DRESDNER.BANK BMW ## 1996-07-16 1996-07-16 2550 323.0 38.15 843.5 ## 1996-07-17 1996-07-17 2572 331.0 38.35 845.2 ## 1996-07-18 1996-07-18 2619 335.0 39.60 844.0 ## 1996-07-19 1996-07-19 2678 336.8 39.50 847.5 ## 1996-07-22 1996-07-22 2632 336.8 39.00 844.0 ## 1996-07-23 1996-07-23 2622 337.5 39.20 844.0 ## SCHERING BASF BAYER BAYERISCHE.VBK. BAYER.HYPBK. DEGUSSA ## 1996-07-16 101.0 41.00 50.45 47.45 40.20 498.0 ## 1996-07-17 102.5 41.87 50.92 48.08 40.55 503.2 ## 1996-07-18 101.2 41.86 52.00 49.05 41.48 507.5 ## 1996-07-19 102.9 42.10 51.85 49.48 41.92 506.0 ## 1996-07-22 101.8 40.70 50.60 49.40 41.40 501.0 ## 1996-07-23 102.0 40.15 50.25 49.88 41.55 499.0 ## DEUTSCHE.BANK CONTINENTAL VOLKSWAGEN DAIMLER.BENZ HOECHST ## 1996-07-16 72.10 23.00 531.0 78.45 49.85 ## 1996-07-17 72.86 23.63 539.0 79.30 50.30 ## 1996-07-18 74.30 24.11 528.5 78.00 50.50 ## 1996-07-19 74.90 24.18 531.0 78.25 50.70 ## 1996-07-22 73.60 24.10 522.2 77.48 49.20 ## 1996-07-23 73.70 24.15 515.0 77.35 48.35 ## SIEMENS KARSTADT LINDE THYSSEN MANNESMANN MAN RWE INDEX ## 1996-07-16 78.75 544.0 923 274.0 536.0 373.0 54.20 2470 ## 1996-07-17 78.79 554.0 925 273.1 542.0 374.5 54.40 2497 ## 1996-07-18 77.61 543.0 920 271.0 536.7 369.0 55.00 2506 ## 1996-07-19 77.24 543.0 932 271.9 535.3 369.5 54.33 2520 ## 1996-07-22 76.49 540.0 931 268.1 529.5 364.0 52.90 2482 ## 1996-07-23 76.90 539.5 935 265.5 530.5 360.0 53.15 2475 With this machinery in hand we can subset the data by starting and ending date as well as the choice of RWE and THYSSEN. #Subset the data using a start and end date start.date &lt;- &quot;1975-06-02&quot; end.date &lt;- &quot;1990-12-30&quot; ##First column looks for filtered dates, second and third columns pull out prices price &lt;- data.all[start.date &lt;= date &amp; date &lt;= end.date, c(&quot;RWE&quot;, &quot;THYSSEN&quot;)] ## We add a check to ensure that price is a matrix and that ncol will work if(!is.matrix(price)) price &lt;- rbind(price, deparse.level=0L) str(price) ## &#39;data.frame&#39;: 4065 obs. of 2 variables: ## $ RWE : num 8.96 9.2 9.16 9.2 9.36 9.24 9.12 9.08 9.04 8.99 ... ## $ THYSSEN: num 69.8 70.8 69.8 68.9 68.8 ... head(price) ## show the beginning ## RWE THYSSEN ## 1975-06-02 8.96 69.82 ## 1975-06-03 9.20 70.77 ## 1975-06-04 9.16 69.82 ## 1975-06-05 9.20 68.88 ## 1975-06-06 9.36 68.79 ## 1975-06-09 9.24 67.94 tail(price) ## and the end ## RWE THYSSEN ## 1990-12-21 36.36 187.5 ## 1990-12-24 36.36 187.5 ## 1990-12-25 36.36 187.5 ## 1990-12-26 36.36 187.5 ## 1990-12-27 36.28 186.5 ## 1990-12-28 35.75 184.5 The code before the str, head, and tail filters the price data by start and end dates. We could also perform this head and tail work using the following code. (end.idx &lt;- dim(price)[1]) ## [1] 4065 (price.2 &lt;- rbind(price[1:5,],price[(end.idx-4):end.idx,])) ## RWE THYSSEN ## 1975-06-02 8.96 69.82 ## 1975-06-03 9.20 70.77 ## 1975-06-04 9.16 69.82 ## 1975-06-05 9.20 68.88 ## 1975-06-06 9.36 68.79 ## 1990-12-24 36.36 187.50 ## 1990-12-25 36.36 187.50 ## 1990-12-26 36.36 187.50 ## 1990-12-27 36.28 186.50 ## 1990-12-28 35.75 184.50 ##$ Try this exercise Now let’s really explore this data. The library psych has a prefabricated scatter plot histogram matrix we can use. With this composite plot we can examine historical relationships between the two risk factors as well as the shape of the risk factors themselves. We can also use this device to look at dependent simulations. After the scatter plots, we then look at the time series plots of the two factors. #Use scatter plots of the two price series along with their histograms to examine the data library(psych) pairs.panels(price) price.rownames &lt;- rownames(price) plot(as.Date(price.rownames), price[,&quot;THYSSEN&quot;], type=&quot;l&quot;, main=&quot;Thyssen stock price data&quot;, ## title xlab=&quot;Date t&quot;, ## x-axis label ylab=expression(Stock~price~price[t])) ## y-axis label plot(as.Date(price.rownames), price[,&quot;RWE&quot;], type=&quot;l&quot;, main=&quot;RWE stock price data&quot;, ## title xlab=&quot;Date t&quot;, ## x-axis label ylab=expression(Stock~price~price[t])) ## y-axis label The pairs.panel plot displays a matrix of interactions between RWE and THYSSEN. Price levels are interesting but, as we have seen, are not stable predictors. Let’s transform them to returns next. 5.4 Now to the Matter at Hand Now to the matter at hand: value at risk and expected shortfall. These two measures are based on the quantiles of losses attributable to risk factors. Value at risk is the quantile at an \\(\\alpha\\) level of tolerance. Expected shortfall is the mean of the distribution beyond the value at risk threshold. To get losses attributable to market risk factors we compute log price differences (also called log price relatives). These can be interpreted as returns, or simply as percentage changes, in the risk factor prices. A plot lets us examine the results. #Here we can compute two items together: log price differences, and their range (to bound a plot) return.range &lt;- range(return.series &lt;- apply(log(price), 2, diff)) ## compute log-returns and range return.range ## [1] -0.2275 0.2201 plot(return.series, xlim=return.range, ylim=return.range, main=&quot;Risk Factor Changes&quot;, cex=0.2) Using the returns we can now compute loss. Weights are defined as the value of the positions in each risk factor. We can compute this as the notional times the last price. Remember we are talking about an input, electricity, and an output, steel. We form the margin: \\[ Margin = price_{steel} \\times tons - price_{power} \\times [(rate_{MWh / tons} \\times tons], \\] where the last term is the power to steel transformation rate that converts power prices $ per MWh to $ per ton. We convert prices to share prices and tons to equivalent values in terms of the number of shares. The naturally short position in power is equivalent to a negative number of shares (in the square brackets). The naturally long position in steel is equivalent to a positive number of shares. By naturally short we mean that power is an input, incurs a cost, and is demanded by the plant, and supplied by a third party. By naturally long we mean that steel is an output, earns a revenue, and demanded by a third party. ## Get last prices price.last &lt;- as.numeric(tail(price, n=1)) ## Specify the positions position.rf &lt;- c(-30,10) ## And compute the position weights w &lt;- position.rf * price.last ## Fan these across the length and breadth of the risk factor series weights.rf &lt;- matrix(w, nrow=nrow(return.series), ncol=ncol(return.series), byrow=TRUE) #We need to compute exp(x) - 1 for very small x: expm1 accomplishes this loss.rf &lt;- -rowSums(expm1(return.series) * weights.rf) summary(loss.rf) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -545.9 -13.3 0.0 -0.2 13.1 475.6 We can visualize the data using this ggplot2 routine which begins with the definition of a data frame. loss.rf.df &lt;- data.frame(Loss = loss.rf, Distribution = rep(&quot;Historical&quot;, each = length(loss.rf))) require(ggplot2) ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.2) + xlim(-100,100) The plot reveals some interesting deep and shallow outliers. The distribution is definitely very peaked. We use the base function expm1 that computes the natural exponent of returns all minus 1. \\[ e^{r} - 1 \\] Some of these returns, or percentage price changes if you will, are very close to zero. High precision arithmetic is needed to get accurate calculations. The function expm1 does this well. Now we can get to estimating value at risk (VaR) and expected shortfal (ES). We set the tolerance level \\(\\alpha\\), for example, equal to 95%. This would mean that a decision maker would not tolerate loss in more than 5% of all risk scenarios. We define the VaR as the quantile for probability \\(\\alpha \\in (0,1)\\), as \\[ VaR_{\\alpha} (X) = inf \\{ x \\in R: F(x) \\geq \\alpha \\}, \\] which means find the greatest lower bound of loss \\(x\\) (what the symbol \\(inf\\) = infimum means in English), such that the cumulative probability of \\(x\\) is greater than or equal to \\(\\alpha\\). Using the \\(VaR_{\\alpha}\\) definition we can also define \\(ES\\) as \\[ ES_{\\alpha} = E [X \\lvert X \\geq VaR_{\\alpha}], \\] where \\(ES\\) is “expected shortfall” and \\(E\\) is the expectation operator, also known as the “mean.” Again, in English, the expected shortfall is the average of all losses greater than the loss at a \\(VaR\\) associated with probability \\(\\alpha\\), and \\(ES \\geq VaR\\). 5.4.1 Try this example Let’s run the following lines of code. We look up the quantile function in R and see that it matches the calculation for VaR.hist. Using VaR we then calculate ES by only looking for losses greater than VaR. We also look closely at the text annotations we can achieve in ggplot2. Here is the code: First the computations of \\(VaR\\) and \\(ES\\): #Simple Value at Risk alpha.tolerance &lt;- .99 (VaR.hist &lt;- quantile(loss.rf, probs=alpha.tolerance, names=FALSE)) ## [1] 67.43 #Just as simple Expected shortfall (ES.hist &lt;- mean(loss.rf[loss.rf &gt; VaR.hist])) ## [1] 97.98 Next we set up the text and plotting environment. VaR.text &lt;- paste(&quot;Value at Risk =&quot;, round(VaR.hist, 2)) ES.text &lt;- paste(&quot;Expected Shortfall =&quot;, round(ES.hist, 2)) ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.2) + geom_vline(aes(xintercept = VaR.hist), linetype = &quot;dashed&quot;, size = 1, color = &quot;blue&quot;) + geom_vline(aes(xintercept = ES.hist), size = 1, color = &quot;blue&quot;) + xlim(0,200) + annotate(&quot;text&quot;, x = 40, y = 0.03, label = VaR.text) + annotate(&quot;text&quot;, x = 140, y = 0.03, label = ES.text) We see that ES is much bigger than VaR but also much less than the maximum historical loss. One note: VaR is computed as a pre-event indicator beyond a loss of 0 in this example. Many applications of this metric center loss at the median loss. Thus, loss would be computed as gross loss minus the median (50th percentile of loss). A box plot might also help us visualize the results without resorting to a probability distribution function. ggplot(loss.rf.df, aes(x = Distribution, y = Loss)) + geom_boxplot(outlier.size = 1.5, outlier.shape = 21) + ylim(-250,10) This box plot might look better with more than one distribution. So far we simply let history speak for itself. We did not assume anything at all about the shape of the data. We just used the empirical record be the shape. In what follows let’s start to put some different shapes into the loss potential of our tolling agreement. 5.5 Carl Friedrich Gauss, I Presume… What we just did was the classic historical simulation technique for computing tail risk measures. Historical simulation is a “nonparametric” technique, since there is no estimation of parameters conditional on a distribution. Only history, unadorned, informs risk measurement. Now we shift gears into the parametric work of Gauss: Gaussian, Generalized Pareto, and as an exercise Gossett’s (Student’s t) distributions. Carl Friedrich Gauss is often credited with the discovery of the normal distribution. So we tack his name often enough to the normal distribution. This distribution has a crucial role in quantitative risk and finance. It is often the basis for most derivative pricing models and for simulation of risk factors in general. It does not exhibit thick tails, and definitely is not skewed or peaked. This distribution definitely does not describe volatility clustering we observe in most financial and commodity time series. Nevertheless, it is otherwise ubiquitous, if only as a benchmark (like “perfect competition” or “efficient markets”). With just a little of math here, we can define the Gaussian (normal) distribution function. If \\(x\\) is a uniformly distributed random variable, then \\[ f(x) = \\frac{1}{\\sigma \\sqrt {2\\pi}}e^{-(x - \\mu)^{2} / 2 \\sigma^{2}} \\] is the probability density function of the normally distributed \\(x\\) with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). “Halfway”\" between the normal Gaussian distribution and Student’s t is the chi-square, \\(\\chi^2\\), distribution. We define \\(\\chi^2\\) as the distribution of the sum of the squared normal random variables \\(x\\) with density function and \\(k\\) degrees of freedom for \\(x &gt; 0\\): \\[ f(x) = \\frac{x^{(k/2-1)}e^{-x/2}}{2^{k/2}\\Gamma(\\frac{k}{2})} \\] and \\(0\\) otherwise. The “degrees of freedom” are the number of normal distributions used to create a chi-square variate. Now on to Student’s t distribution which is defined in terms of the Gaussian and chi-square distributions as the ratio of a Gaussian random variate to the square root of a chi-squared random variate. Student (a pseudonym for William Sealy Gossett) will have thicker tails but also the same symmetry as the normal curve. (Lookup this curve in Wikipedia among other references.) Here is a quick comparison of the standard Gaussian and the Student’s t distributions. The functions rnorm and rt generate Gaussian and Student’s t variates, respectively. The functions qnorm and qt compute the distance from the mean (probability = 50%) for a given probability, here stored in alpha.tolerance. library(mvtnorm) ## Allows us to generate Gaussian and Student-t variates library(ggplot2) set.seed(1016) n.sim &lt;- 1000 z &lt;- rnorm(n.sim) t &lt;- rt(n.sim, df = 5) alpha.tolerance &lt;- 0.95 (z.threshold &lt;- qnorm(alpha.tolerance)) ## [1] 1.645 (t.threshold &lt;- qt(alpha.tolerance, df = 5)) ## [1] 2.015 Now we make a data frame and plot with ggplot: zt.df &lt;- data.frame(Deviations = c(z,t), Distribution = rep(c(&quot;Gaussian&quot;,&quot;Student&#39;s t&quot;), each = n.sim)) ggplot(zt.df, aes(x = Deviations, fill = Distribution)) + geom_density(alpha=.3) + geom_vline(aes(xintercept=z.threshold), color=&quot;red&quot;, linetype =&quot;dashed&quot;, size=1) + geom_vline(aes(xintercept=t.threshold), color=&quot;blue&quot;, linetype=&quot;dashed&quot;, size=1) + xlim(-3,3) The ggplots2 library allows us to control several aspects of the histogram including fill, borders, vertical lines, colors, and line types and thickness. The plot requires a data frame where we have indicated the type of distribution using a replication of character strings. We see the two distributions are nearly the same in appearance. But the Student’s t tail is indeed thicker in the tail as the blue t density overtakes the red z density. This is numerically evident as the t.threshold is \\(&gt;\\) than the z.threshold for a cumulative probability of 95%, the 95th quantile. 5.5.1 Try this example Let’s zoom in on the right tail of the distribution with the xlim facet. ggplot(zt.df, aes(x = Deviations, fill = Distribution)) + geom_density(alpha = 0.2) + geom_vline(aes(xintercept=z.threshold), color=&quot;red&quot;, linetype =&quot;dashed&quot;, size=1) + geom_vline(aes(xintercept=t.threshold), color=&quot;blue&quot;, linetype=&quot;dashed&quot;, size=1) +xlim(1,5) Interesting digression! But not really not too far off the mark. The thresholds are the same with two standard risk measures, scaled for particular risk factors and positions. We have simulated two different values at risk. 5.6 Back to the Future Let’s remember where the returns (as changes) in each risk factor come from. Also, we will extract the last price for use below. # Again computing returns as changes in the risk factors return.series &lt;- apply(log(price), 2, diff) ## compute risk-factor changes price.last &lt;- as.numeric(tail(price, n=1)) ## reserve last price Again to emphasize what constitutes this data, we specify the notional exposure. These are number of shares of stock, number of $1 million contracts of futures, or volumetric contract sizes, e.g., MMBtus or boe. All of these work for us given the that price is dimensioned relative to the notional dimension. So if the risk factors are oil and natural gas prices, then we should use a common volumetric equivalent such as Btu (energy content) or boe (barrel of oil equivalent for volume). Position weights are then calculated as position times the last available price. First, we can set the weights directly and a little more simply than before since we do not need to simulate historically. ## Specify the positions position.rf &lt;- c(-30,10) ## As before ## And compute the position weights directly again as before (w &lt;- position.rf * price.last) ## [1] -1072 1845 Second, we estimate the mean vector and the variance-covariance matrix, the two major inputs to the simulation of normal risk factor changes. Here we use a purely parametric approach. mu.hat &lt;- colMeans(return.series) ## Mean vector mu; estimated = hat Sigma.hat &lt;- var(return.series) ## Variance-covariance matrix Sigma (loss.mean &lt;- -sum(w * mu.hat)) ## Mean loss ## [1] -0.07597 (loss.stdev &lt;- sqrt(t(w) %*% Sigma.hat %*% w)) ## Standard deviation of loss ## [,1] ## [1,] 28.44 Third, we set the level of risk tolerance \\(\\alpha\\). Then let’s calculate VaR and ES: #Compute VaR and ES and return alpha.tolerance &lt;- 0.95 q.alpha &lt;- qnorm(alpha.tolerance) (VaR.varcov &lt;- loss.mean + loss.stdev * q.alpha) ## [,1] ## [1,] 46.71 (ES.varcov &lt;- loss.mean + loss.stdev * dnorm(q.alpha) / (1-alpha.tolerance)) ## [,1] ## [1,] 58.59 and plot VaR.text &lt;- paste(&quot;Value at Risk =&quot;, round(VaR.varcov, 2)) ES.text &lt;- paste(&quot;Expected Shortfall =&quot;, round(ES.varcov, 2)) ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.2) + geom_vline(aes(xintercept = VaR.varcov), colour = &quot;red&quot;, size = 1) + geom_vline(aes(xintercept = ES.varcov), colour = &quot;blue&quot;, size = 1) + xlim(0,200)+ annotate(&quot;text&quot;, x = 30, y = 0.03, label = VaR.text) + annotate(&quot;text&quot;, x = 120, y = 0.03, label = ES.text) 5.7 Try this example Suppose it takes less electricity to make steel than we thought above. We can model this by changing the positions to (-20, 10). Let’s redo steps 1, 2, and 3 (this begs for a function). First, we can set the weights directly a little more simply than before since we do not need to simulate historically. ## Specify the positions position.rf &lt;- c(-20,10) ## As before ## And compute the position weights directly again as before (w &lt;- position.rf * price.last) ## [1] -715 1845 Second, estimate the mean vector and the variance-covariance matrix, the two major inputs to the simulation of normal risk factor changes. Here we use a purely parametric approach. mu.hat &lt;- colMeans(return.series) ## Mean vector mu; estimated = hat Sigma.hat &lt;- var(return.series) ## Variance-covariance matrix Sigma (loss.mean &lt;- -sum(w * mu.hat)) ## Mean loss ## [1] -0.1977 (loss.stdev &lt;- sqrt(t(w) %*% Sigma.hat %*% w)) ## Standard deviation of loss ## [,1] ## [1,] 28.54 Third, set the level of risk tolerance \\(\\alpha\\). Then calculate VaR and ES: #Compute VaR and ES and return alpha.tolerance &lt;- 0.95 q.alpha &lt;- qnorm(alpha.tolerance) (VaR.varcov &lt;- loss.mean + loss.stdev * q.alpha) ## [,1] ## [1,] 46.74 (ES.varcov &lt;- loss.mean + loss.stdev * dnorm(q.alpha) / (1-alpha.tolerance)) ## [,1] ## [1,] 58.67 … and plot VaR.text &lt;- paste(&quot;Value at Risk =&quot;, round(VaR.varcov, 2)) ES.text &lt;- paste(&quot;Expected Shortfall =&quot;, round(ES.varcov, 2)) ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.2) + geom_vline(aes(xintercept = VaR.varcov), colour = &quot;red&quot;, size = 1) + geom_vline(aes(xintercept = ES.varcov), colour = &quot;blue&quot;, size = 1) + xlim(0,200)+ annotate(&quot;text&quot;, x = 20, y = 0.04, label = VaR.text) + annotate(&quot;text&quot;, x = 100, y = 0.04, label = ES.text) Aesthetics may overtake us here as we really shoulf change the x and y annotate coordinates to fit on the graph properly. So ends the story of the main method used for years and embodied in the famous 4:15 (pm, that is) risk report at JP Morgan. Also we remember the loss that we simulate here is an operating income loss, which after taxes and other adjustments, and, say, a one-year horizon, means a loss of additions to retained earnings. Book equity drops and so will market capitalization on average. 5.8 Let’s Go to Extremes All along we have been stylizing financial returns, including commodities and exchange rates, as skewed and with thick tails. We next go on to investigate these tails further using an extreme tail distribution called the Generalized Pareto Distribution (GPD). For very high thresholds, such as value at risk and expected shortfall, GPD not only well describes behavior in excess of the threshold, but the mean excess over the threshold is linear in the threshold. From this we get more intuition around the use of expected shortfall as a coherent risk measure. In recent years markets well exceeded all Gaussian and Student’s t thresholds. For a random variate \\(x\\), this distribution is defined for shape parameters \\(\\xi \\geq 0\\) as: \\[ g(x; \\xi \\geq 0) = 1- (1 + x \\xi/\\beta)^{-1/\\xi} \\] and when the shape parameter \\(\\xi = 0\\), the GPD becomes the exponential distribution dependent only on the scale parameter \\(\\beta\\): \\[ g(x; \\xi = 0) = 1 - exp(-x/\\beta). \\] There is one reason for GPD’s notoriety. If \\(u\\) is an upper (very high) threshold, then the excess of threshold function for the GPD is \\[ e(u) = \\frac{\\beta + \\xi u}{1 - \\xi}. \\] This simple measure is linear in thresholds. It will allow us to visualize where rare events begin (see McNeil, Embrechts, and Frei (2015, chapter 5)). We will come back to this property when we look at operational loss data in a few chapters. Let’s use the QRM library to help us find the optimal fit of losses to the parameters. The fit.GPD function will do this for us. library(QRM) u &lt;- quantile(loss.rf, alpha.tolerance , names=FALSE) fit &lt;- fit.GPD(loss.rf, threshold=u) ## Fit GPD to the excesses (xi.hat &lt;- fit$par.ests[[&quot;xi&quot;]]) ## fitted xi ## [1] 0.1928 (beta.hat &lt;- fit$par.ests[[&quot;beta&quot;]]) ## fitted beta ## [1] 15.9 Now for the closed form (no random variate simulation!) using the McNeil, Embrechts, and Frei (2015, chapter 5) formulae: ## Pull out the losses over the threshold and compute excess over the threshold loss.excess &lt;- loss.rf[loss.rf &gt; u] - u ## compute the excesses over u n.relative.excess &lt;- length(loss.excess) / length(loss.rf) ## = N_u/n (VaR.gpd &lt;- u + (beta.hat/xi.hat)*(((1-alpha.tolerance) / n.relative.excess)^(-xi.hat)-1)) ## [1] 40.41 (ES.gpd &lt;- (VaR.gpd + beta.hat-xi.hat*u) / (1-xi.hat)) ## [1] 60.12 5.8.1 Try this example How good a fit to the data have we found? This plot should look roughly uniform since the GPD excess loss function is a linear function of thresholds u. gpd.density &lt;- pGPD(loss.excess, xi=xi.hat, beta=beta.hat) gpd.density.df &lt;- data.frame(Density = gpd.density, Distribution = rep(&quot;GPD&quot;, each = length(gpd.density))) ## This should be U[0,1] ggplot(gpd.density.df, aes(x = Density, fill = Distribution)) + geom_histogram() And it does look “uniform” enough (in a statistical sort of way as we perform eyeball econometrics again!). 5.9 All Together Now Let’s graph the historical simulation, variance-covariance and GPD results together. loss.plot &lt;- ggplot(loss.rf.df, aes(x = Loss, fill = Distribution)) + geom_density(alpha = 0.2) loss.plot &lt;- loss.plot + geom_vline(aes(xintercept = VaR.varcov), colour = &quot;red&quot;, linetype = &quot;dashed&quot;, size = 1) loss.plot &lt;- loss.plot + geom_vline(aes(xintercept = ES.varcov), colour = &quot;blue&quot;, linetype = &quot;dashed&quot;, size = 1) loss.plot &lt;- loss.plot + geom_vline(aes(xintercept = VaR.gpd), colour = &quot;red&quot;, size = 1) loss.plot &lt;- loss.plot + geom_vline(aes(xintercept = ES.gpd), colour = &quot;blue&quot;, size = 1) loss.plot &lt;- loss.plot + xlim(0,200) loss.plot That was a lot. We will need our “mean over excess” knowledge when we get to operational risk. Actually we will be able to apply that to these risk measures for any kind of risk. But we will save ourselves for operational risk later. Someone might even annotate the graph… 5.10 Summary Filtering - Math to R translation - Graphics - Normal and GPD distributions - VaR and ES - Loss distributions and mean over loss 5.11 Further Reading 5.12 Practice Laboratory 5.12.1 Practice laboratory #1 5.12.1.1 Problem 5.12.1.2 Questions 5.12.2 Practice laboratory #2 5.12.2.1 Problem 5.12.2.2 Questions 5.13 Project 5.13.1 Background 5.13.2 Data 5.13.3 Workflow 5.13.4 Assessment We will use the following rubric to assess our performance in producing analytic work product for the decision maker. The text is laid out cleanly, with clear divisions and transitions between sections and sub-sections. The writing itself is well-organized, free of grammatical and other mechanical errors, divided into complete sentences, logically grouped into paragraphs and sections, and easy to follow from the presumed level of knowledge. All numerical results or summaries are reported to suitable precision, and with appropriate measures of uncertainty attached when applicable. All figures and tables shown are relevant to the argument for ultimate conclusions. Figures and tables are easy to read, with informative captions, titles, axis labels and legends, and are placed near the relevant pieces of text. The code is formatted and organized so that it is easy for others to read and understand. It is indented, commented, and uses meaningful names. It only includes computations which are actually needed to answer the analytical questions, and avoids redundancy. Code borrowed from the notes, from books, or from resources found online is explicitly acknowledged and sourced in the comments. Functions or procedures not directly taken from the notes have accompanying tests which check whether the code does what it is supposed to. All code runs, and the R Markdown file knits to pdf_document output, or other output agreed with the instructor. Model specifications are described clearly and in appropriate detail. There are clear explanations of how estimating the model helps to answer the analytical questions, and rationales for all modeling choices. If multiple models are compared, they are all clearly described, along with the rationale for considering multiple models, and the reasons for selecting one model over another, or for using multiple models simultaneously. The actual estimation and simulation of model parameters or estimated functions is technically correct. All calculations based on estimates are clearly explained, and also technically correct. All estimates or derived quantities are accompanied with appropriate measures of uncertainty. The substantive, analytical questions are all answered as precisely as the data and the model allow. The chain of reasoning from estimation results about the model, or derived quantities, to substantive conclusions is both clear and convincing. Contingent answers (for example, “if X, then Y , but if A, then B, else C”) are likewise described as warranted by the model and data. If uncertainties in the data and model mean the answers to some questions must be imprecise, this too is reflected in the conclusions. All sources used, whether in conversation, print, online, or otherwise are listed and acknowledged where they used in code, words, pictures, and any other components of the analysis. 5.14 References McNeill, Alexander J., Rudiger Frey, and Paul Embrechts. 2015. Quantitative Risk Management: Concepts, Techniques and Tools. Revised Edition. Princeton: Princeton University Press. Ruppert, David and David S. Matteson. 2015. Statistics and Data Analysis for Financial Engineering with R Examples, Second Edition. New York: Springer. "],
["portfolio-analytics.html", "Chapter 6 Portfolio Analytics 6.1 Imagine This 6.2 Let’s Walk Before We Run 6.3 All In 6.4 Optimizing a Portfolio 6.5 Summary 6.6 Further Reading 6.7 Practice Laboratory 6.8 Project 6.9 References", " Chapter 6 Portfolio Analytics The first stage starts with observation and experience and ends with beliefs about the future performances of available securities. The second stage starts with the relevant beliefs about future performances and ends with the choice of portfolio. Harry Markowitz 6.1 Imagine This You are trying to negotiate a new energy contract across 10 different facilities and 20 gas and oil suppliers. Your colleague is managing accounts receivable across 38 different currencies in spot and forward markets. Another colleague manages collateral for workers’ compensation funding in all 50 states. Yet another colleague manages 5 fund managers for a health insurer. Portfolios are everywhere! We can conceive of every margin as a long position in revenue and a short position in costs. In all case of interest at least some notion of the the mean (central tendency) and standard deviation (scale and diffusion) of a portfolio metric (e.g., return) will be traded off. Operational and financial constraints will narrow the possible choices to achieve performance (mean = “mu” = \\(\\mu\\)) and risk (standard deviation = “sigma” = \\(\\sigma\\)) goals. 6.1.1 Our “working example” This will we Working Capital which is Receivables + Inventory - Payables. The CFO needs answers around why it is so big and always seems to bulge when the economic fortunes of our customers are in peril of deteriorating. She knows that there are three culprits: the euro rate, the Sterling rate, and Brent crude. She commissions you and your team to figure out the ultimate combination of these factors that contributes to a $100 million working capital position with a volatility of over $25 million this past year. 6.1.2 This chapter The goal of this chapter is to introduce portfolio analytics and incorporate our knowledge of statistics, optimization, and financial objects into the analysis. 6.2 Let’s Walk Before We Run Suppose management wants to achieve a targeted value at risk on new contracts. The value at risk (VaR) is the \\(\\alpha\\) quantile of portfolio value where \\(\\alpha\\) (“alpha”) is the organization’s tolerance for risk. While VaR is the maximum amount of tolerable loss more loss is possible. Now suppose Management is considering a $1 billion contract with two Iberian companies and one United Kingdom-based (UK) company working in Spain. The deal is constrained by having $100 million in reserves are available to cover any losses. The Board wants some comfort that no more than a 10 % age loss (the average “return” \\(\\mu\\)) would occur. The Board has set the organization’s tolerance for risk at 5%. This means in this case a maximum of 5% of the time could losses exceed 10%. To keep things simple at first we assume that losses are normally distributed. Let’s perform a “back of the envelope” analysis. We let \\(R\\) stand for returns, so that \\(-R\\) is a loss. Our management team wants \\[ Prob(R &lt; -0.10) = 0.05, \\] that is, the probability of a loss worse than 10 % is no more than 5 %. Let’s now do some algebra and first let \\(w\\) be the “weight” invested in the risky contract. The rest of the proportion of the contract, \\(1-w\\), is in high quality collateral assets like treasury bonds. The weight \\(w\\) can take on values from 0% (0.0) to 100% (1.0). No collateral means \\(w = 1\\) No contract means \\(w = 0\\). The average return, \\(\\mu\\), on the contract is \\[ \\mu = w(0.1) + (1-w)(0.02). \\] This is the weighted average return of the contract at 10% and collateral at 2%. The average level of risk in this model is given by the standard deviation of this combination of risky contract and default-free collateral. Management currently believes that a 25% standard deviation of return on the contract, “sigma” or \\(\\sigma\\), is reasonable. Collateral is not “risky” in this scenario and thus its standard deviation is zero (and by definition is not correlated with the contract return). \\[ \\sigma = w^2 (0.25)^2 + (1-w)^2 (0.0). \\] We now try to figure out what \\(w\\) is. More precisely, we solve for the percentage of total assets as investment in this contract, that will make losses happen no greater than 5% of the time. We form a normalizing “z-score” to help us. We shift the largest possible loss to average return to a deviation around the 10% loss, then divide by \\(\\sigma\\) to scale losses to the number of standard deviations around the maximum tolerable loss. \\[ z = \\frac{-0.1 - \\mu}{\\sigma}. \\] The \\(z\\) the ratio of potential deviation of loss from the mean maximum loss per unit of risk. It is dimensionless and represents the number of standard deviations of loss around the maximum tolerable loss. Our job is to find \\(w\\) such that the \\(z\\) score under the normal distribution cannot exceed 5%. \\[ Prob(R &lt; -0.10) = Normal(z(w)) = 0.05, \\] where \\(Normal\\) is the cumulative normal distribution (you might know this as =Norm.S.Dist() in Excel or qnorm() in R), with mean of zero and standard deviation of one. Using our models of \\(\\mu\\) and \\(\\sigma\\) we get \\[ z = \\frac{-0.1 - 0.1 w - 0.02 (1-w)}{0.25 w} \\] After combining constant terms and terms in \\(w\\) and putting this into the target probability: \\[ z = Normal \\left[ \\frac{-0.12 - 0.12 w}{0.25 w} \\right] = 0.05. \\] Finally, we solve for \\(w\\) in a few more steps. We invert the normal distribution on both sides of the equation. On the left hand side we are left with the \\(z\\) score as a function of \\(w\\), the percentage of all wealth in the risk contract. \\[ Inverse \\, Normal \\left[ Normal \\left( \\frac{-0.12 - 0.12 w}{0.25 w} \\right) \\right] = Inverse \\, Normal (0.05) \\] We can calculate \\(Inverse \\, Normal(0.05)\\) using R qnorm(0.05) ## [1] -1.645 or in Excel with =NORM.S.INV(0.05). Each of these takes as input the probability under the normal distribution and calculates the \\(z\\) score associated with the probability. We find that qnorm(0.05) is 1.64. Loss cannot exceed 1.64 times the portfolio standard deviation in the direction of loss (“negative” or less than the mean) using a one-tail interval. We justify a one-tail interval since we, and the Board, are only interested in loss. Inserting this value we get \\[ \\left( \\frac{-0.12 - 0.12 w}{0.25 w} \\right) = NormalInverse (0.05) = -1.64 \\] Multiplying each side by \\(0.25 w\\), combining terms in \\(w\\) and dividing by the coefficient of that last combined \\(w\\) we get \\[ w = \\frac{-0.12}{0.25(-1.64) + 0.12} = 0.42. \\] In R: 0.413793103 ## [1] 0.4138 Implications? 42% of portfolio value = risky contract value. Portfolio value = $1 billion / 0.42 = $2.38 billion. Collateral value = $2.38 billion - $1 billion = $1.38 billion or 68% of portfolio value. We just found the notorious “tangency” portfolio. This portfolio, when combined with a risk-free (really “default-free” asset), will yield the best mix of risky and risk-free assets. “Best” here is in the sense of not violating the organization’s risk tolerance policy. A way to find the tangency portfolio in any situation is then to Find the optimal combination of risky assets, the tangency portfolio. Then find the optimal mix of tangency assets and the risk-free asset. In our example working capital’s “risk-free” asset is the cash account and the process of getting there is the cash-conversion cycle. 6.3 All In Now that we have our basic procedure, let’s complicate this problem with many risky assets. The basic solution will be choosing weights to minimize the portfolio risk given risk-adjusted return targets. This is the Markowitz (1952) portfolio solution. For this task we need to define a matrix version of the portfolio allocation problem. Our three risky “assets” will be the euro/USD and GBP/USD exchange rates and Brent crude. This is all about the normally distributed universe. Let’s define more precisely First, the return matrix \\(R\\) for \\(N\\) assets across \\(T\\) sample periods and the subscript indicates the row (observation) and column (asset): \\[ \\left[ \\begin{array}{ccc} R_{11} &amp; ... &amp; R_{1N} \\\\ ... &amp; ... &amp; ... \\\\ R_{1T} &amp; ... &amp; R_{TN} \\end{array} \\right] \\] Then, the mean return vector \\(\\mu\\) is the arithmetic average of each column of \\(R\\) \\[ \\left[ \\begin{array}{c} \\mu_1 \\\\ ... \\\\ \\mu_N \\end{array} \\right], \\] after exchanging rows for columns (transpose). 6.3.1 Try this exercise First let’s be sure the qrmdata package is installed. We require this package and the daily data in it. Then we will look up the apply function to see how we can compute row averages. require(qrmdata) require(xts) ## The exchange rate data was obtained from OANDA (http://www.oanda.com/) on 2016-01-03 data(&quot;EUR_USD&quot;) data(&quot;GBP_USD&quot;) ## The Brent data was obtained from Federal Reserve Economic Data (FRED) via Quandl on 2016-01-03 data(&quot;OIL_Brent&quot;) data.1 &lt;- na.omit(merge(EUR_USD, GBP_USD, OIL_Brent)) R &lt;- na.omit(diff(log(data.1))*100) names.R &lt;- c(&quot;EUR.USD&quot;, &quot;GBP.USD&quot;, &quot;OIL.Brent&quot;) colnames(R) &lt;- names.R First we compute the mean return. (mean.R &lt;- apply(R, 2, mean)) ## EUR.USD GBP.USD OIL.Brent ## 0.001539 -0.002283 0.010774 Now some questions for us to consider: Let’s look at a summary of a few columns. Is there anything odd or curious? What does the 2 indicate in the apply function. What is Brent crude’s annualized mean “return”? Some immediate results ensue. summary(R) ## Index EUR.USD GBP.USD OIL.Brent ## Min. :2000-01-05 Min. :-2.523 Min. :-4.648 Min. :-19.891 ## 1st Qu.:2003-12-18 1st Qu.:-0.308 1st Qu.:-0.278 1st Qu.: -1.153 ## Median :2007-12-05 Median : 0.014 Median : 0.006 Median : 0.036 ## Mean :2007-12-19 Mean : 0.002 Mean :-0.002 Mean : 0.011 ## 3rd Qu.:2011-12-19 3rd Qu.: 0.322 3rd Qu.: 0.287 3rd Qu.: 1.249 ## Max. :2015-12-28 Max. : 3.464 Max. : 3.141 Max. : 18.130 Means are much less than medians. There are juge maximum and minimum returns. We can also look at acf and ccf, absolute returns, run GARCH models, and so on to hone our exploratory analysis. We ook up ??apply and read that the 2 indicates that we are calculating the mean for the second dimension of the data matrix, namely, the assets. Brent crude’s annualized mean return is calculated on a 252 average days traded in a year basis as: (1 + mean.R[3]/100)^252 - 1 ## OIL.Brent ## 0.02752 Some folks use 253 days. But this is all a back of the envelope computation. 6.3.2 Let’s keep moving on… So, what is the context? We have working capital with three main drivers of risk and return: two exchange rates and a commodity price. Over time we will want to know how these factors act and interact to produce EBITDA returns on Assets. Here EBITDA is Earnings Before Interest and Tax adding back in non-cash Depreciation and Amortization. Then, how does that combination compare with today’s reality and especially answer the CFO’s question of what to do about the millstone of working capital around the neck of EBITDA? Given this context, and the data we found earlier on, we then calculate the variance-covariance matrix. The diagonals of this matrix are the variances, so that the square root of the diagonal will yield standard deviations. The off-diagonals can be converted to correlations as needed. (mean.R &lt;- apply(R,2,mean)) ## EUR.USD GBP.USD OIL.Brent ## 0.001539 -0.002283 0.010774 (cov.R &lt;- cov(R)) ## EUR.USD GBP.USD OIL.Brent ## EUR.USD 0.3341 0.1939 0.1631 ## GBP.USD 0.1939 0.2539 0.1809 ## OIL.Brent 0.1631 0.1809 5.0572 (sd.R &lt;- sqrt(diag(cov.R))) ## remember these are in daily percentages ## EUR.USD GBP.USD OIL.Brent ## 0.5780 0.5039 2.2488 Now for some programming (quadratic that is…) but first let’s get more mathematical about the statement of the problem we are about to solve. In a mathematical nutshell we are formally (more tractable version to follow…) solving the problem of minimizing working capital factors risk, subject to target returns and a budget that says it all has to add up to our working capital position. We define weights as percentages of the total working capital position. Thus the weights need to add up to one. \\[ \\begin{array}{c} min_w w_T \\Sigma w \\\\ subject \\, to \\\\ 1^T w = 1\\\\ w^T \\mu = \\mu_0 \\end{array} \\] where \\(w\\) are the weights in each instrument. \\(\\Sigma\\) is the variance-covariance matrix we just estimated, cov.R. \\(1\\) is a vector of ones’s with length equal to the number of instruments. \\(\\mu\\) are the mean returns we just estimated, mean.R. \\(\\mu_0\\) is the target portfolio return. \\(T\\) is the matrix transpose. \\(min_w\\) means to find weights \\(w\\) that minimizes portfolio risk. The expression \\(w_T \\Sigma w\\) is our measure of portfolio risk and is a quadratic form that looks like this for two instruments: \\[ \\left[ \\begin{array}{cc} w_1 &amp; w_2 \\end{array} \\right] \\left[ \\begin{array}{cc} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{21} &amp; \\sigma_2^2 \\end{array} \\right] \\left[ \\begin{array}{c} w_1 \\\\ w_2 \\end{array} \\right] \\] Multiplied out we get the following quadratic formula for portfolio variance: \\[ \\sigma_P^2 = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + w_1 w_2 \\sigma_{12} + w_2 w_1 \\sigma_{21} \\] and because \\(\\sigma_{12} = \\sigma_{21}\\) this reduces a bit to \\[ \\sigma_P^2 = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + 2 w_1 w_2 \\sigma_{12} \\] Tedious? Definitely. But useful to explain the components of portfolio risk Two dashes of own asset risk \\(w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2\\), and Two dashes of relational risk \\(2 w_1 w_2 \\sigma_{12}\\) When \\(\\sigma_{12} &lt; 1\\) we have diversification. 6.3.3 Try this exercise Suppose we have two commodities (New York Harbor No. 2 Oil and Henry Hub Natural Gas) feeding a production process (Electricity Generation).These are the weights in this process: \\[ w = \\{ w_{oil} = -.5, w_{ng} = -.5, w_{ele} = 1.0 \\} \\] The percentage changes in terms of the prices of these commodities are given by: \\[ \\mu = \\{ \\mu_{oil} = 0.12, \\mu_{ng} = -0.09, \\mu_{ele} = 0.15 \\}. \\] Standard deviations are \\[ \\sigma = \\{ \\sigma_{oil} = 0.20, \\sigma_{ng} = 0.15, \\sigma_{ele} = 0.40 \\} \\] The correlation matrix is \\[ \\rho = \\left[ \\begin{array}{ccc} 1.0 &amp; 0.2 &amp; 0.6 \\\\ 0.2 &amp; 1.0 &amp; 0.4 \\\\ 0.6 &amp; 0.4 &amp; 1.0 \\end{array} \\right] \\] Using the formula \\[ \\Sigma = (\\sigma \\sigma^T) \\rho \\] we can calculate the variance-covariance matrix \\(\\Sigma\\) using our R knowledge of arrays. [Hint: t() is the transpose of an array so that \\(\\sigma^T\\) is t(sigma).] We can also calculate the portfolio mean return and portfolio standard deviation. Let’s Use this R code to put all of this into action. sigma &lt;- c(0.20, 0.15, 0.40) rho = c(1.0 , 0.2 , 0.6, 0.2 , 1.0 , 0.4, 0.6 , 0.4 , 1.0 ) (rho &lt;- matrix(rho, nrow = 3, ncol = 3)) ## [,1] [,2] [,3] ## [1,] 1.0 0.2 0.6 ## [2,] 0.2 1.0 0.4 ## [3,] 0.6 0.4 1.0 (Sigma2 &lt;- (sigma %*% t(sigma)) * rho ) ## [,1] [,2] [,3] ## [1,] 0.040 0.0060 0.048 ## [2,] 0.006 0.0225 0.024 ## [3,] 0.048 0.0240 0.160 The diagonals are the squared standard deviations. Next we tackle the portfolio average rate. w &lt;- c(-0.5, -0.5, 1.0) mu &lt;- c(0.12, -0.09, 0.15) (mu.P &lt;- t(w) %*% mu) ## [,1] ## [1,] 0.135 Now we compute the portfolio average level of “risk”. (Sigma.P &lt;- (t(w) %*% Sigma2 %*% w))^0.5 ## [,1] ## [1,] 0.3265 What does all of this mean? Running a power-generating plant (or refinery, or distribution chain, …) over time financially really means generating a spark spread: the margin between costs of inputs natural gas and oil (the negative or short position) and revenue from the output The average spark spread rate of return for this plant is 10.67%. The spark spread is the difference between the price of electricity and the price of input fuels on a MWh basis (megawatt-hour). The standard deviation of the spark spread is 32.65%. Our next job is to use these mechanics about portfolio means, standard deviations, and correlations to find the best set of weights that minimizes portfolio risk while attempting to achieve a target level of return. 6.4 Optimizing a Portfolio To perform the optimization task we turn to the quadprog quadratic programming package (yes, parabolas are indeed very useful). We worked out a two-asset example that showed us clearly that the objective function has squared terms (and interactive product terms too). These are the tell-tale signs that mark the portfolio variance as quadratic. It is all in the weights. After all of our wrangling above it is useful to define our portfolio optimization problem again here: \\[ \\begin{array}{c} min_{(w)} w_T \\Sigma w \\\\ subject \\, to \\\\ 1^T w = 1\\\\ w^T \\mu = \\mu_0 \\end{array} \\] On the other hand here is what quadprog does in its native format. \\[ \\begin{array}{c} min_d -d^T x +\\frac{1}{2} x^T D x \\\\ subject \\, to \\\\ A_{neq}^T x \\geq b_{neq} \\\\ A_{eq}^T x = b_{eq} \\end{array} \\] Now we need to transform these equations into portfolio paramters to solve our portfolio problem. We do this by setting \\[ A_{eq}^T = \\left[ \\begin{array}{c} 1^T \\\\ \\mu^T \\end{array} \\right] \\] This gives us a stack of equality constraints that looks like: \\[ \\left[ \\begin{array}{c} 1^T w \\\\ \\mu^T w \\end{array} \\right] = \\left[ \\begin{array}{c} 1 \\\\ \\mu_0 \\end{array} \\right] \\] We will allow short positions, like the spark spread experiment above. This means we will not yet impose inequality constraints like \\(w \\geq 0\\). Here is the setup code library(quadprog) Amat &lt;- cbind(rep(1,3),mean.R) ## set the equality constraints matrix mu.P &lt;- seq(min(mean.R - 0.0005), max(mean.R + 0.0005), length=300) ## set of 300 possible target portfolio returns sigma.P &lt;- mu.P ## set up storage for std dev&#39;s of portfolio returns weights &lt;- matrix(0, nrow=300, ncol = ncol(R)) ## storage for portfolio weights colnames(weights) &lt;- names.R Next we build the “efficient frontier.” This curve (a parabola…) traces optimal combinations of risk and return. For each combination there is an underlying set of weights calculated in successive optimizations, one for each target \\(\\mu\\). In effect this is a very specialized sensitivity analysis. for (i in 1:length(mu.P)) { bvec = c(1,mu.P[i]) ## constraint vector result = solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2) sigma.P[i] = sqrt(result$value) weights[i,] = result$solution } First, we plot all of the portfolio combinations. par(mfrow = c(1,1)) plot(sigma.P, mu.P, type=&quot;l&quot;, xlim=c(0,max(sd.R)*1.1), ylim=c(0,max(mean.R)*1.1), lty=3, lwd = 3) ## plot ## the efficient frontier (and inefficient portfolios ## below the min var portfolio) mu.free = 1.3/253 ## input value of risk-free interest rate points(0,mu.free,cex=1,pch=&quot;+&quot;) ## show risk-free asset Then we plot the point on the graph that represents the so-called risk-free (actually more like default-free) asset. Finally we deploy William Sharpe’s ratio. This number is the amount of portfolio premium per unit of risk (the “price” of risk) across all combinations of portfolio assets on the efficient frontier. Its maximum is the best combination for the risk in terms of returns. We figure out where (the index ind) the return to risk is along the frontier, record the weights associated with this unique point in risk-return space, and We find where (the index ind2) the minimum variance portfolio is. We then plot the “efficient frontier”: the efficient frontier will extend from the minimum variance portfolio (a “+” will mark the spot) up and out (in red). Anything else below this line is “inefficient” in the sense you get less and less return for mo Here is the code we just built up. sharpe =( mu.P-mu.free)/sigma.P ## compute Sharpe&#39;s ratios ind = (sharpe == max(sharpe)) ## Find maximum Sharpe&#39;s ratio options(digits=3) lines(c(0,2),mu.free+c(0,2)*(mu.P[ind]-mu.free)/sigma.P[ind],lwd=4,lty=1, col = &quot;blue&quot;) ## show line of optimal portfolios points(sigma.P[ind],mu.P[ind],cex=4,pch=&quot;*&quot;) ## show tangency portfolio ind2 = (sigma.P == min(sigma.P)) ## find the minimum variance portfolio points(sigma.P[ind2],mu.P[ind2],cex=2,pch=&quot;+&quot;) ## show min var portfolio ind3 = (mu.P &gt; mu.P[ind2]) ## finally the efficient frontier lines(sigma.P[ind3],mu.P[ind3],type=&quot;l&quot;, xlim=c(0,max(sd.R)*1.1), ylim=c(min(mean.R)*1.05, max(mean.R)*1.1) ,lwd=3, col = &quot;red&quot;) ## plot the efficient frontier text(sd.R[1],mean.R[1],&quot;EUR.USD&quot;,cex=1.15) text(sd.R[2],mean.R[2],&quot;GBP.USD&quot;,cex=1.15) text(sd.R[3],mean.R[3],&quot;OIL_Brent&quot;,cex=1.15) Altogether we have these results. The weights for the tangency portfolio (\"*\") are in weights[ind,] ## EUR.USD GBP.USD OIL.Brent ## 2.500 -1.807 0.306 sum(weights[ind,]) ## [1] 1 For a given notional amount in your portfolio, go long (buy) 250.% of that position in euros traded against USD, go short (sell) 180.7% of your aggregate position in euros traded against USD, and go long 30.6% in Brent. This means in the working capital accounts: $250 million should be denominated in euros Net of a short (payables?) position of $180 million With another $30 million priced in Brent crude. If our working capital is $100 million in euros, -$200 in sterling, and $200 exposed to Brent, we might think of ways to bring this more into line with the optimal positions we just derived, by changing contract terms and using swaps and other derivative instruments. 6.4.1 Try this exercise In this scenario we don’t allow short positions (negative weights). This means we impose the inequality constraint: \\[w \\geq 0\\] Further, We modify the Amat to Amat to cbind(rep(1,3),mean.R,diag(1,nrow=3)). We set the target return vectormu.P to seq(min(mean.R)+.0001, max(mean.R)-.0001, length=300). We also set the righthand-side vector bvec to c(1,mu.P[i],rep(0,3)). Let’s watch what happens using these questions as a guide. Are the tangency portfolio and minimum variance portfolio weights different? Explain how the constraint matrix and target return vector are different from the first run where \\(w\\) was allowed to be negative. Here is the new setup code where we no longer allow for short positions. library(quadprog) Amat &lt;- cbind(rep(1,3),mean.R,diag(1,nrow=3)) ## set the equality ND inequality constraints matrix mu.P &lt;- seq(min(mean.R) + 0.0001, max(mean.R) - 0.0001, length=300) ## set of 300 possible target portfolio returns sigma.P &lt;- mu.P ## set up storage for std dev&#39;s of portfolio returns weights &lt;- matrix(0,nrow=300,ncol=3) ## storage for portfolio weights Next we build the “efficient frontier.” All of this code is as before. for (i in 1:length(mu.P)) { bvec &lt;- c(1,mu.P[i],rep(0,3)) ## constraint vector with no short positions result &lt;- solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2) sigma.P[i] &lt;- sqrt(result$value) weights[i,] &lt;- result$solution } Then we plot, again the same as before. Plot all of the portfolio combinations. Plot the point on the graph that represents the so-called risk-free (actually more like default-free) asset. par(mfrow = c(1,1)) plot(sigma.P,mu.P,type=&quot;l&quot;,xlim=c(0,max(sd.R)*1.1),ylim=c(min(mean.R)*1.05, max(mean.R)*1.1),lty=3, lwd = 3) ## plot the efficient frontier (and inefficient portfolios ## below the min var portfolio) mu.free &lt;- 1.3/253 ## input value of risk-free interest rate points(0,mu.free,cex=1.5,pch=&quot;+&quot;) ## show risk-free asset And… Now for William Sharpe’s ratio, again as before, where This number is the amount of portfolio premium per unit of risk (the “price” of risk) across all combinations of portfolio assets on the efficient frontier. Its maximum is the best combination for the risk in terms of returns. We figure out where (the index ind) the return to risk is along the frontier, record the weights associated with this unique point in risk-return space, and Find where (the index ind2) the minimum variance portfolio is. Plot the “efficient frontier”: the efficient frontier will extend from the minimum variance portfolio (a “+” will mark the spot) up and out (in red). Anything else below this line is “inefficient” in the sense you get less and less return for more and m Here is the code (again) for using Sharpe’s ratio. par(mfrow = c(1,1)) plot(sigma.P,mu.P,type=&quot;l&quot;,xlim=c(0,max(sd.R)*1.1),ylim=c(min(mean.R)*1.05, max(mean.R)*1.1),lty=3, lwd = 3) ## plot the efficient frontier (and inefficient portfolios ## below the min var portfolio) mu.free &lt;- 1.3/253 ## input value of risk-free interest rate points(0,mu.free, cex=1.5, pch=&quot;+&quot;) ## show risk-free asset mu.free &lt;- 1.3/253 ## input value of risk-free interest rate points(0, mu.free, cex=1.5, pch=&quot;+&quot;) ## show risk-free asset sharpe =( mu.P-mu.free)/sigma.P ## compute Sharpe&#39;s ratios ind = (sharpe == max(sharpe)) ## Find maximum Sharpe&#39;s ratio options(digits=3) lines(c(0,2),mu.free+c(0,2)*(mu.P[ind]-mu.free)/sigma.P[ind],lwd=4,lty=1, col = &quot;blue&quot;) ## show line of optimal portfolios points(sigma.P[ind],mu.P[ind],cex=4,pch=&quot;*&quot;) ## show tangency portfolio ind2 = (sigma.P == min(sigma.P)) ## find the minimum variance portfolio points(sigma.P[ind2],mu.P[ind2],cex=1.5,pch=&quot;+&quot;) ## show min var portfolio ind3 = (mu.P &gt; mu.P[ind2]) lines(sigma.P[ind3],mu.P[ind3],type=&quot;l&quot;,xlim=c(0,max(sd.R)*1.1),ylim=c(min(mean.R)*1.05, max(mean.R)*1.1),lwd=3, col = &quot;red&quot;) ## plot the efficient frontier text(sd.R[1],mean.R[1],&quot;EUR.USD&quot;,cex=1.15) text(sd.R[2],mean.R[2],&quot;GBP.USD&quot;,cex=1.15) text(sd.R[3],mean.R[3],&quot;OIL.Brent&quot;,cex=1.15) Amat ## mean.R ## EUR.USD 1 0.00154 1 0 0 ## GBP.USD 1 -0.00228 0 1 0 ## OIL.Brent 1 0.01077 0 0 1 bvec ## [1] 1.0000 0.0107 0.0000 0.0000 0.0000 Here bvec changes for each of the three assets. Here we see one of them. The short position bvec has three zeros appended to it. The Amat constraint matrix has the identity matrix appended to it to represent \\(w_i = 0\\) in the formulation of the inequality constraints parsed by quantprog. The tangency of the line from the risk-free rate to the maximum Sharpe ratio point on the efficient frontier does not change. The weights are ## [1] 0.0108 0.0000 0.9892 The picture radically changes: Long working capital position with only a $1 million euro exposure. No pounding sterling exposure at all. A huge $99 million Brent exposure. 6.5 Summary We learned portfolio maths and finance: building feasible combinations of risk and return called the efficient frontier, figured out in 1952 by Harry Markowitz. We also looked at a simple example of tolerance for loss to imply the amount of collateral (risk-free asset) to hold. Using that idea and a ruler we drew a line to the efficient frontier to discover the best portfolio of exposures for a hypothetical working capital position: the one that maximizes the return for the risk, the ratio that William Sharpe figured out in 1966. 6.6 Further Reading McNeil et al. has sections on . Ruppert et al. has a chapter on portfolio selection upon which much of the R programming in this chapter is based. Bassett et al. has a way forward for more risk management minded analytics that uses quantile regression techniques. 6.7 Practice Laboratory 6.7.1 Practice laboratory #1 6.7.1.1 Problem 6.7.1.2 Questions 6.7.2 Practice laboratory #2 6.7.2.1 Problem 6.7.2.2 Questions 6.8 Project 6.8.1 Background 6.8.2 Data 6.8.3 Workflow 6.8.4 Assessment We will use the following rubric to assess our performance in producing analytic work product for the decision maker. The text is laid out cleanly, with clear divisions and transitions between sections and sub-sections. The writing itself is well-organized, free of grammatical and other mechanical errors, divided into complete sentences, logically grouped into paragraphs and sections, and easy to follow from the presumed level of knowledge. All numerical results or summaries are reported to suitable precision, and with appropriate measures of uncertainty attached when applicable. All figures and tables shown are relevant to the argument for ultimate conclusions. Figures and tables are easy to read, with informative captions, titles, axis labels and legends, and are placed near the relevant pieces of text. The code is formatted and organized so that it is easy for others to read and understand. It is indented, commented, and uses meaningful names. It only includes computations which are actually needed to answer the analytical questions, and avoids redundancy. Code borrowed from the notes, from books, or from resources found online is explicitly acknowledged and sourced in the comments. Functions or procedures not directly taken from the notes have accompanying tests which check whether the code does what it is supposed to. All code runs, and the R Markdown file knits to pdf_document output, or other output agreed with the instructor. Model specifications are described clearly and in appropriate detail. There are clear explanations of how estimating the model helps to answer the analytical questions, and rationales for all modeling choices. If multiple models are compared, they are all clearly described, along with the rationale for considering multiple models, and the reasons for selecting one model over another, or for using multiple models simultaneously. The actual estimation and simulation of model parameters or estimated functions is technically correct. All calculations based on estimates are clearly explained, and also technically correct. All estimates or derived quantities are accompanied with appropriate measures of uncertainty. The substantive, analytical questions are all answered as precisely as the data and the model allow. The chain of reasoning from estimation results about the model, or derived quantities, to substantive conclusions is both clear and convincing. Contingent answers (for example, “if X, then Y , but if A, then B, else C”) are likewise described as warranted by the model and data. If uncertainties in the data and model mean the answers to some questions must be imprecise, this too is reflected in the conclusions. All sources used, whether in conversation, print, online, or otherwise are listed and acknowledged where they used in code, words, pictures, and any other components of the analysis. 6.9 References Bassett, Gilbert W., Jr., Roger Koenker, and Gregory Kordas. 2004. Pessimistic Portfolio Allocation and Choquet Expected Utility. Journal of Financial Econometrics (2004) 2 (4): 477-492. Markowitz, Harry. 1952. Portfolio Selection. The Journal of Finance, Vol. 7, No. 1. (March, 1952), pp. 77-91. McNeill, Alexander J., Rudiger Frey, and Paul Embrechts. 2015. Quantitative Risk Management: Concepts, Techniques and Tools. Revised Edition. Princeton: Princeton University Press. Ruppert, David and David S. Matteson. 2015. Statistics and Data Analysis for Financial Engineering with R Examples, Second Edition. New York: Springer. Sharpe, William F. 1966. “Mutual Fund Performance.” Journal of Business, January 1966, pp. 119-138. "],
["aggregating-enterprise-risk.html", "Chapter 7 Aggregating Enterprise Risk 7.1 The Problem with Enterprise Risk 7.2 Let’s make copulas 7.3 Sklar’s in the house… 7.4 Analyze that… 7.5 Risk measures 7.6 Let’s build an app … 7.7 The simulation function 7.8 The UI 7.9 The server 7.10 Run the app 7.11 What else could we do? 7.12 Summary 7.13 Further Reading 7.14 Practice Laboratory 7.15 Project 7.16 References", " Chapter 7 Aggregating Enterprise Risk 7.1 The Problem with Enterprise Risk International Mulch &amp; Compost Company\\footnote(IM&amp;C os a ficitious company dreamt up and used by Brealey and Myers.) makes and distributes an emerging energy source made from guano and prairie grass briquets. IM&amp;C is about to go IPO. Corporate policy dictates that management must assess risks to equity annually and whether a circumstance dictates. Such a circumstance is an IPO. Management knows of at least three material risks: Customers defect so there is uncertainty in revenue growth. Suppliers stop competing on price, quantity, and quality so there is uncertainty in variable expense. There are major compliance breaches which impact fixed expense. No one knows much about these risks from history because this company is the first in its market to produce this very innovative product from bio-engineered guano. Very abundant prairie grass grows alongside every highway in North America. Management does have considerable experience in marketing, production, and operations. IM&amp;C ponders its SEC disclosure for the IPO where it will report its view of material risks. One question management knows someone will ask is how likely is it that the net operating margin will fall below, say, indicated earnings of $400 million. IM&amp;C thinks it needs to know how much capital is involved in this risk venture. 7.2 Let’s make copulas Our problem is: We have three major risk factors and each has their own distribution. We also know that they are somehow correlated. How can we aggregate the three into one risk measure that is tangible, and preserve the correlation? 7.2.1 We do this from scratch. Our first task is to generate multivariate normal variates that are correlated with one another. Here we relate three standard normal random variables together.A standard normal random variable has a mean, \\(\\mu = 0\\), and variance, \\(\\sigma^2 = 1\\). The variable sigma in the code below is the correlation matrix. library(mvtnorm) set.seed(1016) n.risks &lt;- 3 ## Number of risk factors m &lt;- n.risks n.sim &lt;- 1000 sigma &lt;- matrix(c(1, 0.4, 0.2, 0.4, 1, -0.8, 0.2, -0.8, 1), nrow=3) z &lt;- rmvnorm(n.sim, mean=rep(0, nrow(sigma)),sigma = sigma, method = &quot;svd&quot;) In the rmvnorm function svd stands for the “singular value decomposition” that allows us to fan the correlations across the z values. 7.2.2 Example Let’s use the .panels feature in the psych library to look at the variates so far. We also calculate two kinds of correlations, Spearman and Pearson. library(psych) cor(z,method=&#39;spearman&#39;) ## Textbook calculation ## [,1] [,2] [,3] ## [1,] 1.000 0.408 0.139 ## [2,] 0.408 1.000 -0.801 ## [3,] 0.139 -0.801 1.000 cor(z,method=&quot;pearson&quot;) ## Rank order calculation ## [,1] [,2] [,3] ## [1,] 1.000 0.426 0.152 ## [2,] 0.426 1.000 -0.811 ## [3,] 0.152 -0.811 1.000 pairs.panels(z) Here is the result. ## [,1] [,2] [,3] ## [1,] 1.000 0.408 0.139 ## [2,] 0.408 1.000 -0.801 ## [3,] 0.139 -0.801 1.000 ## [,1] [,2] [,3] ## [1,] 1.000 0.426 0.152 ## [2,] 0.426 1.000 -0.811 ## [3,] 0.152 -0.811 1.000 Notice how close the correlations are to the ones we specified in sigma. 7.3 Sklar’s in the house… Next we use a result from mathematical probability called Sklar’s theorem (1959): If \\(x\\) is a random variable with distribution \\(F\\), then \\(F(x)\\) is uniformly distributed in the interval \\([0, 1]\\). Let’s translate this idea into R and look at the resulting interactions. require(psych) u &lt;- pnorm(z) pairs.panels(u) We see that the Gaussian (normal) distribution has been reshaped into a uniform distribution, just as Sklar predicted. The idea around this theorem is the same as around the number 1. We can multiply any real number by one and get the real number back. This is an identity operation. (Please remember we are not trying to be mathematicians! My apologies to the mathematical community.) In a somewhat analogous way, the uniform distribution serves a role as an distribution identity operator.When we operate on the uniformly distributed random numbers with a distribution, we get back that distribution. But in this case the identity distribution has structure in it (correlations) that the new distribution inherits. A 3-D plot looks more interesting. In the Rstudio graphics device window we can the roll the cube around to see into the relationships among the random variables. Try this at home for an interactive experience. library(rgl) plot3d(u[,1],u[,2],u[,3],pch=20,col=&#39;orange&#39;) Now, we only need to select the marginal probabilities of the risks we are assessing and apply them to the dependently related ‘u’ variates. Suppose the marginal probability distributions for revenue growth is gamma, variable expense ratio is beta, and the fixed expense ratio is Student’s t distributed with these parameters: x1 &lt;- qgamma(u[,1],shape=2,scale=1) x2 &lt;- qbeta(u[,2],2,2) x3 &lt;- qt(u[,3],df=5) Nice outliers! Starting from a multivariate normal distribution we created dependent uniform variates. Using the dependent uniform variates we created dependent distributions of our choosing. factors.df &lt;- cbind(x1/10,x2,x3/10) colnames(factors.df) &lt;- c(&quot;Revenue&quot;, &quot;Variable Cost&quot;, &quot;Fixed Cost&quot;) pairs.panels(factors.df) cor(factors.df,method=&#39;spearman&#39;) ## Revenue Variable Cost Fixed Cost ## Revenue 1.000 0.408 0.139 ## Variable Cost 0.408 1.000 -0.801 ## Fixed Cost 0.139 -0.801 1.000 7.4 Analyze that… Now to use all of this simulation to project revenue, expense, and margin. revenue &lt;- 1000*(1+factors.df[,1]) variable.cost &lt;- revenue * factors.df[,2] fixed.cost &lt;- revenue * factors.df[,3] total.cost &lt;- variable.cost + fixed.cost operating.margin &lt;- revenue - variable.cost - fixed.cost analysis &lt;- cbind(revenue,total.cost,operating.margin) colnames(analysis) &lt;- c(&quot;Revenue&quot;, &quot;Cost&quot;, &quot;Margin&quot;) 7.4.1 Example Run pairs.panels using the analysis data frame. What do you see? Here’s the result. pairs.panels(analysis) What do we see? Variable and fixed cost aggregate into a distribution that is right-skewed. Margin has a high density across a broad range of potential outcomes. An increase (decrease) in cost will probably result in an increase (decrease) in revenue. Revenue and margin also seem to be counter cyclical, a non-intuitive result, but one that makes sense only by looking at the negative correlation between cost and margin. 7.5 Risk measures We are not yet done. The whole point of this analysis is to get consistent and coherent measures of risk to a consumer of the analysis, namely, the decision maker who is the CFO in this case. We define the value at risk, \\(VaR\\), as the \\(\\alpha\\) quantile of the performance metric of interest. Higher \\(\\alpha\\) means lower risk tolerance. Here is the relationship: \\[ Q(x,\\alpha) = F(x; Prob[X] &gt; \\alpha). \\] The metric \\(x\\) in this case is margin. Expected Shortfall, \\(ES\\), is then the mean of the margin beyond \\(VaR\\). The parameter \\(\\alpha\\) is the level of organizational risk tolerance. If \\(\\alpha = 0.99\\), then the organization would want risk capital to cover a potential loss of \\(VaR\\), and more conservatively, \\(ES\\). The organization is even more conservative the higher the \\(\\alpha\\). We purloin the R code from the market risk material here: ### Simple Value at Risk expected.margin &lt;- 400 ## Center margin loss on expected margin loss.rf &lt;- -(expected.margin - operating.margin) ## Assign metric of interest to reusable code summary(loss.rf) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -211 63 180 176 290 499 ## Always review a key variable&#39;s content alpha.tolerance &lt;- .99 ## Very intolerant! ## Remember that putting a variable assignment in parentheses also prints the result (VaR.hat &lt;- quantile(loss.rf, probs=alpha.tolerance, names=FALSE)) ## [1] 437 ### Just as simple Expected shortfall (ES.hat &lt;- mean(loss.rf[loss.rf &gt; VaR.hat])) ## [1] 456 Let’s plot the results. hist(loss.rf, xlab = &quot;Operating Margin&quot;, ylab = &quot;Frequency&quot;, main = &quot;Margin Loss Tolerance&quot;) abline(v = VaR.hat, col = &quot;red&quot;) Sklar provides us with a way to join together any set of distributions. It transforms correlated variates into a uniform distribution. The uniform distribution takes on the role of the number 1 in algebra. Anything multiplied by 1 returns itself. In a very loose way, the uniform distribution is the identity distribution, just like one is the identity term in algebra. So that whenever we operate on the uniform distribution we get back the same distribution – but this time with correlation. The rub is the starting point. Here we used the Gaussian (normal) distribution. This is not a very thickly tailed distribution, and it can be shown that extreme events are not dependent on one another using this distribution. This is NOT a useful feature ultimately. So, analysts use more thickly tailed distributions such as the Student-t and the generalized Pareto distribution (GPD) to get dependency far out into the tails. This is nearly perfect for risk managers and decision makers. 7.5.1 Example Let’s use this R code to modify the copula-making machine we just built. Instead of rmvnorm we will use rmvt to generate the correlated risk factors. This is called a t-copula. library(mvtnorm) library(psych) set.seed(1016) ## Freezes the random seed to reproduce results exactly n.risks &lt;- 3 ## Number of risk factors m &lt;- n.risks n.sim &lt;- 1000 sigma &lt;- matrix(c(1, 0.4, 0.2, 0.4, 1, -0.8, 0.2, -0.8, 1), nrow = m) z &lt;- rmvt(n.sim, delta = rep(0, nrow(sigma)),sigma = sigma, df = 6, type = &quot;shifted&quot;) Here are the results of our experiment. Let’s go through the paces. First we look at the z variates we simulated using the multivariate Student’s t-distribution. pairs.panels(z) We then run the uniform distribution generator (with correlation structure). Now, we only need to select the marginal probabilities of the risks we are assessing and apply them to the dependently related ‘u’ variates. Again suppose the marginal probability for revenue growth is gamma, for the variable expense ratio is beta, and fixed expense ratio is Student’s t distributed with these parameters: x1 &lt;- qgamma(u[,1],shape=2,scale=1) x2 &lt;- qbeta(u[,2],2,2) x3 &lt;- qt(u[,3],df=6) Starting from a multivariate Student’s t-distribution we created dependent uniform variates. Using the dependent uniform variates we created dependent distributions of our choosing. Next we combine the series into a data frame and review the scatterplot matrix. factors.df &lt;- cbind(x1/10,x2,x3/10) colnames(factors.df) &lt;- c(&quot;Revenue&quot;, &quot;Variable Cost&quot;, &quot;Fixed Cost&quot;) pairs.panels(factors.df) ## cor(df,meth=&#39;spearman&#39;) could also be run to verify the pairs.panels() Again, we have nice outliers! (We could run the qqplot to see this). Now to use all of this to project revenue, expense, and margin. revenue &lt;- 1000*(1+factors.df[,1]) variable.cost &lt;- revenue * factors.df[,2] fixed.cost &lt;- revenue * factors.df[,3] total.cost &lt;- variable.cost + fixed.cost operating.margin &lt;- revenue - variable.cost - fixed.cost analysis.t &lt;- cbind(revenue,total.cost,operating.margin) colnames(analysis.t) &lt;- c(&quot;Revenue&quot;, &quot;Cost&quot;, &quot;Margin&quot;) And again here is the scatterplot matrix. pairs.panels(analysis.t) We can… Experiment with different degrees of freedom to sensitive ourselves to the random numbers generated. Parameterize correlations. This means assign correlations to a variable and place that variable into the sigma matrix. This might get into trouble with an error. It would mean we would have to reassign the correlation. The mathematical problem is finding a positive definite variance-covariance matrix. How different are the value at risk and expected shortfall measures between the use of the Gaussian (normal) copula and the t-copula? Why should a decision maker care? All of that experimentation begs for an interactive decision tool. 7.6 Let’s build an app … The application (the “app”) will be housed in an R script that contain four architectural layers. Analytics User Interface (UI) Server Application generator 7.6.1 Analytics Libraries used in app processes Function that wraps analytical script Inputs from UI layer to server layer Outputs from server layer to UI layer 7.6.2 UI Slide bars for user to input range of parameters Plots to display results Text to report results 7.6.3 Server Run analytics with inputs from the UI and from a simulation function Generate outputs for UI 7.6.4 Application generator Here we run application function with UI and Server inputs 7.7 The simulation function The risk.sim function is a wrapper that pulls all of the risk aggregation together. In our scenario we vary the correlation coefficients. Shiny calls these input and this is what is given to risk.sim through the ui to risk.sim by way of the server. risk.sim then outputs the results into result called analysis.t. This is fetched by the server and rendered in the app. library(shiny) require(mvtnorm) require(psych) risk.sim &lt;- function(input) { ## Begin enterprise risk simulation set.seed(1016) ## Freezes the random seed to reproduce results exactly n.risks &lt;- 3 ## Number of risk factors m &lt;- n.risks n.sim &lt;- 1000 ## pull slider settings into the sigma correlation matrix sigma &lt;- matrix(c(1, input[1], input[2], input[1], 1, input[3], input[2], input[3], 1), nrow = m) z &lt;- rmvt(n.sim, delta = rep(0, nrow(sigma)),sigma = sigma, df = 6, type = &quot;shifted&quot;) u &lt;- pt(z, df = 6) x1 &lt;- qgamma(u[,1],shape=2,scale=1) x2 &lt;- qbeta(u[,2],2,2) x3 &lt;- qt(u[,3],df=6) factors.df &lt;- cbind(x1/10,x2,x3/10) colnames(factors.df) &lt;- c(&quot;Revenue&quot;, &quot;Variable Cost&quot;, &quot;Fixed Cost&quot;) revenue &lt;- 1000*(1+factors.df[,1]) variable.cost &lt;- revenue * factors.df[,2] fixed.cost &lt;- revenue * factors.df[,3] total.cost &lt;- variable.cost + fixed.cost operating.margin &lt;- revenue - variable.cost - fixed.cost analysis.t &lt;- cbind(revenue,total.cost,operating.margin) colnames(analysis.t) &lt;- c(&quot;Revenue&quot;, &quot;Cost&quot;, &quot;Margin&quot;) return(analysis.t) } 7.8 The UI Here is a mock-up of the screen we will implement in Shiny. UI Design Here is what the Shiny UI code looks like: ui &lt;- fluidPage( titlePanel(&quot;Enterprise Risk Analytics&quot;), sidebarLayout( sidebarPanel( sliderInput(inputId = &quot;cor.1&quot;, label = &quot;Set the Revenue - Variable Cost Correlation&quot;, value = 0.5, min = 0.1, max = 0.9), sliderInput(inputId = &quot;cor.2&quot;, label = &quot;Set the Revenue - Variable Cost Correlation&quot;, value = 0.5, min = 0.1, max = 0.9), sliderInput(inputId = &quot;cor.3&quot;, label = &quot;Set the Variable - Fixed Cost Correlation&quot;, value = 0.5, min = 0.1, max = 0.9) ), mainPanel( plotOutput(&quot;pairs.1&quot;) ) ) ) 7.9 The server The Shiny server is a function The function gets inputs from the UI Generates outputs that are sent back to the UI server &lt;- function(input, output) { output$pairs.1 &lt;- renderPlot({ analysis.t &lt;- risk.sim(c(input$cor.1, input$cor.2, input$cor.3)) pairs.panels(analysis.t) }) } 7.10 Run the app This function call the Shiny application process with inputs ui and server. shinyApp(ui = ui, server = server) Here is what you see when you run the app in the script window of Rstudio. ERM Application Screenshot 7.11 What else could we do? Build tabs for various components of the analysis Use tables to summarize metrics (e.g., VaR, ES) Whatever else the consumer of this analysis would need 7.12 Summary More and more R, finance, risk, statistics, probability Multivariate simulation of risk factors Math to R translation Graphics Normal, t, gamma, and beta distributions VaR and ES Aggregation of multiple risk factors Introduction to Shiny and application development 7.13 Further Reading 7.14 Practice Laboratory 7.14.1 Practice laboratory #1 7.14.1.1 Problem 7.14.1.2 Questions 7.14.2 Practice laboratory #2 7.14.2.1 Problem 7.14.2.2 Questions 7.15 Project 7.15.1 Background 7.15.2 Data 7.15.3 Workflow 7.15.4 Assessment We will use the following rubric to assess our performance in producing analytic work product for the decision maker. The text is laid out cleanly, with clear divisions and transitions between sections and sub-sections. The writing itself is well-organized, free of grammatical and other mechanical errors, divided into complete sentences, logically grouped into paragraphs and sections, and easy to follow from the presumed level of knowledge. All numerical results or summaries are reported to suitable precision, and with appropriate measures of uncertainty attached when applicable. All figures and tables shown are relevant to the argument for ultimate conclusions. Figures and tables are easy to read, with informative captions, titles, axis labels and legends, and are placed near the relevant pieces of text. The code is formatted and organized so that it is easy for others to read and understand. It is indented, commented, and uses meaningful names. It only includes computations which are actually needed to answer the analytical questions, and avoids redundancy. Code borrowed from the notes, from books, or from resources found online is explicitly acknowledged and sourced in the comments. Functions or procedures not directly taken from the notes have accompanying tests which check whether the code does what it is supposed to. All code runs, and the R Markdown file knits to pdf_document output, or other output agreed with the instructor. Model specifications are described clearly and in appropriate detail. There are clear explanations of how estimating the model helps to answer the analytical questions, and rationales for all modeling choices. If multiple models are compared, they are all clearly described, along with the rationale for considering multiple models, and the reasons for selecting one model over another, or for using multiple models simultaneously. The actual estimation and simulation of model parameters or estimated functions is technically correct. All calculations based on estimates are clearly explained, and also technically correct. All estimates or derived quantities are accompanied with appropriate measures of uncertainty. The substantive, analytical questions are all answered as precisely as the data and the model allow. The chain of reasoning from estimation results about the model, or derived quantities, to substantive conclusions is both clear and convincing. Contingent answers (for example, “if X, then Y , but if A, then B, else C”) are likewise described as warranted by the model and data. If uncertainties in the data and model mean the answers to some questions must be imprecise, this too is reflected in the conclusions. All sources used, whether in conversation, print, online, or otherwise are listed and acknowledged where they used in code, words, pictures, and any other components of the analysis. 7.16 References "],
["appendix-1-r-environment-for-financial-analytics.html", "Chapter 8 Appendix 1 – R environment for financial analytics 8.1 Setting Up R for Analytics 8.2 Nomenclature", " Chapter 8 Appendix 1 – R environment for financial analytics 8.1 Setting Up R for Analytics 8.1.1 Motivation The general aim of this section is to situate the software platform R as part of your learning of statistics, operational research, and data analytics that accompanies nearly every domain of knowledge, from epidemiology to financial engineering. The specific aim of this appendix is to provide detailed instructions on how to install R an integrated development environment (IDE), RStudio, and a documentation system R Markdown on a personal computing platform (also known as your personal computer). This will enable us to learn the statistical concepts usually included in an analytics course with explanations and examples aimed at the appropriate level. This appendix purposely does not attempt to teach you about R’s many fundamental and advanced features. 8.1.2 Some useful R resources There are many R books useful for managing implementation of models in this course. Three useful R books include: Paul Teetor, The R Cookbook Phil Spector, Data Manipulation with R Norman Matloff, The Art of R Programming: A Tour of Statistical Software Design John Taveras, R for Excel Users at https://www.rforexcelusers.com/book/. The first one will serve as our R textbook. The other books are extremely valuable reference works. You will ultimately need all three (and whatever else you can get your hands on) in your professional work. John Taveras’s book is an excellent bridge and compendium of Excel and R practices. Much is available in books, e-books, and online for free. This is an extensive online user community that links expert and novice modelers globally. The standard start-up is at CRAN http://cran.r-project.org/manuals.html. A script in the appendix can be dropped into a workspace and played with easily. Julian Faraway’s https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf is a fairly complete course on regression where you can imbibe deeply of the many ways to use R in statistics. Along econometrics lines is Grant Farnsworth’s https://cran.r-project.org/doc/contrib/Farnsworth-EconometricsInR.pdf. Winston Chang’s http://www.cookbook-r.com/ and Hadley Wickham’s example at http://ggplot2.org/ are online graphics resources. Stack Overflow is a programming user community with an R thread at http://stackoverflow.com/questions/tagged/r. The odds are that if you have a problem, error, or question, it has already been asked, and answered, on this site. For using R Markdown there is a short reference at https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf. Cosma Shalizi has a much more extensive manual at http://www.stat.cmu.edu/~cshalizi/rmarkdown/. 8.1.3 Install R on your computer Directions exist at the R website, &lt;(http://cran.rproject.org/)&gt; for installing R. There are several twotorials, including some on installation that can be helpful at http://www.twotorials.com/. Here are more explicit instructions that tell you what to do. Download the software from the CRAN website. There is only one file that you need to obtain (a different file depending on the operating system). Running this file begins the installation process which is straight-forward in most, if not all, systems. Download R from the web. Go the R home page at http://cran.us.r-project.org/. If you have Windows (95 or later), then perform these actions. Click on the link Windows (95 and later), then click on the link called base, and finally click on rw1071.exe (or the most recent version which could have a larger number in the file name). This begins the download of a file whose size is currently about 20MB. After the download is complete, double-click on the downloaded file and follow the on screen installation instructions. If you have Macintosh (OS X), then perform these actions. Click on the link MacOS (System 8.6 to 9.1 and MacOS X), then click on rm171.sit (or the most recent version which could have a larger number in the file name) which begins the download. When given a choice to unstuff or save, choose save and save it on your desktop. Double-click on the downloaded file. Your Mac will unstuff the downloaded file and create an R folder. Inside this folder, there are many files including one with the R logo. You may drag a copy of this to your panel and then drag the whole R folder to your Applications folder (located on the hard drive). After completing this, you can drag the original downloaded file to your trash bin. 8.1.4 Install RStudio Every software platform has a graphical user interface (“GUI” for short). One of the more popular GUIs, and the one used exclusively in this course, is provided by RStudio at http://www.rstudio.com. RStudio is a freely distributed integrated development environment (IDE) for R. It includes a console to execute code, a syntax-highlighting editor that supports direct code execution, as well as tools for plotting, reviewing code history, debugging code, and managing workspaces. In the following steps you will navigate to the RStudio website where you can download R and RStudio. These steps assume you have a Windows or Mac OSX operating system. Click on https://www.rstudio.com/products/RStudio/ and navigate down to the Download Desktop button and click. Click on the Download button for the RStudio Desktop Personal License choice. Navigate to the sentence: “RStudio requires R 2.11.1+. If you don’t already have R, download it here.” If you have not downloaded R (or want to again), click on here. You will be directed to the https://cran.rstudio.com/ website in a new browser tab. In the CRAN site, click on Download R for Windows, or Download R for (MAC) OS X depending on the computer you use. This action sends you to a new webpage in the site. Click on base. This action takes you to the download page itself. If you have Windows Click on Download R 3.3.2 for Windows (62 megabytes, 32/64 bit) (as of 11/8/2016; other version numbers may appear later than this date). A Windows installer in an over 70 MB R-3.3.2-win.exe file will download through your browser. In the Chrome browser, the installation-executable file will reside in a tray at the bottom of the browser. Click on the up arrow to the right of the file name and click Open in the list box. Follow the many instructions and accept default values throughout. Use the default Core and 32-Bit files if you have a Windows 32-bit Operating System. You may want to use 64-Bit files if that is your operating system architecture. You can check this out by going to the Control Panel, then System and Security, then System, and look up the System Type:. It may read for example 32-bit Operating System. Click Next to accept defaults. Click Next again to accept placing R in the startup menu folder. Click Next again to use the R icon and alter and create registries. At this point the installer extracts files, creates shortcuts, and completes the installation. Click Finish to finish. If you have a MAC OS X Click on Download R 3.3.2 for MACs (62 megabytes, 32/64 bit) (as of 11/8/2016; other version numbers may appear later than this date). A Windows installer in an over 70 MB R-3.3.2-win.exe file will download through your browser. When given a choice to unstuff or save, choose save and save it on your desktop. Double-click on the downloaded file. Your Mac will unstuff the downloaded file and create an R folder. Inside this folder, there are many files including one with the R logo. Inside the R folder drag a copy of R logo file to your panel and then drag the whole R folder to your Applications folder (located on the hard drive). Now go back to RStudio browser tab. Click on RStudio 1.0.44 - Windows Vista/7/8/10 or RStudio 1.0.44 - MAC OS X to download RStudio. Executiable files will download. Follow the directions exactly, and similarly, to the ones above. 8.1.5 Install R Markdown Click on RStudio in your tray or start up menu. Be sure you are connected to the Internet. A console panel will appear. At the console prompt &gt; type install.packages(&quot;rmarkdown&quot;) This action will install the RMarkdown package. This package will enable you to construct documentation for your work in the course. Assignments will be documented using RMarkdown for submission to the learning management system. This extremely helpful web page, http://rmarkdown.rstudio.com/gallery.html, is a portal to several examples of R Markdown source files that can be loaded into RStudio, modified, and used with other content for your own work. 8.1.6 Install LaTex R Markdown uses a text rendering system called LaTeX to render text, including mathematical and graphical content. Install the MikTeX document rendering system for Windows or MacTeX document rendering system for Mac OS X. For Windows, navigate to the https://miktex.org/download page and go to the 64- or 32- bit installer. Click on the appropriate Download button and follow the directions. Be very sure you select the COMPLETE installation. Frequently Asked Questions (FAQ) can be found at https://docs.miktex.org/faq/. If you have RStudio already running, you will have to restart your session. For MAC OS X, navigate to the http://www.tug.org/mactex/ page and download the MacTeX system and follow the directions. This distribution requires Mac OS 10.5 Leopard or higher and runs on Intel or PowerPC processors. Be very sure you select the FULL installation. Frequently Asked Questions (FAQ) can be found at https://docs.miktex.org/faq/. If you have RStudio already running, you will have to restart your session. FAQ can be found at http://www.tug.org/mactex/faq/index.html. 8.1.7 R Markdown Open RStudio and see something like this screenshot… You can modify the position and content of the four panes by selecting View &gt; Panes &gt; Pane Options. Under File &gt; New File &gt; Rmarkdown a dialog box invites you to open document, presentation, Shiny, and other files. Upon choosing documents you may open up a new file. Under File &gt; Save As save the untitle file in an appropriate directory. The R Markdown file extension Rmd will appear in the file name in your directory. When creating a new Rmarkdown file, RStudio deposits a template that shows you how to use the markdown approach. You can generate a document by clicking on knit in the icon ribbon attached to the file name tab in the script pane. If you do not see knit, then you might need to install and load the knitr package with the following statements in the R console. You might need also to restart your RStudio session. install.packages(&quot;knitr&quot;) library(knitr) The Rmd file contains three types of content: An (optional) YAML header surrounded by --- on the top and the bottom of YAML statements. YAML is “Yet Another Markdown (or up) Language”. Here is an example from this document: --- title: &quot;Setting Up R for Analytics&quot; author: &quot;Bill Foote&quot; date: &quot;November 11, 2016&quot; output: pdf_document --- Chunks of R code surrounded by ``` (find this key usually with the ~ symbol). Text mixed with text formatting like # heading and _italics_ and mathematical formulae like $z = \\frac{(\\bar x-\\mu_0)}{s/\\sqrt{n}}$ which will render \\[z = \\frac{(\\bar x-\\mu_0)}{s/\\sqrt{n}}\\]. When you open an .Rmd file, RStudio provides an interface where code, code output, and text documentation are interleaved. You can run each code chunk by clicking the Run icon (it looks like a play button at the top of the chunk), or by pressing Cmd/Ctrl + Shift + Enter. RStudio executes the code and displays the results in the console with the code. You can write mathematical formulae in an R Markdown document as well. For example, here is a formula for net present value. $$ NPV = \\sum_{t=0}^{T} \\frac{NCF_t}{(1+WACC)^t} $$ This script will render \\[ NPV = \\sum_{t=0}^{T} \\frac{NCF_t}{(1+WACC)^t} \\] Here are examples of common in file text formatting in R Markdown. Text formatting ------------------------------------------------------------ *italic* or _italic_ **bold** __bold__ `code` superscript^2^ and subscript~2~ Headings ------------------------------------------------------------ # 1st Level Header ## 2nd Level Header ### 3rd Level Header Lists ------------------------------------------------------------ * Bulleted list item 1 * Item 2 * Item 2a * Item 2b 1. Numbered list item 1 1. Item 2. The numbers are incremented automatically in the output. Links and images ------------------------------------------------------------ &lt;http://example.com&gt; [linked phrase](http://example.com) ![optional caption text](path/to/img.png) Tables ------------------------------------------------------------ First Header | Second Header ------------- | ------------- Content Cell | Content Cell Content Cell | Content Cell Math ------------------------------------------------------------ $\\frac{\\mu}{\\sigma^2}$ \\[\\frac{\\mu}{\\sigma^2}] More information will be provided on R Markdown documentation throughout the course. 8.1.8 jaRgon (directly copied from Patrick Burns at http://www.burns-stat.com/documents/tutorials/impatient-r/jargon/, and annotated a bit, for educational use only.) atomic vector An object that contains only one form of data. The atomic modes are: logical, numeric, complex and character. attach The act of adding an item to the search list. You usually attach a package with the require function, you attach saved files and objects with the attach function. data frame A rectangular data object where each column may be a different type of data. Conceptually a generalization of a matrix, but implemented entirely differently. factor A data object that represents categorical data. It is possible (and often unfortunate) to confuse a factor with a character vector. global environment The first location on the search list, and the place where objects that you create reside. See search list. list A type of object with possibly multiple components where each component may be an arbitrary object, including a list. matrix A rectangular data object where all cells have the same data type. Conceptually a specialization of a data frame, but implemented entirely differently. This object has rows and columns. package A collection of R objects in a special format that includes help files and such. Most packages primarily or exclusively contain functions, but some packages exclusively contain datasets. search list The collection of locations that R searches for objects when it is evaluating a command. 8.2 Nomenclature Here is a table of symbols used throughout the text. "],
["appendix-useful-financial-concepts.html", "Chapter 9 Appendix: useful financial concepts 9.1 Net present value 9.2 Internal rate of return", " Chapter 9 Appendix: useful financial concepts In this appendix we will discuss returns, especially continuously compounded returns, net present value, internal rate of return, foreign exchange markets, and other fascinating topics. 9.0.1 A question is raised Why do we take differences of logarithms to calculate returns? A great question deserves a story. You can find the source code here. 9.0.2 Jacob Bernoulli has a story to tell In 1683, Jacob Bernoulli posed this problem in compound interest: Invest 1 ducat for 1 year at 100% interest. This yields \\[ I = I ( 1 + r) ^t = 1 (1 + 1)^1 = 2 \\] \\indent where \\(I =\\) investment at the beginning of the year, \\(r =\\) the return in one year, and \\(t =\\) the number of years (in this case simply 1 year). Invest 1 ducat for one-half a year at annual rate $r = $ 1 and semi-annual equivalent $r/2 = $0.5. Reinvest what you earn in one-half a year for another one-half a year. This yields \\[ I = I ( 1 + r/2) (1 + r/2) = 1 (1 + 0.5)^2 = 2.25 \\] 3. Invest 1 ducat for one-quarter of a year at annual rate $r = $ 1 and quarterly equivalent $r/4 = $0.25. Reinvest what you earn in a quarter for another quarter. Invest the two quarter balance for the third. Invest the three quarter balance for the fourth and final quarter. This yields \\[ I = I ( 1 + r/4) (1 + r/4) (1 + r/4) (1 + r/4) = 1 (1 + 0.25)^4 = 2.441 \\] Cut up the quarter into smaller and smaller time units, also known as compounding periods. Say we are now at an investment period of only one day. There are 365 days in this year. There are thus 365 compounding periods. Using the same logic we get \\[ I = I ( 1 + r/365) (1 + r/365) ... (1 + r/365) = 1 (1 + 0.003)^{365} = 2.715 \\] In general, Bernoulli was calculating the expression \\[ \\left(1 + \\frac{1}{n}\\right)^n \\] as \\(n \\rightarrow \\infty\\), that is, as the number of compounding periods \\(n\\) got ever larger. In the limit, this expression converges to the transcendental number \\(e=\\) 2.718. In finance, this is the equivalent of earning 2.718 ducats in a year for an investment of 1 ducat at the beginning of the year at 100% rate of return per year compounded continuously. Here is a picture of the calculated approximation of \\(e\\) versus the number of compounding periods \\(n\\). library(ggplot2) # to plot e approximation versus number of compounding periods library(latex2exp) # to build latex math expressions in ggplot using TeX to generatelatex string options(digits = 5) # overall rounding r &lt;- 1 # 100% annual rate of return as in Bernoulli (1683) t &lt;- 1 # one year as in Bernoullil (1683) n &lt;- 1:365 # number of compounding periods e_approx &lt;- (1 + r/n)^n # vector of e approximations e_asymptote &lt;- round(exp(r*t), 5) # need to round result for ggplot e_label &lt;- paste0(&quot;e = &quot;, e_asymptote) # horizontal label for asymptote e_title &lt;- &quot;Jacob Bernoulli&#39;s 1683 compound interest experiment&quot; # graph title y_label &lt;- TeX(&#39;$(1 + 1/n)^n$&#39;) # rmarkdown needs \\\\ for latex escape e_df &lt;- data.frame(e_approx = e_approx, n = n) # data frame needed for ggplot ggplot(e_df, aes(x = n, y = e_approx)) + ylab(y_label) + xlab(&quot;n compounding periods&quot;) + geom_line(color = &quot;blue&quot;, size = 1.5) + geom_hline(yintercept = e_asymptote, color = &quot;red&quot;) + annotate(&quot;text&quot;, label = e_label, x = 10, y = e_asymptote + 0.04, color = &quot;darkred&quot; ) + ggtitle(e_title) Calculations are rounded to 5 decimal places. 9.0.3 Where’s the finance? In finance, and in Bernoulli’s example, \\(r =\\) 1.00, or an annual return of 100%. For a duration of \\(t=\\) 1 year, an investor would earn \\[ Ie^{rt} = 1 \\times e^{1.00 \\times 1} = 2.71828 \\] if the investor could earn continuously through every minute, every second of every day in the year. For example, let the investor earn \\(r=\\) 5% per year on a $1,000 investment for one month. The number of years in a single month is a fraction \\(t=1/12\\) 0.08333. The investor would have at the end of a month \\[ Ie^{rt} = 1000 \\times e^{0.05 \\times (1/12)} = 1004.17536 \\] We calculate the inverse of \\(e^{rt}\\) to calculate, isolate \\(r\\). This \\(r\\) is the annually continuously compounded rate of return. In algebra, \\(e\\) is the base and \\(rt\\) is the exponent. The inverse of any exponential term is the logarithm. The natural logarithm, \\(ln()\\), is the logarithm to the base \\(e\\). For any base raised to a power, the logarithm answers the question: “What is the power?” In finance, we have a future value. With continuous compounding, we ask: “What is the corresponding rate of return?” Here date 0 is the beginning of the month and date 1 is the end of the month. Let’s name the result at the end of one month \\(I_1\\) and the investment at the beginning of the month \\(I_0\\). Then we have with continuous compounding \\[ I_1 = I_0 e^{rt} \\] Divide both sides of the this equation by \\(I_0\\) to get \\[ \\frac{I_1}{I_0} = e^{rt} \\] Then take logarithms of both sides to isolate \\(rt\\) to get \\[ ln \\left(\\frac{I_1}{I_0}\\right) = ln(e^{rt}) = rt \\] From algebra we know that the log of a ratio is the same as the difference of the logs of the numerator and denominator. Then also dividing both sides by \\(t\\), the holding period, we have \\[ r = \\frac{ln(I_1)-ln(I_0)}{t} = \\frac{6.91192 - 6.90776}{0.08333} = 0.05 \\] We must remember that this is the annual rate of return continuously compounded over a one month holding period. 9.0.4 The answer please? The difference in logarithms is equivalent to the logarithm of the ratio of future to present value. The difference must be divided by the fraction (or multiple) of the annual holding period. Subtract one and it looks a lot like a percentage change in value, also known as the rate of return. 9.1 Net present value 9.1.1 What is net present value? Net present value (NPV) is the present value of the cash flows at the required rate of return of your project compared to an initial investment. In practical terms, it’s a method of calculating your return on investment, or ROI, for a project or expenditure. By looking at all of the money we expect to make from the investment and translating those returns into today’s dollars, you can decide whether the project is worthwhile. We translate all of the future cash flows into today’s dollars (or euros, yen, name it) by finding the present value of those flows. Present value is the amount of money today that fans out into future cashflows at a rate of return for each future period in which cash flow occurs. This process and metric is also referred to as the time value of money. 9.1.2 What do companies typically use it for? When a manager needs to compare projects and decide which ones to pursue, there are generally three options available: internal rate of return, payback method, and net present value. There are two reasons why financial analysts and investors use NPV. NPV considers the time value of money, translating future cash flows into today’s dollars. It provides a concrete number that managers can use to easily compare an initial outlay of cash against the present value of the return. It’s far superior to the payback method, which is the most commonly used. The attraction of payback is that it is simple to calculate and simple to understand: when will you make back the money you put in? But it doesn’t take into account that the buying power of money today is greater than the buying power of the same amount of money in the future. That’s what makes NPV a superior method. With financial calculators and Excel spreadsheets, NPV is now nearly just as easy to calculate as payback. Managers also use NPV to decide whether to make large purchases, such as equipment or software. It’s also used in mergers and acquisitions (though it’s called the discounted cash flow model in that scenario). In fact, it’s the model that Warren Buffet uses to evaluate companies. Any time a company is using today’s dollars for future returns, NPV is a solid choice. 9.1.3 How can we calculate NPV? There is an NPV function in Excel that makes it easy once you’ve entered your stream of costs and benefits. Many financial calculators, including ones on smart phones, also include an NPV function. Here is the math behind NPV. Let’s suppose we have this information about a contract your very good business associate wants to sell you for $950 today. You hope you can earn at least what the market makes. That amount is 10% per year. This means that if you put $1 into the earning machine, you expect $1.10 in one year. Year Cash Flow 1 100 2 1100 How do we know if the price of the contract is right? We find the contract’s net present value. Here’s how we do it. The calculation looks like this: Find the amount of money today that translates into $100 in one year at 10% per year, that is, find the present value of $100 in one year at 10%. Call this number PV1. \\[ PV_1(1+0.10)=100 \\] Solve for PV_1 to get \\[ PV_1=1001.10=90.91 \\] Find the amount of money today that translates into $1,100 in two years at 10% per year, that is, find the present value of $1,100 in two years at 10%. Call this number \\(PV_2\\). We do this in two steps. In one year \\(PV_2\\) will grow at 10% this way: \\[ PV_2(1+0.10) \\] From the end of year one to the end of year two, PV2(1.1) will grow into this amount: \\[ PV_2(1+0.1)(1+0.1) \\] All of this will yield $1,100: \\[ PV2(1.1)(1.1)=PV2(1.1)2=1100 \\] Solve for the amount of money today that grows at 10% per year to yield $1,100 in two years, PV2: PV2=11001.12=909.91 Pull all of this into the Net Present Value (NPV). Today you pay $950, or a negative cash flow of 50. Today you would have in present value $90.01 for the first year’s worth of cash flow, and $909.91 for the second year’s worth of cash flow. \\[ NPV=−950+PV1+PV2=−950+90.91+909.91=50 \\] If NPV &gt; 0, then today’s value of future cash flows, given your expectation of return, exceeds what you paid for the investment. If NPV = 0, you have break-even. If NPV &lt; 0, then the investment costs more than your expectations of value. Since NPV = 50 &gt; 0, you would believe that the investment is more valuable than what you pay for it, so, yes, indeed you might take up your friend on this offer. 9.1.4 What can go wrong? There are three things that managers need to be aware of when using NPV. The result might be difficult to explain to others. You would need to establish that the goal is to find value. But this depends on your expectations of the growth in your initial investment. If you believe that there is more value in this contract than the asking price, then you should buy the contract. NPV helps you understand that phrase “more value.” Your expectations of the future growth of money might be wrong. This means that the rate above is higher or lower than 10%. For example if the rate is really 15%, then \\[ NPV=−950+\\frac{100}{1.15^1}+\\frac{1100}{1.15^2}=−31.29 \\] In this case NPV &lt; 0 and this would be a bad value deal. You would be paying more that you think the contract is worth, in fact $31.29 more. You do not receive all of the cash flow that is promised. Suppose the contract defaults (a little bit!) and your business associate can only assure you that she can pay you 75 cents on the dollar in a worse case scenario. NPV=−950+(0.75)100/1.10+(0.75)1100/1.102=−200 Here we figure we only make $75 in the first year for sure and $825 in the second year. Quite a potential loss! If we truly believe that this scenario could occur, then this is indeed a really bad deal! 9.2 Internal rate of return 9.2.1 What is the internal rate of return? The internal rate of return (IRR) is the discount rate when the net present value (NPV) is zero, or breaks even. If the cash flows come from a bond or other fixed income instrument, then the IRR is also called the yield and also the yield to maturity (YTM). When the NPV breaks even, the present value of negative (investing) cash flows equals the present value of positive (earning) cash flows. It is a true inflection point of equilibrium that balances cash flows for a project, financial instrument, portfolio, or contract. At this point no one would want to buy or sell. In fact at that point there are no further value gains or losses. Gains exactly equal losses. This indeed is the definition of a market clearing equilibrium. What do companies typically use IRR for? If anything at all, the IRR is a benchmark measure that allows us to compare two sets of cash flows, one each from two projects, contracts, or investments. Many managers use, especially in financial services organizaitons, use IRR as a rate of return compared with a hurdle rate. The hurdle rate is usually the required rate of return for the risks undertaken in investment. The risks undertaken involve any potential loss of cash flows that emanate from the investment. If the IRR &gt; hurdle rate, then the project will have a positive NPV. When the IRR = hurdle rate, NPV = 0. When IRR &lt; hurdle rate, NPV &lt; 0. The manager might as well use NPV. Still, like NPV, the IRR uses the time value of money as its basis for concluding the value-worthiness of a set of cash flows. In this way the IRR is also better than the payback method, which is the most commonly used. The attraction of payback is that it is simple to calculate and simple to understand: when will you make back the money you put in? But it doesn’t take into account that the buying power of money today is greater than the buying power of the same amount of money in the future. That’s what makes NPV and IRR better methods. With financial calculators and Excel spreadsheets, IRR is now nearly just as easy to calculate as payback. 9.2.2 How can we calculate IRR? The calculation of IRR requires the calculation of the NPV of cashflows. There is an IRR function in Excel that makes it easy once you’ve entered your stream of costs and benefits. Many financial calculators, including ones on smart phones, also include an IRR function. Here is the math behind IRR. Let’s suppose we have this information about a contract your very good (and credible and reliable) business associate wants to sell you for $950 today. You hope you can earn at least what the market makes. That amount is 10% per year. This means that if you put $1 into the earning machine, you expect $1.10 in one year. Year Cash Flow 1 100 2 1100 How do you know if the price of the contract is right? Compare the contract’s internal rate of return with your hurdle rate of 10%. Here’s how we do it. Calculate the NPV of the cash flows at the hurdle rate. We do this calculation for these cash flows in the NPv section. If the NPV &gt; 0, then the IRR will be greater than the hurdle rate. Guess a 200 basis point adder to the hurdle rate as your initial estimate of the IRR. \\[ NPV=−950+1001.10+11001.102=50 \\] Since the NPV &gt; 0, recalculate the NPV at the initial IRR estimate of 10% + 2%. Recalculate NPV. If this recalculation yields a positive NPV, continue to increase the discount rate, in this case, to 13%. If the NPV &lt; 0, then the IRR will be less than 12%, so you would decrease the initial estimate by a small amount, say, to 11% and recalculate NPV. Here is the NPV at 12%. \\[ NPV=−950+1001.12+11001.122=16.20 \\] NPV is smaller but still greater than zero. Increase the estimate of IRR to 13%. \\[ NPV=−950+1001.13+11001.132=−0.043 \\] NPV is slightly less than zero, so that 13% is a bit too high. Consider a trial estimate of 12.95%. \\[ NPV=−950+1001.1395+11001.12952=0.75 \\] One more trial: set the trial IRR to just under 13% to get very close to break-even NPV. \\[ NPV=−950+1001.129973+11001.1299732=0.00021 \\] This tedious workout iterates to an approximate solution. We can get ever so close to NPV = 0, but never quite get there. There is much need for just a bit of tolerance for ambiguity in this case! If anything, the rule that accept a project, or even proclaim that a price is right, can be made using IRR &gt; hurdle rate. 9.2.3 What can go wrong? There are three things that managers need to be aware of when using IRR. The result might be difficult to explain to others. You would need to establish that the goal is to find a yield that exceeds the hurdle rate. In any case the manager must have a consensus on what the hurdle rate should be. The same caveats to NPV also hold. If you believe that there is more yield, that is, return, in this contract than the hurdle rate, then you should buy the contract. IRR helps you understand that phrase “more yield.” Your hurdle rate might be wrong. This means that the rate might be higher or lower than 10%. For example if the rate is really 15%, then an IRR = 13% means that the contract does not earn at least the hurdle rate and you should reject the offer. Just as in NPV analysis you might not receive all of the cash flow that is promised. Suppose the contract defaults (a little bit!) and your business associate can only assure you that she can pay you 75 cents on the dollar in a worse case scenario. As in step one above, we calculate the NPV at the hurdle rate. \\[ NPV=−950+\\frac{(0.75)100}{1.10^1}+\\frac{(0.75)1100}{1.10^2}=−200 \\] Here we figure we only make $75 in the first year for sure and $825 in the second year. Quite a potential loss on a NPV basis! If we truly believe that this scenario could occur, then this is indeed a really bad deal from a NPV perspective! Also the IRR of the cash flows would be negative! An IRR = -0.0278 falls far short of the hurdle. ## Foreign exchange markets 9.2.4 What is the foreign exchange market? The foreign exchange market is the network of individuals, organizations, financial institutions, and governments that buy and sell U.S. dollars in exchange for other currencies. Currencies are often quoted in USD since better than The market for dollars, for example, is composed of all the traders in all of the locations in which trade transacts, including London, Singapore, Tokyo, New York, and so on. The principal function of the foreign exchange market is the transfer of funds, and thus purchasing power, from one nation and currency to another. Other functions provide short-term credits to finance trade and facilities for speculating and hedging foreign exchange risks. In purely doestic trade, payments transact only in the domestic currency. When doemstic entities conduct foreign trade, purchases and receipts for goods and services are denominated in the foreign currency. Thus in foreign trade the domestic currency must be exchanged for the foreign currency at a value of one for the other. When buyers and sellers move goods or provision services across as span of time, credit is needed to finance the future receipt or disbursement of funds. In a typical export trade, the exporter might allow 90 days for payment from the importer in the exporter’s currency. To get cash now, the exporter will take the 90 day note from the importer and discount the note at the exporter’s bank. The exporter receives the discounted cash flow in the exporter’s currency. In 90 days the bank then takes delivery of the importer’s currency and may then exchange the importer’s currency to the bank’s functional currency. 9.2.5 What are exchange rates? An exchange rate is the price or value of one nation’s currency in terms of another another nation’s currency. An exchange rate has two components, the value of domestic currency and the value of foreign currency, expressed as the ratio (rate) of the two currency values quoted either directly or indirectly. In a direct quotation, the price of a unit of foreign currency is expressed in terms of the domestic currency. In an indirect quotation, the price of a unit of domestic currency is expressed in terms of the foreign currency. Exchange rates are nearly always (post Bretton Woods, 1946) quoted in values against the US dollar. However, exchange rates can also be quoted against another nations currency, which are known as a cross currency, or cross rate. For example if the the current rate of USD (U.S. dollar) for each GBP (Great Britain pound sterling) is USD2.00 = GBP 1.00. An increase in the exchange rate from 2 to 2.20 means that it takes 10% more USD to buy GBP = \\[ \\frac{2.20 - 2.00}{2.00} = 0.10 \\] Because it takes more USD, this is a depreciation of the dollar against the pound. Similarly we could calculate the pound point of view as GBP1.00 = USD2.00 or GBP0.50 = USD1.00. If the pound is now GBP1.00 = 2.20 or GPB0.45455 = USD1.00000. This means that it will take 10% fewer GBP to buy USD. \\[ \\frac{0.45455 - 0.50000}{0.50000} = - 0.10 \\] Because it takes fewer GBP, this is an appreciation of the pound against the dollar, that is, the pound is dearer than the dollar. 9.2.6 What is a foreign exchange arrbitrage? It does happen that the exchange rate in one location, say New York, is different from the same exchange rate in, say, Dubai. Suppose that New York quotes USD1.98 = GBP1.00 and Dubai quotes USD2.00 = GBP1.00. This will be a 2-point arbitrage or trade. Traders can buy GBP in NY for USD1.98, and immediately sell them in Dubai for USD2.00 to make an arbitrage profit of USD0.02, less any transaction costs. As traders continue this arbitrage, there will be an increase in the demand for GBP relative to USD so that the USD price of GBP in NY will increase, and will fall in Dubai because of the increased supply of GBP. Arbitrage trading will continue until the price ratio of the two currencies is approximately the same. In a 3-point arbitrage there will be three traders in 3 money centers, but essentially the same result will occur = one rate, approximately, in equilibrium. 9.2.6.1 3-point or triagular arbitrage Now we have 3 traders vying for our treasury business. New York quotes EUR1.10 = USD1.00 and Dubai quotes USD1.98 = GBP1.00, while Singapore is quoting GBP0.51 = EUR1.00. In practice, traders will compute this ratio for the three currency pairs first and test if it is equal to 1. \\[ \\frac{EUR}{USD}\\frac{USD}{GBP}\\frac{GBP}{EUR} = \\frac{EUR}{EUR} = 1 \\] \\[ (1.10)(1.98)(0.51) = 1.11 \\geq 1 \\] This appears to be an 11% premium to the base currency USD. The ratio is embedded in these three transactions. Use USD1.00 (base currency) to buy EUR1.10 (counter-currency 1) in NY. Sell EUR1.10 in Singapore for GBP0.561 (counter-currency 2, 1.10 x 0.51). Sell GBP0.561 in Dubai to buy USD1.11 (0.561 x 1.98) to yield USD0.11 profit in the base currency. Is this a tidy profit? We must lock in these rates, pay our staff, pay our brokers and banks, and still make enough to earn a return for our weary investors. In any case, these rates will be traded to a near zero profit in high frequency and efficient markets. 9.2.7 How can you hedge with foreign exchange? 9.2.7.1 Forward exchange rate market A spot transaction requires delivery of currency usually within 2 or fewer (business) days, often defined by contract between buyer and seller. The spot exchange rate is the that covers this delivery condition. For example, we could enter into a spot transaction that says you will deliver to me GBP100 within 2 days at which time I will give you USD200. The spot exchange rate is thus USD2.00 = GBP1.00. A forward transaction is an agreement today to buy or sell a specific amount of currency at a specific location at a specific date in the future, under further specific conditions. Examples of forward transactions are forward rate agreements (FRAs), futures contracts (standardized forward rate agreements traded on exchanges), swaps, and options on FRAs, futures, and swaps. For example, we could enter into a FRA that says you will deliver to me GBP100 in 3 months at which time I will give you USD202. The 3 month forward exchange rate is thus USD2.02 = GBP1.00. In this transaction, and relative to the spot rate of USD2.00 = GBP1.00, GBP is at a USD0.02 forward premium, because at delivery, it takes fewer GBP to buy USD than at spot (today) rates. If the 3 month forward rate were USD1.98 = GBP1.00 then the GBP market would be a 3 month forward discount of USD0.02. The forward premium is thus \\[ \\frac{2.02 - 2.00}{2.00} = 0.01 \\] per 90 days. In an ACTUAL/365 day count this is \\(90/365 = 0.24658\\) years or \\(365/90 = 4.05556\\) 90 day periods. Across 4.05556 periods this amounts to \\[ (1.01)^{4.05556} - 1 = 1.04118 - 1 = 0.04118 \\] or a little over 4% per annum (year). Forward transactions arise from hedging and speculating in foreign exchange rate movements, and covered interest arbitrage. 9.2.7.2 Using a hedge Hedging is the act of mitigating unwanted movements, in this case, in foreign exchange rates. Spot rates will fluctuate through time as the historical record shows. In a hedge, the hedger forgoes any gains in future spot rates beyond what she contracts for today. But at the same time, she will also not suffer any losses beyond what she contracted for. A US importer has agreed to pay GBP1,000,000 for the delivery of aluminium ingots in 3 months time in the Port of Newark. The spot rate today is USD2.00 and the 3 month forward rate for GBP is USD2.02. How can she hedge the risk that she might have to pay more than GBP1,000,000 when converting USD to GBP? The hedger can buy GBP1,000,000 for delivery in 3 months at a forward rate of USD2.02. She is thus willing to pay \\(USD1,000,000 \\times USD0.02 = USD20,000\\) more to insure against future spot rates moving against her, that is, that the rate does not move higher than USD2.02 in three months. In three months the hedger will pay USD2,020,000 and will receive GBP1,000,000 to make the payment to the aluminium supplier. Suppose in 3 months the spot rate rose to USD2.05? If the US importer did not hedge, then she would have had to pay USD2,050,000 or USD30,000 more than if she had hedged. Instead what if the in 3 months the spot rate fell to USD1.99? If the US importer did not hedge, then she would have had to pay USD 1,990,000 or USD30,000 less than if she had hedged. But she did hedge, and thus forgoes this possible gain of USD30,000. 9.2.8 Covered interest arbitrage 9.2.8.1 Interest arbitrage and foreign exchange risk Potential discrepancies and anomalies in the debt securities markets between nations can also give rise to an arbitrage or trading opportunity. Interest arbitrage transfers liquid funds from one money center to another. If a MNC invests or must pay vendors or receive revenue from customers in a foreign country, the MNC must exchange home country funds into foreign country funds. The question is how much of one currency versus another, and when. As soon as we think about the future, we must now include interest rates into the discussioin. 9.2.8.2 Try this trade In parity, where there is no arbitrage (profitable trading) opportunity, the following two transactions are of the same value in one year: GBP to USD Spot- USD Loan Repayment: borrow GBP1.00 and convert to USD at USD2.00 = GBP1.00 and repay in one year at 2% and repay USD 2 x 1.02 or USD2.04 for each forward GBP1.00. USD to GBP Spot-GBP Loan Repayment - GBP to USD Forward: convert USD2.00 to GBP1.00, then repay in one year at 4% GBP1.00 x 1.04 or GBP1.04, and convert to USD at the forward rate of USD1.96154 = GBP1.00 to get USD2.04. At so-called parity, the two transactions are equivalent and thus there is no profitable trading opportunity. But what if the forward rate is not USD1.96154 = GBP1.00? In parity, the forward domestic value of a lending or a borrowing in a foreign account must equal the spot value of an account in a domestic account. \\[ Forward\\,rate \\times (1 + foreign\\,rate) = Spot\\,rate \\times (1 + domestic\\,rate) \\] Solving for the forward rate (domestic currency = one unit of the foreign currency) we get \\[ Forward\\,\\,rate = Spot\\,\\,rate \\times \\frac{(1 + domestic\\,\\,rate)}{(1 + foreign\\,\\,rate)} \\] 9.2.8.3 An opportunity? Suppose that the treasurer observes a forward rate of USD2.02 = GBP1.00. The savvy treasurer might exploit this arbitrage opportunity with these transactions. Borrow USD500,000 at 2% per annum and repay the loan in one year with USD510,000. Convert the USD500,000 at the spot rate USD2.00 = GBP1.00 into GBP250,000 because it offers a higher one-year interest rate of 4%. Deposit GBP500,000 in a London Bank at 4% per annum, and simultaneously enter into a forward contract that converts the full maturity amount of the deposit GBP260,000 into USD at the one-year forward rate of USD2.02 = GBP1.00. After one year, settle the one year forward contract at the contracted one year forward rate of USD2.02 = GBP1.00, which would give the savvy treasurer USD525,200. Repay the loan amount of USD510,000 and reap a profit of USD15,200. Get permission from the CFO to have a party for the team. 9.2.8.4 Going backwards But wait! What if the one year forward rate is USD1.92 = GBP1.00. Then the treasurer does the reverse of the above transaction. Borrow GBP250,000 at 4% per annum and repay the loan in one year with GBP260,000. Convert the borrowed GBP into USD500,000 at the spot rate USD2.00 = GBP1.00. Deposit USD500,000 in a New York Bank at 2% per annum, and simultaneously enter into a forward contract that converts the full maturity amount of the deposit USD510,000 into GBP at the one-year forward rate of USD1.92 = GBP1.00. After one year, settle the one year forward contract at the contracted one year forward rate of USD1.92 = GBP1.00, which would give the savvy treasurer GBP265,625 (\\(=510000/1.92\\)). Repay the loan amount of GBP260,000 and reap a profit of GBP5,625. Get permission from the CFO to have a smaller party for the team. 9.2.9 What are exchange rate movements? Exchange rate movements are commonly measured by the percentage change in their values over a specified period, such as a month or a year. MNCs closely monitor exchange rate movements over the period in which they have cash flows denominated in the foreign currencies of concern. The equilibrium exchange rate between two currencies at any time is based on demand and supply conditions. Changes in the demand for a currency or the supply of a currency for sale will affect the equilibrium exchange rate. 9.2.9.1 What affects exchange rate movements? The key economic factors that can influence exchange rate movements through their effects on demand and supply conditions are relative inflation rates, interest rates, income levels, and government controls. When these factors lead to a change in international trade or financial flows, they affect the demand for a currency or the supply of currency for sale and thus the equilibrium exchange rate. If a foreign country experiences an increase in interest rates, the price of the foreign country’s debt instruments decreases (relative to U.S. interest rates and security prices). THis condition creates an excess demand for foreign treasuries and thus the foreign currency to buy these treasuries. If investors are buying foreign currency they are exchanging the currency for domestic (here U.S. currency) and thus there will be an inflow of U.S. funds to purchase its securities. Demand for domestic currency increases, Supply of foreign currency decreases, and There will be an upward pressure on the foreign currency’s equilibrium value relative to the domestic currency. All relevant factors must be considered simultaneously when attempting to predict the most likely movement in a currency’s value. 9.2.9.2 How do trade and financial flows affect exchange rates? There are distinct international trade and financial flows between every pair of countries. These flows dictate the unique supply and demand conditions for the currencies of the two countries, which affect the equilibrium cross exchange rate between their currencies. Trade factors include current account movements in the demand for imports and the supply of exports from a country. Export-import flows of goods and services impact the prices, and thus the inflation and deflation of the prices of goods and services. Government controls export-import flows through tariffs, most favored nation treatment, and through quotas. Each of these mechanisms affects the price levels of exports and imports and will thus impact the demand and supply of home country currency. For example, if the home government imposes a tariff on imported aluminium, the home supply of aluminium will be restricted and home aluminium prices will rise and home producers will demand less aluminium. If demand for foreign aluminium eases, then so does the demand for foreign currency to buy the aluminium from foreign aluminium producers. This will cause a downward pressure on foreign currency and an upward pressure on the home currency. 9.2.10 How can financial services benefit? Financial institutions can attempt to benefit from the expected appreciation of a currency by purchasing that currency. Analogously, they can benefit from expected depreciation of a currency by borrowing that currency and exchanging it for their home currency. "],
["references-4.html", "References", " References "]
]
